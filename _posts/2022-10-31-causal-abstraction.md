---
layout: post
title: "Faithful, Interpretable Model Explanations via Causal Abstraction"
short-summary: "Causal abstraction provides a powerful set of techniques for achieving accurate, human-interpretable explanations of AI models."
summary: "Causal abstraction provides a powerful set of techniques for achieving accurate, human-interpretable explanations of AI models."
feature-img: "/assets/img/posts/2022-10-31-causal-abstraction/large_img.png"
thumbnail: "/assets/img/posts/2022-10-31-causal-abstraction/large_img.png"
author: <a href="https://atticusg.github.io">Atticus Geiger</a>, <a href="https://zen-wu.social">Zhengxuan Wu</a>, <a href="https://kareldo.github.io">Karel D'Oosterlinck</a>, <a href="https://www.elisakreiss.com">Elisa Kreiss</a>, <a href="https://cocolab.stanford.edu/ndg">Noah D. Goodman</a>, <a href="https://web.stanford.edu/~icard/">Thomas Icard</a>, and <a href="https://web.stanford.edu/~cgpotts/">Christopher Potts</a>
tags: [machine learning, explanation, interpretability, causal inference]
draft: True
---
## Seeking human-intelligible explanations

Explaining _why_ a deep learning model makes the predictions it does has emerged as one of the most challenging questions in AI ([Lipton 2018](#id.2bn6wsx), [Pearl 2019](#id.1pxezwc)). There is something of a paradox about this, however. After all, deep learning models are closed, deterministic systems that give us ground-truth knowledge of the causal relationships between all their components. Thus, their behavior is in many ways easy to explain: one can mechanistically walk through the mathematical operations or the associated computer code, and this can be done at varying levels of detail. These are common and valuable modes of explanation in the classroom.

What is missing from these explanations? They don't provide human-intelligible answers to the actual questions that motivate explanation methods in AI -- questions like "Is the model robust to specific kinds of input", "Does it treat all groups fairly?", and "Is it safe to deploy?" For explanations that can engage with these questions, we need methods that are provably faithful to the low-level details ([Jacovi and Goldberg 2020](#id.1ci93xb)) but stated in higher-level conceptual terms.

## The importance of interventions

Over a series of recent papers ([Geiger et al. 2020](#id.4i7ojhp), [Geiger et al. 2021](#id.1y810tw), [Geiger et al. 2022](#id.2xcytpi), [Wu et al. 2022a](#id.ihv636), [Wu et al. 2022b](#id.eamkrplgtjub)), we have argued that the theory of _causal abstraction_ ([Chalupka et al. 2016](#id.msfz0o95blmp), [Rubinstein et al. 2017](#id.wby8gkzc7cqd), [Beckers and Halpern 2019](#id.26in1rg), [Beckers et al. 2019](#id.3rdcrjn)) provides a powerful toolkit for achieving the desired kinds of explanation in AI. In causal abstraction, we assess whether a particular high-level (possibly symbolic) mode  _H_ is a faithful proxy for a lower-level (in our setting, usually neural) model _N_ in the sense that the causal effects of components in _H_ summarize the causal effects of components of _N_. In this scenario, _N_ is the AI model that has been deployed to solve a particular task, and _H_ is one's probably partial, high-level characterization of how the task domain works (or should work). Where this relationship between _N_ and _H_ holds, we say that _H_ is a _causal abstraction_  of _N_. This means that we can use _H_ to directly engage with high-level questions of robustness, fairness, and safety in deploying _N_ for real-world tasks.

Causal abstraction can be situated within a larger class of explanation methods that are grounded in formal theories of causality. We review a wide range of such methods below. A unifying property of all these approaches is that they involve intervening on model representations to create counterfactual model states and then systematically studying the effects of these interventions on model behavior. This leverages a fundamental insight of causal reasoning: to isolate and characterize causal factors, we need to systematically vary components of our model and study the effects this has on outcomes. In this way, we can try to piece together a complete causal model of the process of interest.

In this post, we review the technical details of causal abstraction in intuitive terms. We focus in particular on the core operation of _interchange interventions_ and seek to relate this operation to interventions employed by other causal explanation methods. Using this idea, we also define _interchange intervention accuracy_, a new metric that allows us to move beyond simple testing based purely on input--output behavior to directly assess the degree to which a neural model is explained by a high-level model. Finally, we identify some key questions for future work in this area.

## Causal abstraction

Through the following series of animations, we seek to provide an intuitive overview of how causal abstraction techniques work. The first figure asks you to imagine that you have a neural network that takes in three numbers and adds them together. We can assume that the network does this job perfectly. There is still a further question: How does the network achieve this behavior?

{% figure %}
<img class="postimage_75" src="{{ site.baseurl }}/assets/img/posts/2022-10-31-causal-abstraction/interchange-intervention-1.png" alt="Diagram of a neural network that takes in three numbers and adds them. The network has a single hidden layer consisting of three neurons, densely connected to the inputs, and it has a one-dimensional output layer that is densely connected to the hidden layer. The caption reads, 'Our neural network successfully adds three numbers. In huamn-interpretable terms, how does it do it?'">
{% endfigure %}

Suppose you formulate the _hypothesis_ that the network (1) adds the first two inputs together to form an intermediate value _S_<sub>1</sub>, (2) encodes the value of the third input in a separate internal state _w_, and (3) uses _S_<sub>1</sub> and _w_ directly to compute the final sum _S_<sub>2</sub>. This hypothesis is described by the causal model in green:

{% figure %}
<video controls autoplay loop muted playsinline class="postimage_75">
  <source src="{{ site.baseurl }}/assets/img/posts/2022-10-31-causal-abstraction/interchange-intervention-2.mp4" type="video/mp4" title="A continuation of the above image. The neural network is repeated, but now with a causal model depicted in green. The causal model takes in three numbers, stores the sum of the first two in a variable S1, stores the value of the third input in a variable w, and adds S1 and w to produce the output sum S2. The video then shows a hypothesized alignment: the third hidden neuron in the neural network is aligned with the variable S1 in the causal model.">
  Your browser does not support the video tag.
</video>
{% endfigure %}

Using causal abstraction, we can assess this hypothesis about how our network works. This analysis is based on a series of interchange interventions. The first such intervention happens on the causal model. Using this model, we process the input \[1, 3, 5\] and obtain the output 9 (purple diagram). Then we use the model to process \[4, 5, 6\] to get 15 (blue diagram). We also record the intermediate value, _S_<sub>1</sub> = 9, on the way to this result. Finally, we intervene on the first computation by replacing the value of the _S_<sub>1</sub> variable (_S_<sub>1</sub> = 4) with the value we recorded when processing \[4, 5, 6\] (intervention A). Since the structure of the causal model is fully deterministic and understood, we know that this intervention changes the output to 14.

{% figure %}
<video controls autoplay loop muted playsinline class="postimage_75">
  <source src="{{ site.baseurl }}/assets/img/posts/2022-10-31-causal-abstraction/interchange-intervention-3.mp4" type="video/mp4" title="A continuation of the above video. The neural network and causal model are repeated. At the top, we use the causal model to process [1, 3, 5], and we use it again to process [4, 5, 6]. We then take the S1 value for the [4, 5, 6] input, which is 9, and use it to replace the S1 value from [1, 3, 5]. This causes the causal model to produce 14, since the causal model's outputs depend only on S1 and w.">
  Your browser does not support the video tag.
</video>
{% endfigure %}

For the target neural model, the results of intervention are likely to be less clearly known ahead of time, due to the opacity of how these networks map inputs to outputs. Suppose we believe that the hidden state _L_<sub>3</sub> plays the role of _S_<sub>1</sub> in the causal model. To test this, we use the network to get values for inputs \[1, 3, 5\] and \[4, 5, 6\] as before. Then we intervene: we take the computed value at _L_<sub>3</sub> from the orange computation and use it in the corresponding place in the yellow computation (intervention B). If this leads the network to output 14, then we have a piece of evidence that the network localizes the _S_<sub>1</sub> variable at _L_<sub>3</sub>.

{% figure %}
<video controls autoplay loop muted playsinline class="postimage_75">
  <source src="{{ site.baseurl }}/assets/img/posts/2022-10-31-causal-abstraction/interchange-intervention-4.mp4" type="video/mp4" title="A continuation of the above video. The diagram is now extended with two uses of the neural model: one processing inputs [1, 3, 5] and the other processing [4, 5, 6], in parallel to what we did with the causal model. The video them shows an intervention: The L3 hidden neuron from the [4, 5, 6] example is used to replace the L3 variable from the [1, 3, 5] example. We then show an intervention success: the neural model outputs 14 under this intervention, which is a piece of evidence that L3 plays the causal role of S1.">
  Your browser does not support the video tag.
</video>
{% endfigure %}

If the causal and neural models agree in this way on _all_ inputs, then we have shown that _S_<sub>1</sub> and _L_<sub>3</sub> are in this aligned relationship. In other words, _S_<sub>1</sub> is a causal abstraction of _L_<sub>3</sub>: _L_<sub>3</sub> plays the same _causal role_ in the network that _S_<sub>1</sub> plays in the high-level algorithm.

We can perform the same kind of interchange interventions to assess whether _L_<sub>1</sub> plays the role of _w_. The technique could also lead us to discover that the state _L_<sub>2</sub> plays no causal role in the network's behavior, if interventions on that value never affect the model's input--output behavior. In this way, interchange interventions reveal to us the causal structure of the neural model. If we can successfully align all of the variables in the high-level model with variables (or sets of them) in the low-level model, then we say that the high-level model is a causal abstraction of the low-level model (under our chosen alignment).

This is a simple illustrative example, but the techniques apply to complex, large-scale problems as well. [Geiger et al. (2020)](#id.4i7ojhp) employed an early version of causal abstraction analysis to explain why Transformer-based models achieve robust compositional generalization for natural language inference (NLI) examples involving lexical entailment and negation. [Geiger et al. (2021)](#id.1y810tw) and [Geiger et al. (2022)](#id.2xcytpi) subsequently developed the theoretical foundations for this kind of analysis, building on [Beckers and Halpern's (2019)](#id.26in1rg) proposals, and applied the method to more complex NLI examples with quantifiers, negation, and modifiers, as well as to problems in computer vision and grounded language use. [Li, Nye, and Andreas (2021)](#id.3whwml4) employed this method to find implicit semantic representations in language models.

## Interchange intervention training

A clear limitation of the above methods is that we needed to figure out which alignment hypotheses to test. If we test the wrong ones, we might fail to identify the causal structure that is actually present. For current deep learning models, the number of representations to check might be very large, and the tenets of causal abstraction allow us to test alignments involving subparts of these representations as well as sets of representations. This leads to an astronomically large set of alignment hypotheses to test. We may be able to rule out some of these hypotheses in principle, but there will still be many, many plausible ones.

In practice, we have sought to address this by using probe models (e.g., [Tenney et al. 2019](#id.p1mwr4cykxbc)) and feature attribution methods (e.g, [Sudararajan et al. 2017](#id.3o7alnk)) to heuristically find alignments that are likely to be good given our high-level models. For example, in our simple addition network, feature attribution methods might tell us that _L_<sub>2</sub> makes no contribution to the model's predictions, and thus we could eliminate _L_<sub>2</sub> as a candidate for playing the role of _S_<sub>1</sub> or _w_ in the causal model. Furthermore, supervised probes might allow us to home in on _L_<sub>3</sub>  as the most likely counterpart of the _S_<sub>1</sub>  variable. These techniques are especially important where one is testing hypotheses about a very large model that can't be changed, but rather must be studied and understood in its own right.

In many situations, though, we have the freedom to move out of this pure analysis mode and can instead actually train networks to have the causal structure that we hope to see. We call this _interchange intervention training_ (IIT; [Geiger et al. 2022](#id.2xcytpi),[Wu et al. 2022](#id.ihv636)). With IIT, we push the neural model to conform to the causal model while still allowing it to learn from data as usual. IIT is a very intuitive extension of causal abstraction: our interventions create "Frankenstein" back-propagation graphs that combine the graphs from other examples. This creates new training instances that we can label with our causal model:

{% figure %}
<video controls autoplay loop muted playsinline class="postimage_75">
  <source src="{{ site.baseurl }}/assets/img/posts/2022-10-31-causal-abstraction/iit-movie.mp4" type="video/mp4" title="A continuation of the above video. Now, we suppose that our intervention on L3 did not lead to the same output as we get from our causal model. The difference between the network's prediction and the causal model's output gives us an error signal. We back-propagate as usual, but the L3 node now include the computation graph from the current example as well as the one from the source example. The effect of these updates is to localize the S1 variable in L3.">
  Your browser does not support the video tag.
</video>
{% endfigure %}

For our running example, this means that we train the network not only to handle inputs like \[1, 3, 5\] properly, but also to make correct predictions under the intervention storing 4 + 5 at _L_<sub>3</sub>. Comparable interventions could support the alignment of _w_ and _L_<sub>1</sub>. The result is a neural model that is not only successful at our task but corresponds to the causal dynamics of the high-level model. For complex tasks grounded in real data, this will combine the benefits of standard data-driven learning with the structure provided by the high-level model.

## Beyond simple behavioral evaluation: Interchange Intervention Accuracy

Throughout AI, we rely on behavioral evaluations to determine how successful our models are. For example, classifier models are assessed by their ability to accurately predict the labels for held-out test examples, and language models are assessed by their ability to assign high probability to held-out test strings. Such evaluations can be highly informative, but their limitations immediately become evident when we start to think about deploying models into unfamiliar contexts where their inputs are likely to be different from those they were evaluated on. In such contexts, their behavior might be unsystematic, or systematically biased in problematic ways.

Interchange interventions can be used to define more stringent evaluation criteria. Our primary metric in this space is _interchange intervention accuracy_, which assesses how accurate the model is under the counterfactuals created by interchange interventions of a particular kind. Such evaluations directly assess how close the target model comes to realizing a specific high-level causal model.

As [Geiger et al. (2022)](#id.2xcytpi) show, models can have perfect behavioral accuracy but imperfect interchange intervention accuracy. Such models have found good solutions that are nonetheless unsystematic with respect to the high-level causal model. If it is vital that we conform to the causal model, due to reasons related to safety or fairness, then we have diagnosed a pressing problem that was hidden by standard behavioral testing.

## Other intervention-based methods as a special case of causal abstraction

Causal abstraction is a general framework for explanation methods in AI. The basic operation is the intervention, which we can express using notation from [Pearl (2009)](#id.3as4poj): _M_<sub><em>Y←y</em></sub> is the model that is identical to _M_ except the values for the variables _Y_ are set to the constant values _y_. The nature of this intervention is left wide open, and different choices give us different methods. Many popular behavioral and intervention-based explanation methods in AI can be explicitly understood as a special case of causal abstraction. A number of prominent examples are described in [our appendix](#appendix-details-on-other-intervention-based-methods).

## Ongoing work

The methods explored above are in their infancy, especially with respect to explainability applications in AI. To close this post, let's chart out a few exciting open avenues for research:

### Benchmarking explanation methods

CEBaB ([Abraham et al. 2022](#id.17dp8vu)) is a naturalistic benchmark for making apples-to-apples comparisons between explanation methods. Upon its release, existing methods generally did not perform better than simple baselines. In [Wu et al. 2022b](#id.eamkrplgtjub), we use intervention-based methods to develop Causal Proxy Models (CPMs). Input-based CPMs use simple interventions on inputs, whereas hidden-state CPMs use IIT to localize concepts in specific internal representations. Both kinds of CPM achieve state-of-the-art results on CEBaB. In addition, they achieve strong task performance, which suggests that they can actually be deployed as more explainable variants of the models they were initially intended to explain.

### Model distillation

[Wu et al. (2022)](#id.ihv636) augment the standard language model distillation objectives ([Sanh et al. 2019](#id.2p2csry)) with an IIT objective and show that it improves over standard distillation techniques in language model perplexity (Wikipedia), the GLUE Benchmark, and the CoNLL-2003 named-entity recognition task. These preliminary results show that IIT has great promise for distillation. This has inherent value and also begins to show us how general and abstract the causal models for IIT can be; in our earlier use-cases, it is a symbolic model, whereas here it is essentially just an alignment between the hidden representations in two neural models.

### Theoretical foundations

We are seeking to further develop the core theory of causal abstraction so that we can confidently apply the methods to a wider range of models and unify existing approaches rigorously.

## A rallying cry

AI models are at work all around us, in diverse contexts serving diverse goals. The prevalence of these models in our lives tracks directly with their increasing size and deepening opacity. Explanation methods are a crucial part of making sure these deployments go as expected. Causal abstraction analysis supports a family of causal methods that seem to be our best bet at achieving explanations that are both human-interpretable and true to the actual underlying causal structure of these models. We hope that centering the discussion around _interventions_ and _abstraction_ helps point the way to new and even more powerful methods.

## Acknowledgements

Our thanks to SAIL editors Jacob Schreiber and Megha Srivastava for extremely valuable input.

## Appendix: Details on other intervention-based methods

We [noted above](#other-intervention-based-methods-as-a-special-case-of-causal-abstraction) that a number of explanation methods in AI can be described using tools and techniques from causal abstraction. Here we provide some examples with references.

1. Explanation methods purely grounded in the behavior of a model can be directly interpreted in causal terms when we interpret the act of providing an input to a model as an intervention to the input variable. Methods like LIME ([Ribeiro et al. 2016](#id.2b5tb06dt22k)), ConceptSHAP ([Yeh et al. 2020](#id.8wrr6nktadgt)), and estimating the causal effect of real-world concepts on model behavior only consider input--output behavior, so causal abstraction can accurately capture these simple methods using two-variable (input and output) causal models.
2. Input manipulations and data augmentation methods generally correspond to interventions on model inputs ([Molnar 2022](#id.7baf97dsls7o)). Many feature attribution methods can be stated in these terms. For example, deleting a feature _Y_ in model _M_ corresponds to the intervention _M_<sub><em>Y</em>←0</sub>. In permutation-based feature importance, we randomly shuffle the value of a feature. This can be seen as a series of random interventions on inputs. [Begus (2020)](#id.j0vqygd6mw1w) uses input interventions to argue that unsupervised deep speech networks learn to represent phonemes. All these methods also can be seen as causal abstraction analysis with a two-variable chain.
3. Interventions could even happen at the level of the data-generating process ([Feder et al. 2021](#id.z337ya), [Abraham et al. 2022](#id.17dp8vu)) or model training data statistics ([Elazar et al. 2022](#id.44sinio)). Causal abstraction represents these methods as marginalizing away every variable other than the real-world concept of interest and the model output.
4. Causal mediation analysis ([Vig et al. 2020](#id.23ckvvd), [Meng et al. 2022](#id.qsh70q)) is an early and influential use of interventions to explain models. In this mode, we study the ways in which a model's input--output relationships are mediated by intermediate variables, and we do this by intervening on those intermediate variables to identify direct and indirect causal effects. This can be understood as causal abstraction analysis with a three-variable chain for the high-level model.
5. In Iterative Nullspace Projection, we project the original vector onto the null space of a linear probe ([Ravfogel et al. 2021](#id.49x2ik5), [Elazar et al. 2021](#id.2jxsxqh)). This can also be modeled as abstraction by a three-variable causal model.
6. Circuit-based explanations ([Cammarata et al. 2020](#kix.i18d39aii9vh)) contend that neural networks consist of meaningful, understandable features connected in circuits formed by weights. Causal abstraction analysis can be directly applied to operationalize such questions.
7. When interventions target neural vector representations, the values of the intervention _y_ might be zero vectors, a perturbed or jittered version of the original vector, a learned binary mask applied to the original vector ([Csordás, Steenkiste, and Schmidhuber 2021](#id.35nkun2), [De Cao et al. 2021](#id.1ksv4uv)), or a tensor product representation ([Soulos et al. 2020](#id.147n2zr)). Interchange interventions are a special case of this where the values _y_ are values realized by the representation on some other actual input.

## References

<a id="id.17dp8vu"></a>
Abraham, Eldar David, Karel D'Oosterlinck, Amir Feder, Yair Ori Gat, Atticus Geiger, Christopher Potts, Roi Reichart, and Zhengxuan Wu. 2022. [CEBaB: Estimating the causal effects of real-world concepts on NLP model behavior](https://arxiv.org/abs/2205.14140). To appear in _Advances in Neural Information Processing Systems_.

<a id="id.3rdcrjn"></a>
Beckers, Sander, Frederick Eberhardt, and Joseph Y. Halpern. 2019. [Approximate causal abstractions](http://proceedings.mlr.press/v115/beckers20a.html). In _Proceedings of the 35th Uncertainty in Artificial Intelligence Conference_, 606--615.

<a id="id.26in1rg"></a>
Beckers, Sander, and Joseph Halpern. 2019. [Abstracting causal models](https://ojs.aaai.org/index.php/AAAI/article/view/4117). In <em>AAAI Conference on Artificial Intelligence</em>.

<a id="id.j0vqygd6mw1w"></a>
Beguš, Gašper. 2020. [Generative adversarial phonology: Modeling unsupervised phonetic and phonological learning with neural networks](https://www.frontiersin.org/articles/10.3389/frai.2020.00044/full). _Frontiers in Artificial Intelligence_ 3.

<a id="kix.i18d39aii9vh"></a>
Cammarata, Nick, Shan Carter, Gabriel Goh, Chris Olah, Michael Petrov, Ludwig Schubert, Chelsea Voss, Ben Egan, and Swee Kiat Lee. 2020. [Thread: Circuits](https://distill.pub/2020/circuits). _Distill_.

<a id="id.msfz0o95blmp"></a>
Chalupka, Krzysztof, Frederick Eberhardt, and Pietro Perona. 2016. [Multi-level cause-effect systems](http://proceedings.mlr.press/v51/chalupka16.html). In _Proceedings of the 19th International Conference on Artificial Intelligence and Statistics_, 361--369, Cadiz, Spain.

<a id="id.lnxbz9"></a>
Chattopadhyay, Aditya, Piyushi Manupriya, Anirban Sarkar, and Vineeth N Balasubramanian. 2019. [Neural network attributions: A causal perspective](https://proceedings.mlr.press/v97/chattopadhyay19a.html). In _Proceedings of the 36th International Conference on Machine Learning_.

<a id="id.35nkun2"></a>
Csordás, Róbert, Sjoerd van Steenkiste, and Jürgen Schmidhuber. 2021. [Are neural nets modular? Inspecting functional modularity through differentiable weight masks](https://openreview.net/forum?id=7uVcpu-gMD). In _9th International Conference on Learning Representations_.

<a id="id.1ksv4uv"></a>
De Cao, Nicola, Leon Schmid, Dieuwke Hupkes, and Ivan Titov. 2021. [Sparse interventions in language models with differentiable masking](https://arxiv.org/abs/2112.06837). arXiv:2112.06837.

<a id="id.44sinio"></a>
Elazar, Yanai, Nora Kassner, Shauli Ravfogel, Amir Feder, Abhilasha Ravichander, Marius Mosbach, Yonatan Belinkov, Hinrich Schütze, and Yoav Goldberg. 2022. [Measuring causal effects of data statistics on language model's 'factual' predictions](https://doi.org/10.48550/arXiv.2207.14251). arXiv.2207.14251.

<a id="id.2jxsxqh"></a>
Elazar, Yanai, Shauli Ravfogel, Alon Jacovi, and Yoav Goldberg. 2021. [Amnesic probing: Behavioral explanation with amnesic counterfactuals](https://doi.org/10.1162/tacl_a_00359). _Transactions of the Association for Computational Linguistics_ 9: 160--75.

<a id="id.z337ya"></a>
Feder, Amir, Nadav Oved, Uri Shalit, and Roi Reichart. 2021. [CausaLM: Causal model explanation through counterfactual language models](https://doi.org/10.1162/coli_a_00404). _Computational Linguistics_ 47: 333--386.

<a id="id.3j2qqm3"></a>
Geiger, Atticus, Ignacio Cases, Lauri Karttunen, and Christopher Potts. 2019. [Posing fair generalization tasks for natural language inference](https://doi.org/10.18653/v1/D19-1456). In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing_, 4475--85. Association for Computational Linguistics.

<a id="id.1y810tw"></a>
Geiger, Atticus, Hanson Lu, Thomas F. Icard, and Christopher Potts. 2021. [Causal abstractions of neural networks](https://openreview.net/forum?id=RmuXDtjDhG). In _Advances in Neural Information Processing Systems_, 9574--9586.

<a id="id.4i7ojhp"></a>
Geiger, Atticus, Kyle Richardson, and Christopher Potts. 2020. [Neural natural language inference models partially embed theories of lexical entailment and negation](https://doi.org/10.18653/v1/2020.blackboxnlp-1.1). In _Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP_. Association for Computational Linguistics.

<a id="id.2xcytpi"></a>
Geiger, Atticus, Zhengxuan Wu, Hanson Lu, Josh Rozner, Elisa Kreiss, Thomas Icard, Noah Goodman, and Christopher Potts. 2022. [Inducing causal structure for interpretable neural networks](https://proceedings.mlr.press/v162/geiger22a.html) In _Proceedings of the 39th International Conference on Machine Learning_, 7324--7338.

<a id="id.1ci93xb"></a>
Jacovi, Alon, and Yoav Goldberg. 2020. [Towards faithfully interpretable NLP systems: How should we define and evaluate faithfulness?](https://doi.org/10.18653/v1/2020.acl-main.386) In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, 4198--4205. Association for Computational Linguistics.

<a id="id.3whwml4"></a>
Li, Belinda Z., Maxwell I. Nye, and Jacob Andreas. 2021. [Implicit representations of meaning in neural language models](https://doi.org/10.18653/v1/2021.acl-long.143) In _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing_, 1813--1827. Association for Computational Linguistics.

<a id="id.2bn6wsx"></a>
Lipton, Zachary C. 2018. [The mythos of model interpretability](https://doi.org/10.1145/3233231). _Communications of the ACM_ 10: 36--43.

<a id="id.qsh70q"></a>
Meng, Kevin, David Bau, Alex Andonian, and Yonatan Belinkov. 2022. [Locating and editing factual associations in GPT](https://doi.org/10.48550/ARXIV.2202.05262), arXiv:2202.05262.

<a id="id.7baf97dsls7o"></a>
Molnar, Christoph. 2020. [Interpretable machine learning: A guide for making black box models explainable](https://christophm.github.io/interpretable-ml-book/).

<a id="id.3as4poj"></a>
Pearl, Judea. 2009. _Causality_. Cambridge University Press.

<a id="id.1pxezwc"></a>
Pearl, Judea. 2019. The limitations of opaque learning machines. In John Brockman, ed., _Possible Minds: Twenty-Five Ways of Looking at AI_, 13--19.

<a id="id.49x2ik5"></a>
Ravfogel, Shauli, Grusha Prasad, Tal Linzen, and Yoav Goldberg. 2021. [Counterfactual interventions reveal the causal effect of relative clause representations on agreement prediction](https://doi.org/10.18653/v1/2021.conll-1.15). In _Proceedings of the 25th Conference on Computational Natural Language Learning_, 194--209. Association for Computational Linguistics.

<a id="id.2b5tb06dt22k"></a>
Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. 2016. ["Why Should I Trust You?: Explaining the predictions of any classifier](https://dl.acm.org/doi/10.1145/2939672.2939778) In _Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_.

<a id="id.wby8gkzc7cqd"></a>
Rubenstein, Paul K., Sebastian Weichwald, Stephan Bongers, Joris M. Mooij, Dominik Janzing, Moritz Grosse-Wentrup, and Bernhard Schölkopf. 2017. [Causal consistency of structural equation models](http://auai.org/uai2017/proceedings/papers/11.pdf). In _Proceedings of the 33rd Conference on Uncertainty in Artificial Intelligence_. Association for Uncertainty in Artificial Intelligence.

<a id="id.2p2csry"></a>
Sanh, Victor, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. [DistilBERT, a distilled version of BERT: Smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108). arXiv:1910.01108.

<a id="id.147n2zr"></a>
Soulos, Paul, R. Thomas McCoy, Tal Linzen, and Paul Smolensky. 2020. [Discovering the compositional structure of vector representations with role learning networks](https://aclanthology.org/2020.blackboxnlp-1.23/). In _Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP_, 238--254. Association for Computational Linguistics.

<a id="id.3o7alnk"></a>
Sundararajan, Mukund, Ankur Taly, and Qiqi Yan. 2017. [Axiomatic attribution for deep networks](https://dl.acm.org/doi/10.5555/3305890.3306024). In _Proceedings of the 34th International Conference on Machine Learning_ 70, 3319--3328.

<a id="id.p1mwr4cykxbc"></a>
Tenney, Ian, Dipanjan Das, and Ellie Pavlick. 2019. [BERT rediscovers the classical NLP pipeline](https://www.aclweb.org/anthology/P19-145). In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, 4593--4601.

<a id="id.23ckvvd"></a>
Vig, Jesse, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer, and Stuart Shieber. 2020. [Causal mediation analysis for interpreting neural NLP: The case of gender bias](https://arxiv.org/abs/2004.12265). arXiv:2004.12265.

<a id="id.ihv636"></a>
Wu, Zhengxuan, Atticus Geiger, Joshua Rozner, Elisa Kreiss, Hanson Lu, Thomas Icard, Christopher Potts, and Noah Goodman. 2022a. [Causal distillation for language models](https://doi.org/10.18653/v1/2022.naacl-main.318). In _Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, 4288--4295. Association for Computational Linguistics.

<a id="id.eamkrplgtjub"></a>
Wu, Zhengxuan;\* Karel D\'Oosterlinck;\* Atticus Geiger;\* Amir Zur; and Christopher Potts; 2022b. [Causal Proxy Models for concept-based model explanations](https://arxiv.org/abs/2209.14279). arXiv: 2209.14279.

<a id="id.8wrr6nktadgt"></a>
Yeh, Chih-Kuan, Been Kim, Sercan Arik, Chun-Liang Li, Tomas Pfister, and Pradeep Ravikumar. 2020. [On completeness-aware concept-based explanations in deep neural networks](https://proceedings.neurips.cc/paper/2020/hash/ecb287ff763c169694f682af52c1f309-Abstract.html). _Advances in Neural Information Processing Systems_ 33:20554--20565.
