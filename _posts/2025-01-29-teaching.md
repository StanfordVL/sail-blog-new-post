---
layout: post
title: "Productive Struggle: The Future of Human Learning in the Age of AI"
short-summary: "What happens to human learning when superhuman intelligence is as accessible as a Google search?"
summary: "What happens to human learning when superhuman intelligence is as accessible as a Google search?"
subtitle: "The Future of Human Learning in the Age of AI"
feature-img: "assets/img/posts/2025-01-29-teaching/tutorcopilot.001.jpeg"
thumbnail: "assets/img/posts/2025-01-29-teaching/tutorcopilot.001.jpeg"
author: <a href='https://rosewang2008.github.io/'>Rose Wang</a> and <a href='cs.stanford.edu/~megha'>Megha Srivastava</a>
tags: [education, NLP, robotics, AI, human-AI interaction]
---

<link rel="stylesheet" href="path_to_bigfoot/bigfoot-default.css">
<script src="path_to_bigfoot/bigfoot.min.js"></script>
<script>
  document.addEventListener("DOMContentLoaded", function () {
    bigfoot({
      duplicateFootnoteStrategy: "combine", // Keeps repeated citations as popovers
    });
  });
</script>


Walking through our computer science building, we can see ChatGPT on nearly every screen. Today, students can use AI at every stage of their learning process. For example, instead of struggling to figure out how to start a coding assignment, students can simply copy and paste the question into an AI model. Even if the solution doesn’t work perfectly out of the box, they can re-prompt the model with its own solution and an error description to receive a fixed solution. 

We can’t help but compare this to our own experiences learning to program during undergrad. We remember the struggle of writing our first lines of code, the days spent debugging with friends at the student center, and the feeling of success after a night’s sleep when finally fixing the bug. We didn’t enjoy being stuck in the moment; but now, we look back and understand that going through these surmountable struggles was important for our learning. Our **productive struggles** not only helped us provide the correct solution in the short run, but also how to write stronger, less error-prone code in the long run.


 {% figure %}
<div><img class="postimage" src="{{ site.baseurl }}/assets/img/posts/2025-01-29-teaching/tweet.png" style="width: 70%;"/></div>
<figcaption>
  <b>Figure 1: </b>Productive struggle is important for human learning. Have we become too dependent on ChatGPT? <a href="https://x.com/daanksy/status/1872354066791346272" target="_blank">Source</a>
</figcaption>
{% endfigure %}

AI systems like ChatGPT are undeniably *exciting*, but they also challenge the very essence of how we, humans, learn. These systems excel at tasks that requires years of training to master, such as competitive mathematics or college-level programming [^openaigpt4]. They are also becoming *more accessible*, ready to be used whenever a task poses the slightest bit of difficulty for us. Despite progress in AI, skills such as literacy in both children and adults are declining [^malkus2025aei], raising the question: what aspects of learning do we want technology to cultivate?

We need to struggle in order to develop new skills [^warshauer2015teach][^hiebert2007math]. By “struggle”, we mean the effort students put into understanding a concept and working through challenges to uncover solutions that are not immediately obvious. While we may not enjoy the struggles of learning, struggle teaches persistence and deepens understanding. Our worry is that with AI, we may develop a habit of avoiding struggle, and that habit risks eroding the depth of our knowledge [^dewey2011think].

**How do we preserve meaningful learning in a world where answers are just a prompt away?**



## **The Evaluation Paradox**



Our traditional paradigms for evaluating AI systems often rely on user satisfaction ratings or benchmark assessments — metrics that research has shown to be insufficient for education.  For example, Hiroko Warshauer and James Hiebert show that effective support for struggle  in pedagogical settings requires attention to multiple dimensions, such as the nature of a teacher’s language [^warshauer2015struggle][^hiebert1993math], the design of tasks[^warshauer2015teach1], and the broader learning environment[^yeager2019growth][^noddings1985small][^schoenfeld1985ideas]. Furthermore, studies by Arthur Glenberg and Michael Pressley et al., have shown that students often overestimate their own understanding and may prefer systems that reduce struggle in the short term [^glenberg1982illusion]. How is this paradox between user preference and their learning reflected in our current AI systems?

 In our work on evaluating interactions between humans and AI systems [^lee2023evaluating] —in this case, language models—for information-seeking tasks like  question answering, we also observed a disconnect between users' views of helpfulness and their task performance: the language models which users self-reported as helpful were not always the ones that led to higher task accuracy (**Figure 2**)! This result was attributed to users putting misplaced trust on to the “confident” and “definitive” language generated by certain language models, particularly those with additional fine-tuning [^srivastava2023halie]. Only if these users encountered a confident-sounding answer that was obviously wrong, did their assessment of the language model rapidly decline. On the other hand, a few users working with language models that provided in-direct and lengthier answers trusted that their struggle was intentional and held value, including one participant who stated *“the task may not be as fun if the AI would give you all the answers!”*[^srivastava2023halie1]. 

 {% figure %}

<div style="border: 2px solid white;"><img class="postimage" src="{{ site.baseurl }}/assets/img/posts/2025-01-29-teaching/sailblog_halie.001.jpeg" style="width: 120%; height: auto;"/></div>

<figcaption style="font-size: 14px;">
 <b>Figure 2: </b>We asked users to answer College Chemistry questions from the <a href="https://arxiv.org/abs/2009.03300">MMLU dataset</a> while given access to different language models (LMs) for help. Those interacting with stronger LMs (e.g. instruction-tuned models) received more direct and confident responses than those interacting with weaker LMs, even in the presence of hallucinations. This resulted in a discrepancy between user helpfulness and task performance, suggesting overreliance that can hinder learning.
</figcaption>
{% endfigure %}

 Another domain where this evaluation paradox occurs is rehabilitation technology, where it is crucial for the patient to trust that any robot-assisted therapy (e.g. repetitive movements) will actually lead to long-term improvement [^kellmeyer2018social]. In our work on AI-assisted motor learning [^srivastava2022teaching], we again saw that users self-reported decreased preference for the type of AI-assistance that actually led to improved learning (**Figure 3**) . Our experiment asked participants to learn to control a vehicle in a simulated environment, and we found that our AI-based instruction encouraged participants to learn a new skill of successfully operating the vehicle in reverse. However, participants found reversing uncomfortable and frustrating. In this setting, AI succeeded at helping students learn a new skill, leading to overall task improvement, but failed to inspire student resilience. 


 {% figure %}
<div style="border: 2px solid white;"><img class="postimage" src="{{ site.baseurl }}/assets/img/posts/2025-01-29-teaching/sailblog_motor.gif" style="width: 120%; height: auto;"/></div>
<figcaption style="font-size: 14px;">
  <b>Figure 3: </b>Participants that received  <a href="https://arxiv.org/pdf/2211.14003">personalized AI training</a> in our parking simulator were encouraged to try to learn to reverse – a challenging skill that, when learned, leads to task improvement (e.g. taking less time to park). While participants found  personalized AI training less helpful (left) than the control training curricula, it led to higher task performance (middle) and usage of this skill in evaluation trials (right), showing how user preference is a poor proxy for learning gain. 
</figcaption>
{% endfigure %}

 If student self-reported assessments and engagement isn’t necessarily reflective of learning, what then does good teaching look like when students are struggling? Can we help teachers guide their students through productive struggle with AI? 

## **Fostering Productive Struggle by Empowering Teachers**

While many people can be a teacher—from parents, mentors, tutors to traditional classroom teachers—good teachers are hard to come by. Good teachers gain their expertise through years of training or trial and error, and students who most need experienced teachers often have the least access to them [^hong2022lowincome][^peske2006teaching]. This inequity impacts the quality of their education and the nature of how students struggle. 

One exciting direction is using AI to help human educators create better moments of productive struggle for their students. In earlier research, we observed that novice educators have difficulties helping struggling students, particularly under time pressure [^wang2023bridging]. These educators were not sure how to nudge students and come up with the right thing to say on the spot. Without guidance on how to effectively foster productive struggle, they frequently defaulted to providing the solution to the student. This meant missed opportunities to turn a student’s struggle into meaningful learning!

To address this, we developed Tutor CoPilot [^wang2024tutor], an AI-powered system designed to provide live suggestions to human tutors on how to foster productive struggle (**Figure 4**). Tutor CoPilot is a language model that generates expert-like suggestions on scaffolding the student’s learning, such as asking a guiding question or providing a hint. Unlike generic tools like ChatGPT that risk providing the answer to students or may not be able to engage a student for an entire hour of learning, Tutor CoPilot focuses on amplifying the tutor’s ability to foster productive struggle. 

 {% figure %}
<div><img class="postimage" src="{{ site.baseurl }}/assets/img/posts/2025-01-29-teaching/copilot.gif" style="width: 70%;"/></div>
<figcaption>
  <b>Figure 4:</b> Illustration of Tutor CoPilot. Tutor CoPilot provides real-time suggestions for tutors on how to help struggling students. These suggestions follow expert-informed strategies, like asking a guiding question or providing a hint.
</figcaption>
{% endfigure %}


We tested Tutor CoPilot to provide tutors guidance in a large randomized controlled trial and found that the technology improved student performance on math tests (**Figure 5a**). But, what actually changed in the tutor’s instruction to enable students to learn better? Were the tutors actually fostering productive struggle? When we looked at all the tutors’ language, we found that tutors who had access to Tutor CoPilot were indeed using language that better scaffolded learning and fostered productive struggle, such as prompting students to explain their answers (**Figure 5b**). Tutors who didn’t have access to Tutor CoPilot gave away the answer and solution strategy. By improving how tutors foster productive struggle, students were learning better as a result!

 {% figure %}
<div><img class="postimage" src="{{ site.baseurl }}/assets/img/posts/2025-01-29-teaching/tutorcopilot.001.jpeg" style="width: 100%;"/></div>
<figcaption>
  <b>Figure 5:</b> Tutor CoPilot results. (Left) We found that students working with tutors that had access to Tutor CoPilot were 4 percentage points (p.p.) more likely to pass their math lesson tests. (Right) Tutors with access to Tutor CoPilot used strategies that better fostered productive struggle, whereas tutors who didn’t have access gave away answers and gave generic encouragement to the students.
</figcaption>
{% endfigure %}

There are other exciting approaches that invite us to reimagine AI’s role in engaging real educators and students in productive struggle. For example, work from CU Boulder explores how AI can help students collaborate better with their peers by establishing community agreements [^breideband2023community]. Work from Amplify leverages technology to make classroom learning more social by enabling students to try different ideas, share their math observations and have more meaningful classroom discussions [^meyer2024math]. These examples illustrate how we can support learning by inviting student curiosity and empowering human relationships between students and educators via technology. 
 

## **Conclusion** 

At its heart, learning is so much more than about just finding the right answer: it’s about building resilience, fostering curiosity, and enriching the journey of discovery for students. Despite incredible advances in technology such as AI, human skills – from literacy to fine motor skills – are continuing to decline, with some blaming increased screen time and technology reliance.   In an era where AI can deliver instant solutions and gratification, we must reconsider how to actively preserve the essential aspects of learning. 

We believe AI’s role in education isn’t to eliminate struggle but to enhance it. Whether it’s by empowering educators with tools like Tutor CoPilot, or empowering students with inquiry-driven environments, we wish to ensure that AI supports deeper, more meaningful learning experiences. Let’s build a future where AI systems encourage—not shortcut—meaningful learning from the get-go!


Contact: [rewang@cs.stanford.edu](mailto:rewang@cs.stanford.edu)  and [megha@cs.stanford.edu](mailto:megha@cs.stanford.edu) 


[^openaigpt4]: OpenAI. "GPT-4 Technical Report." ArXiv abs/2303.08774 (2023): n. pag.

[^warshauer2015teach]: Warshauer, H.K. Productive struggle in middle school mathematics classrooms. J Math Teacher Educ 18, 375–400 (2015). https://doi.org/10.1007/s10857-014-9286-3

[^warshauer2015teach1]: Warshauer, H.K. Productive struggle in middle school mathematics classrooms. J Math Teacher Educ 18, 375–400 (2015). https://doi.org/10.1007/s10857-014-9286-3

[^hiebert2007math]: Hiebert, J., & Grouws, D. A. (2007). The Effects of Classroom Mathematics Teaching on Students’ Learning. In F. Lester (Ed.), Second Handbook of Research on Mathematics Teaching and Learning (pp. 371-404). Charlotte, NC: Information Age.

[^dewey2011think]: Dewey, John. How We Think. D. C. HEATH & CO., PUBLISHERS, 1910, www.gutenberg.org/files/37423/37423-h/37423-h.htm.

[^malkus2025aei]: Malkus, Nat. Testing Theories of Why: Four Keys to Interpreting US Student Achievement Trends. American Enterprise Institute, https://www.aei.org/research-products/report/testing-theories-of-why-four-keys-to-interpreting-us-student-achievement-trends/

[^warshauer2015struggle]: Warshauer, Hiroko K. "Strategies to Support Productive Struggle." Mathematics Teaching in the Middle School, vol. 20, no. 7, National Council of Teachers of Mathematics, Mar. 2015, pp. 390-393. JSTOR, https://www.jstor.org/stable/10.5951/mathteacmiddscho.20.7.0390.

[^hiebert1993math]: Hiebert, James, and Diana Wearne. "Instructional Tasks, Classroom Discourse, and Students’ Learning in Second-Grade Arithmetic." American Educational Research Journal, vol. 30, no. 2, Summer 1993, pp. 393-425.

[^yeager2019growth]:Yeager, David S., et al. "A National Experiment Reveals Where a Growth Mindset Improves Achievement." Nature, vol. 573, 2019, pp. 364-369.

[^noddings1985small]:Noddings, Nel. "Small Groups as a Setting for Research on Mathematical Problem Solving." Teaching and Learning Mathematical Problem Solving: Multiple Research Perspectives, edited by Edward A. Silver, 1st ed., Routledge, 1985.

[^schoenfeld1985ideas]: Schoenfeld, Alan H. "Ideas in the Air: Speculations on Small Group Learning, Environmental and Cultural Influences on Cognition, and Epistemology." Teaching and Learning Mathematical Problem Solving: Multiple Research Perspectives, edited by Edward A. Silver, Routledge, 1985.

[^glenberg1982illusion]: Glenberg, Arthur M., Alex Cherry Wilkinson, and William Epstein. "The Illusion of Knowing: Failure in the Self-Assessment of Comprehension." Memory & Cognition, vol. 10, 1982, pp. 597-602.

[^srivastava2022teaching]: Srivastava, Megha et al. \"Assistive Teaching of Motor Control Tasks to Humans\" Advances in Neural Information Processing Systems 36 (2022). https://arxiv.org/pdf/2211.14003

[^wang2023bridging]: Wang, Rose E., et al. "Bridging the Novice-Expert Gap via Models of Decision-Making: A Case Study on Remediating Math Mistakes." arXiv, 6 Apr. 2024, https://arxiv.org/abs/2310.10648.

[^hong2022lowincome]: Hong, Joe, and Erica Yee. "Low-Income Students Are More Likely to Be in Classrooms with Underqualified Teachers." KQED, 21 July 2022, https://www.kqed.org.

[^peske2006teaching]: Peske, Heather G., and Kati Haycock. Teaching Inequality: How Poor and Minority Students Are Shortchanged on Teacher Quality. The Education Trust, 2006.

[^wang2024tutor]: Wang, Rose E., et al. "Tutor CoPilot: A Human-AI Approach for Scaling Real-Time Expertise." arXiv, 26 Jan. 2025, https://arxiv.org/abs/2410.03017.

[^kellmeyer2018social]: Kellmeyer, Philipp, et al. "Social Robots in Rehabilitation: A Question of Trust." Science Robotics, vol. 3, no. 21, 22 Aug. 2018, DOI: 10.1126/scirobotics.aat1587.

[^lee2023evaluating]: Lee, Mina, and Megha Srivastava and Amelia Hardy and John Thickstun et al. "Evaluating Human-Language Model Interaction." Transactions on Machine Learning Research, vol. 9, 2023.

[^srivastava2023halie]: Srivastava, Megha, and John Thickstun. "Observations from HALIE: A Closer Look at Human-LM Interactions in Information-Seeking Contexts." Center for Research on Foundation Models Blog, Stanford University, 2023.

[^srivastava2023halie1]: Srivastava, Megha, and John Thickstun. "Observations from HALIE: A Closer Look at Human-LM Interactions in Information-Seeking Contexts." Center for Research on Foundation Models Blog, Stanford University, 2023.

[^breideband2023community]: Breideband, Thomas, et al. "The Community Builder (CoBi): Helping Students to Develop Better Small Group Collaborative Learning Skills." CSCW '23 Companion: Companion Publication of the 2023 Conference on Computer Supported Cooperative Work and Social Computing, 2023.

[^hendrycks2021mmlu]: Hendrycks, Dan, et al. "Measuring Massive Multitask Language Understanding." International Conference on Learning Representations (ICLR), 2021, arXiv:2009.03300.

[^meyer2024math]: Meyer, Dan. "The Math Kids Most Want to Learn." Dan Meyer Blog, 19 June 2024.

[^openvla]: Kim, Moo Jin, et al. “OpenVLA: An Open-Source Vision-Language-Action Model.” arXiv preprint arXiv:2406.09246 (2024). 

[^prism]: Karamcheti, Siddharth, et al. "Prismatic vlms: Investigating the design space of visually-conditioned language models." arXiv preprint arXiv:2402.07865 (2024). 

[^evla]: Budzianowski, Paweł, et al. “EdgeVLA: Efficient Vision-Language-Action Models.” 
 
[^libero]: Liu, Bo, et al. "Libero: Benchmarking knowledge transfer for lifelong robot learning." Advances in Neural Information Processing Systems 36 (2024). 

[^vq]: Lee, Seungjae, et al. "Behavior generation with latent actions." arXiv preprint arXiv:2403.03181 (2024).  

[^bridge]: Walke, Homer Rich, et al. "Bridgedata v2: A dataset for robot learning at scale." Conference on Robot Learning. PMLR, 2023\.  

[^simpler]: Li, Xuanlin, et al. "Evaluating Real-World Robot Manipulation Policies in Simulation." arXiv preprint arXiv:2405.05941 (2024).  

[^octo]: Team, Octo Model, et al. "Octo: An open-source generalist robot policy." arXiv preprint arXiv:2405.12213 (2024).

[^baku]: Haldar, Siddhant, Zhuoran Peng, and Lerrel Pinto. "BAKU: An Efficient Transformer for Multi-Task Policy Learning." arXiv preprint arXiv:2406.07539 (2024).


