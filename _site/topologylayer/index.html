<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <!-- Main JS (navbar.js, katex_init.js and masonry_init.js)-->
    <script defer=true src="/blog/assets/js/main.min.js"></script>
    
    <!-- CSS -->
    <link rel="stylesheet" href="/blog/assets/css/main.css">

    <!--Favicon-->
    <link rel="icon" type="image/png" href="/blog/assets/img/favicon-32x32.png" sizes="32x32" />
    <link rel="icon" type="image/png" href="/blog/assets/img/favicon-16x16.png" sizes="16x16" />

    <!-- Canonical -->
    <link rel="canonical" href="http://0.0.0.0:4000/blog/topologylayer/">

    <!-- RSS -->
    <link rel="alternate" type="application/atom+xml" title="The Stanford AI Lab Blog" href="http://0.0.0.0:4000/blog/feed.xml"/>

    <!-- Font Awesome -->
    <!-- <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet"> -->
    <link rel="stylesheet" type="text/css" href="/blog/assets/css/vendor/font-awesome.min.css">
    
    <!-- Bootstrap-3.3.7 isolation CSS -->
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" integrity="sha384-JcKb8q3iqJ61gNV9KGb8thSsNjpSL0n8PARn9HuZOnIxN0hoP+VmmDGMN5t9UJ0Z" crossorigin="anonymous">


    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js" integrity="sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha384-B4gt1jrGC7Jh4AgTPSdUtOBvfO8shuf57BaghqFfPlYxofvL8/KUEfYiJOMMV+rV" crossorigin="anonymous"></script>
    
    <!-- JQuery 2.2.4 -->
    <script src="https://code.jquery.com/jquery-2.2.4.min.js" integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin="anonymous"></script>
    
    <link rel="stylesheet" type="text/css" href="/blog/assets/css/vendor/bigfoot-number.css">
    <script type="text/javascript" src="/blog/assets/js/vendor/bigfoot.min.js"></script>
    <script type="text/javascript">
    $(document).ready(function() { 
      $.bigfoot();
      
    window.onload = function() {

        var videos = document.getElementsByTagName("video"),
            fraction = 0.8;

        function checkScroll() {

            for (var i = 0; i < videos.length; i++) {

                var video = videos[i];

                var x = video.offsetLeft,
                    y = video.offsetTop,
                    w = video.offsetWidth,
                    h = video.offsetHeight,
                    r = x + w, //right
                    b = y + h, //bottom
                    visibleX, visibleY, visible;

                visibleX = Math.max(0, Math.min(w, window.pageXOffset + window.innerWidth - x, r - window.pageXOffset));
                visibleY = Math.max(0, Math.min(h, window.pageYOffset + window.innerHeight - y, b - window.pageYOffset));

                visible = visibleX * visibleY / (w * h);

                if (visible > fraction) {
                    video.play();
                } else {
                    video.pause();
                }

            }

        }

        window.addEventListener('scroll', checkScroll, false);
        window.addEventListener('resize', checkScroll, false);
    }
    }); 
    </script>
    


    <!-- Google Fonts -->
    
    <link href="//fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,700italic,400italic" rel="stylesheet" type="text/css"> 
    

    <!-- KaTeX 0.8.3 -->
    
    <link rel="stylesheet" type="text/css" href="/blog/assets/css/vendor/katex.min.css">
    <script src="/blog/assets/js/vendor/katex.min.js">
    </script>
    
    
    
    <!-- Google Analytics -->
    
    <script>
        (function(i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r;
            i[r] = i[r] || function() {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date();
            a = s.createElement(o),
                m = s.getElementsByTagName(o)[0];
            a.async = 1;
            a.src = g;
            m.parentNode.insertBefore(a, m)
        })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'UA-129018108-1', 'auto');
        ga('send', 'pageview');

    </script>
    
    
    <!-- seo tags -->
    <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>A Topology Layer for Machine Learning | SAIL Blog</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="A Topology Layer for Machine Learning" />
<meta name="author" content="<a href='http://www.bruel.org'>Rickard Brüel Gabrielsson</a>" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="We often use machine learning to try to uncover patterns in data. In order for those patterns to be useful they should be meaningful and express some underlying structure. Geometry deals with such structure, and in machine learning we especially leverage local geometry. This can be seen in the Euclidean-inspired loss functions we use for generative models as well as for regularization. However, global geometry, which is the focus of Topology, also deals with meaningful structure, the only difference being that the structure is global instead of local. Topology is at present less exploited in machine learning, which is also why it is important to make it more available to the machine learning community at large." />
<meta property="og:description" content="We often use machine learning to try to uncover patterns in data. In order for those patterns to be useful they should be meaningful and express some underlying structure. Geometry deals with such structure, and in machine learning we especially leverage local geometry. This can be seen in the Euclidean-inspired loss functions we use for generative models as well as for regularization. However, global geometry, which is the focus of Topology, also deals with meaningful structure, the only difference being that the structure is global instead of local. Topology is at present less exploited in machine learning, which is also why it is important to make it more available to the machine learning community at large." />
<link rel="canonical" href="http://0.0.0.0:4000/blog/topologylayer/" />
<meta property="og:url" content="http://0.0.0.0:4000/blog/topologylayer/" />
<meta property="og:site_name" content="SAIL Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-08-23T00:00:00-07:00" />
<script type="application/ld+json">
{"description":"We often use machine learning to try to uncover patterns in data. In order for those patterns to be useful they should be meaningful and express some underlying structure. Geometry deals with such structure, and in machine learning we especially leverage local geometry. This can be seen in the Euclidean-inspired loss functions we use for generative models as well as for regularization. However, global geometry, which is the focus of Topology, also deals with meaningful structure, the only difference being that the structure is global instead of local. Topology is at present less exploited in machine learning, which is also why it is important to make it more available to the machine learning community at large.","author":{"@type":"Person","name":"<a href='http://www.bruel.org'>Rickard Brüel Gabrielsson</a>"},"@type":"BlogPosting","url":"http://0.0.0.0:4000/blog/topologylayer/","headline":"A Topology Layer for Machine Learning","dateModified":"2019-08-23T00:00:00-07:00","datePublished":"2019-08-23T00:00:00-07:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://0.0.0.0:4000/blog/topologylayer/"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    
    <!-- Manual seo tags -->
    <title>A Topology Layer for Machine Learning | The Stanford AI Lab Blog</title>
    <meta name="description" content="Topology is a combinatorial property that is tricky to utilize in gradient based methods, but it is also a useful and underexploited feature of data. We pres...">
    
    <!-- Twitter Cards -->
    <meta name="twitter:title" content="A Topology Layer for Machine Learning">
    
    <meta name="twitter:description" content="Topology is a combinatorial property that is tricky to utilize in gradient based methods, but it is also a useful and underexploited feature of data. We present an easy-to-use TopologyLayer that allows for backpropagation through a loss based on Persistent Homology.">
    
    <meta name="twitter:creator" content="@StanfordAILab">
    
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:image" content="http://0.0.0.0:4000/blog/assets/img/posts/2019-08-23-topologylayer/examples.png">
    <meta name="og:image" content="http://0.0.0.0:4000/blog/assets/img/posts/2019-08-23-topologylayer/examples.png">
    
</head>

  <body class="post-body">
      <!-- Toggle menu -->
<header class="site-header">

<nav class="clear navbar navbar-expand-lg navbar-light bg-white flex-column flex-md-row bd-navbar fixed-top" id="main_nav">
  
  <div class="container">

    <a class="navbar-brand mr-0 mr-md-2 text-black d-flex align-items-center" href="/blog/" aria-label="Bootstrap">
    
	  <div class="branding">
	    <a href="http://ai.stanford.edu/">
	      <img class="avatar" src="/blog/assets/img/sail-logo.png" alt=""/>
		  </a>

      <a href="/blog/">
	      <h1 class="site-title">
			    The Stanford AI Lab Blog
		    </h1> 
		  </a>
	  </div>
    
    
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
    </button>
    
    <div class="collapse navbar-collapse" id="navbarResponsive">

      <ul class="navbar-nav ml-auto">
      
      
      <li class="nav-item">
      <a class="nav-link" href="/blog/about">About</a>
      </li>
      
      <li class="nav-item dropdown">
        <a class="nav-link dropdown-toggle" data-toggle="dropdown" href="#" role="button" aria-haspopup="true" aria-expanded="false">Posts</a>
        <div class="dropdown-menu">
        <a class="dropdown-item" href="/blog/">All</a>
        <a class="dropdown-item" href="/blog/conferences">Conferences</a>
        <a class="dropdown-item" href="/blog/vision">Computer Vision</a>
        <a class="dropdown-item" href="/blog/robotics">Robotics</a>
        <a class="dropdown-item" href="/blog/nlp">NLP</a>
        <a class="dropdown-item" href="/blog/ml">Machine Learning</a>
        <a class="dropdown-item" href="/blog/rl">Reinforcement Learning</a>

        </div>
      </li>
        
      
      <li class="nav-item">
      <a class="nav-link" href="/blog/subscribe">Subscribe</a>
      </li>
      
      
      <li class="nav-item">
      <a class="nav-link" href="http://ai.stanford.edu/">SAIL</a>
      </li>
      
      </ul> 

    </div>

  </div>
</nav>

</header>

  
    <div class="content">
      

<article>

  <header id="main">
    
    <h1 id="post_title">A Topology Layer for Machine Learning</h1>
    <p class="meta">
    <a href='http://www.bruel.org'>Rickard Brüel Gabrielsson</a>
    <div class="post-date">August 23, 2019</div>
    </p>
  <hr>
  </header>


  <section class="post-content">
  
    <p>We often use machine learning to try to uncover patterns in data. In order for those patterns to be useful they should be meaningful and express some underlying structure. Geometry deals with such structure, and in machine learning we especially leverage <em>local geometry</em>. This can be seen in the Euclidean-inspired loss functions we use for generative models as well as for regularization. However, <em>global geometry</em>, which is the focus of <a target="_blank" a_="" href="https://en.wikipedia.org/wiki/Topology">Topology</a>, also deals with meaningful structure, the only difference being that the structure is global instead of local. Topology is at present less exploited in machine learning, which is also why it is important to make it more available to the machine learning community at large.</p>

<p>Still, topology applied to real world data using persistent homology has started to find applications within machine learning (including deep learning), but again, compared to its sibling <em>local geometry</em>, it is heavily underrepresented in these domains. In this post, we provide a high-level description of how our <a style="text-decoration: underline; font-weight: bold" target="_blank" a_="" href="https://github.com/bruel-gabrielsson/TopologyLayer">TopologyLayer</a> allows (in just a few lines of PyTorch) for backpropagation through <a target="_blank" a_="" href="https://en.wikipedia.org/wiki/Persistent_homology">Persistent Homology</a> computations and provides instructive, novel, and useful applications within machine learning and deep learning.</p>

<p>As a teaser, consider Figure 1 below. We will show how, in just a few lines of code and a few iterations of SGD, we can define a topology loss and make a generator go from outputting images such as those on the left hand side to those on the right hand side, improving the topological fidelity.</p>

<figure class="figure"><div class="figure__main">
<div style="text-align: center; padding: 10px">
  <img src="/blog/assets/img/posts/2019-08-23-topologylayer/costdig.png" width="700" />
</div>
<figcaption>
Figure 1: Left: Before training with topology loss. Right: after training with topology loss.
</figcaption>
</div></figure>
<p><br /></p>

<h2 id="the-gist">The Gist</h2>

<p>Many of us have seen the continuous deformation of a mug into a donut used to explain topology, and indeed, topology is the study of geometric properties that are preserved under continuous deformation. Such properties include number of connected components, number of rings or holes, and the number of voids.</p>

<figure class="figure"><div class="figure__main">
<div style="text-align: center"><img src="/blog/assets/img/posts/2019-08-23-topologylayer/Mug_and_Torus_morph.gif" width="500" /></div>
<figcaption>
Figure 2: Continuous deformation of mug into a donut/torus (source: Wikipedia)
</figcaption>
</div></figure>
<p><br /></p>

<p>However, in many real world situations, data doesn’t come with an immediate sense of connectivity and neighborhood, and seeing every data point as merely its own connected component is not very interesting. Persistent homology was introduced to study topological properties under a continuously growing range (<script type="math/tex">\epsilon \geq 0</script>) of estimates of neighborhoods around the points, such that points are considered connected if their neighborhoods intersect. Under this growing estimate the topology of the space changes, and persistent homology provides us with a <a target="_blank" a_="" href="https://en.wikipedia.org/wiki/Topological_data_analysis#/media/File:Illustration_of_Typical_Workflow_in_TDA.jpeg">Persistence Diagram</a> that shows when topological features appear (birth time) and disappear (death time). This gives us an informative overview of the topology of the data under different <em>perspectives</em>, and the ability to naturally consider those topological features that are present under a greater range (with a greater lifetime) of our estimate of neighborhood as more significant.</p>

<figure class="figure"><div class="figure__main">
<div style="text-align: center; padding: 10px"><img src="/blog/assets/img/posts/2019-08-23-topologylayer/phgif.gif" width="500" /></div>
<figcaption>
Figure 3: Continuous range of estimates for neighborhoods (source: <a target="_blank" a_="" href="https://github.com/smu160/Persistent-Homology">Link</a>)
</figcaption>
</div></figure>
<p><br />
<!-- $$P(R\mid\xi_1,\ldots,\xi_n) \propto \exp\left(\sum_{i=1}^n R(\xi_i))\right)$$ --></p>

<p>In many situations, it is possible to establish an invertible map from the birth and death time of a topological feature to a pair of points in the data. This map allows us to backpropagate from a loss function on the persistence diagram (the list of topological features with their birth and death times) to the underlying data. In doing so, we can use gradient descent to change the data to minimize our loss and encourage a wide array of topological structures expressed via our loss function. This includes a diverse set of structures, with some showcased in Figure 4 below, where we start with a random collection of points (top center) and use SGD to encourage specific topological features.</p>

<figure class="figure"><div class="figure__main">
<div style="text-align: center; padding: 10px"><img src="/blog/assets/img/posts/2019-08-23-topologylayer/examples2.png" width="800" /></div>
<figcaption>
Figure 4: Some different topological structures that can be encouraged.
</figcaption>
</div></figure>

<h2 id="some-details">Some Details…</h2>

<p>As mentioned, above we described distance-based filtrations (ie filtering operations over connections based on distance), but other filtrations that are also very useful include <em>level set filtrations</em>. Instead of thinking about neighborhoods as growing balls around each point, level set filtrations consider all components that are below (for <em>sublevel</em> filtrations) a growing threshold as ‘connected’. If Figure 5 makes sense to you and you don’t yearn for a more precise deposition, you may skip ahead to the next section and look at the results.</p>

<p>Sounds iffy? Ok, let me provide a little mathematical rigor. In fact, persistent homology is a very general framework that can compute topological features from many different perspectives and on many different spaces – which may be very different from what might appear most natural to us on data in an euclidean space. If you’re interested in getting the complete picture check out this <a target="_blank" a_="" href="https://www.maths.ed.ac.uk/~v1ranick/papers/edelcomp.pdf">book</a>.</p>

<p>We will consider geometric <a target="_blank" a_="" href="https://en.wikipedia.org/wiki/Simplicial_complex">simplicial complexes</a>, where the vertices correspond to points in some ambient space, e.g. <script type="math/tex">\mathbb{R}^d</script>, although the simplices need not be embedded in the space. Persistent homology studies an increasing sequence of simplicial complexes, or a <em>filtration</em>, <script type="math/tex">\emptyset = \mathcal{X}_0 \subset \mathcal{X}_1 \subset ... \subset \mathcal{X}_0 = \mathcal{X}</script>. We consider sublevel set filtrations of a function <script type="math/tex">f : \mathcal{X} \rightarrow \mathbb{R}</script>. The filtration is defined by increasing the parameter <script type="math/tex">\alpha</script>, with <script type="math/tex">\mathcal{X}_{ \alpha } = f^{-1} (- \infty, \alpha]</script>. A <script type="math/tex">k</script>-dimensional persistence diagram, <script type="math/tex">PD_k</script>, is a multi-set of points in <script type="math/tex">\mathbb{R}^2</script>. Each point, <script type="math/tex">(b, d)</script> represents a <script type="math/tex">k</script>-dimensional topological feature which appears when <script type="math/tex">\alpha = b</script> and disappears when <script type="math/tex">\alpha = d</script>. These are called the the birth time and death time respectively. Alternatively, we can view the persistence diagram as a map from a
filtration to a set of points in <script type="math/tex">\mathbb{R}^2</script>:</p>

<script type="math/tex; mode=display">PD_{k} : (\mathcal{X}, f) \rightarrow \{ b_i, d_i \}_{i \in \mathcal{I}_k}</script>

<p>As a notational convenience, we assume that the indexing of the points is by decreasing lifetimes, i.e. <script type="math/tex">d_i - b_i \geq d_j - b_j</script> for <script type="math/tex">i>j</script>. An intuitive way to understand this machinery is to consider a filtration where simplices are added one at a time. It is a standard but non-obvious result that a
<script type="math/tex">k</script>-dimensional simplex either creates a <script type="math/tex">k</script>-dimensional feature or destroys a <script type="math/tex">(k-1)</script>-dimensional feature. The persistence diagram captures the pairing of these events which are represented by a pair of simplices <script type="math/tex">(\sigma, \tau)</script>, where <script type="math/tex">b=f(\sigma)</script> and <script type="math/tex">d=f(\tau)</script>. This allows us to define an inverse map:</p>

<script type="math/tex; mode=display">\pi_{f}(k) : \{ b_i, d_i \}_{i \in \mathcal{I}_k} \rightarrow (\sigma, \tau)</script>

<p>As persistence diagrams are a collection of points in <script type="math/tex">\mathbb{R}^2</script>, there are many notions of distances between diagrams and loss functions on diagrams which depend on the points. We will use loss functions that can be expressed in terms of three parameters:</p>

<script type="math/tex; mode=display">\mathcal{E}(p,q,i_0; PD) = \sum_{i=i_0}^{\infty} \mid d_i-b_i\mid^p (\frac{d_i+b_i}{2})^q</script>

<p>We sum over lifetimes beginning with the <script type="math/tex">i_0</script> most persistent point in the diagram. For example, if <script type="math/tex">i_0 = 2</script>, we consider all but the most persistent class. We also use the Wasserstein distance between diagrams – this is defined as the optimal transport distance between the points of the two diagrams. One
technicality is that the two diagrams may have different cardinalities, which is why points may be mapped to the diagonal.</p>

<p>We use two different types of filtrations: (1) a sub/superlevel set filtration where a function is defined on a fixed simplicial complex <script type="math/tex">\mathcal{X}</script> (Check out Figure 5), and (2) a distance-based filtration whose input are points embedded in some ambient space. We refer to (1) as level set persistence.  As an example,  we consider images where superlevel set filtrations are more natural. The underlying complex is the collection of pixels and the function is given by the pixel values, i.e. the superlevel set are all pixels whose value is greater than some <script type="math/tex">\alpha</script>. If we represent each pixel by a vertex and triangulate <script type="math/tex">\mathbb{R}^2</script>, the value of a simplex is given by the minimum of pixel values of vertices in the simplex. This defines a map
<script type="math/tex">\omega_{ls}(\sigma) =\mathrm{argmin}_{v\in\sigma} f(v)</script> from each simplex to a vertex/pixel. Composing with <script type="math/tex">\pi_f</script>, we obtain a map from a point in the diagram to a pair of pixels – evaluating the gradients at these pixels gives the gradient with respect to the diagram via the chain rule.</p>

<figure class="figure"><div class="figure__main">
<div style="text-align: center; padding: 10px"><img src="/blog/assets/img/posts/2019-08-23-topologylayer/sub.png" width="800" /></div>
<p>Figure 5: A one dimensional example of a persistence diagram and the inverse map <script type="math/tex">\pi</script>. The function on the left has critical points at points <script type="math/tex">p</script>, <script type="math/tex">r</script> and <script type="math/tex">q</script>. The local minima create components in the sub-level sets and so represent birth times (<script type="math/tex">x</script>-axis), while the maxima kills one of the components (the younger one) and so is a death time (<script type="math/tex">y</script>-axis). The inverse map for a point in the diagram returns the corresponding critical points/simplicies.</p>
</div></figure>

<p>In our other scenario, the input consists of points in <script type="math/tex">\mathbf{R}^d</script>. One construction for this situation is the Rips filtration, <script type="math/tex">\mathcal{R}_\alpha</script>. A Vietoris-Rips complex is constructed in two steps. First, connect all pairs of points <script type="math/tex">(x,y)</script> if <script type="math/tex">% <![CDATA[
\mid \mid x-y \mid \mid < \alpha %]]></script>. Then take the resulting graph and construct the <em>clique complex</em> by filling in all possible simplices, which correspond to cliques in the graph. In this setting, the filtration function is defined as <script type="math/tex">f(\sigma) = \max_{(v,w)\in\sigma} \mid \mid v-w \mid \mid</script> and the corresponding inverse map is <script type="math/tex">\omega_{\mathcal{R}} (\sigma) = \mathrm{argmax}_{(v,w)\in\sigma} \mid \mid v-w \mid \mid</script>. This relies on the points being embedded – extending this definition to a general metric space would require additional work. Again composing with <script type="math/tex">\pi_f</script> gives potentially four points and the gradient can be evaluated at those four points. The Rips filtration can often become too large to compute efficiently. Rather than connect all pairs of points which are sufficiently close, we take as the graph a subset of the Delaunay graph. We refer to this as the <em>weak Alpha filtration</em>. With the maps defined the derivation of the gradient is straightforward application of the chain rule.</p>

<p>For example, in Figure 4, we used weak Alpha filtration with loss functions (a): <script type="math/tex">- \mathcal{E}(2,0,2; PD_0)</script>, (b): <script type="math/tex">\mathcal{E}(2,0,2; PD_0)</script>, (c): <script type="math/tex">- \mathcal{E}(2,0,1; PD_1)</script>, (d): <script type="math/tex">- \mathcal{E}(2,1,1; PD_1) + \mathcal{E}(2,0,2; PD_0)</script>, and (d): <script type="math/tex">\mathcal{E}(2,0,1; PD_1)</script>. Similarly, we can use superlevel set filtration to denoise an image of a MNIST digit, where we encourage one global maximum via loss function <script type="math/tex">\mathcal{E}(1,0,2; PD_0)</script>, the result can be seen below in Figure 6.</p>

<figure class="figure"><div class="figure__main">
<div style="text-align: center; padding: 10px">
  <img src="/blog/assets/img/posts/2019-08-23-topologylayer/gen2.png" width="700" />
</div>
<figcaption>
Figure 6: Denoising the number of connected component of a MNIST image. (a): Image before minimizing topology loss, with multiple local optima. (b): Image after minimizing topology loss, with a single local optimum.
</figcaption>
</div></figure>

<h2 id="what-we-can-do">What we can do!</h2>

<p>When it comes to machine learning, topology is not as ubiquitous as local geometry, but in almost all cases where local geometry is useful so is topology. However, topology is harder to wrap your head around. We will describe applications in three domains where our <a style="text-decoration: underline; font-weight: bold" target="_blank" a_="" href="https://github.com/bruel-gabrielsson/TopologyLayer">TopologyLayer</a> makes leveraging topology easy peasy.</p>

<h4 id="topology-priors-in-regularization">Topology Priors in Regularization</h4>

<p>The following examples demonstrate how topological information can be incorporated effectively to add regularization or incorporate prior knowledge into problems. Furthermore, they demonstrate how topological information can be directly encoded, such as penalties on the number of clusters or number of maxima of a function, in a natural way that is difficult to accomplish with more traditional schemes.</p>

<p>Regularization is used throughout machine learning to prevent over-fitting, or to solve ill-posed problems. In a typical problem, we observe data <script type="math/tex">\{X_i\}</script> and responses <script type="math/tex">\{y_i\}</script>, and we would like to fit a predictive model with parameters <script type="math/tex">\hat{\beta}</script> that will allow us to make a prediction <script type="math/tex">\hat{y}_i = f(\hat{\beta}; X_i)</script> for each observation. The quality of the model is assessed by a loss function <script type="math/tex">\ell</script>, such as the mean squared error. However, many models are prone to <em>over-fitting to training data or are ill-posed if there are more unknown parameters than observations</em>. In both these cases, adding a regularization term <script type="math/tex">P(\beta)</script> can be beneficial. The estimated value of <script type="math/tex">\hat{\beta}</script> for the model becomes:</p>

<script type="math/tex; mode=display">\hat{\beta} = argmin_{\beta} \sum_{i=1}^n \ell \big(y_i, f(\beta; X_i)\big) + \lambda P(\beta) \ \ \ \ \ \ \ \ \ \text{(1)}</script>

<p>where <script type="math/tex">\lambda</script> is a free tuning parameter. We compare some common regularization to two topological regularizations <em>Top1</em> and <em>Top2</em> which stands for <script type="math/tex">\mathcal{E} (1,0,2;PD_0)</script> (or <script type="math/tex">\sum_{i=2}^{\infty} \mid  d_i - b_i \mid</script> over <script type="math/tex">PD_0</script>) and <script type="math/tex">\mathcal{E} (1,0,4;PD_0)</script> (or <script type="math/tex">\sum_{i=4}^{\infty} \mid d_i-b_i \mid</script> over <script type="math/tex">PD_0</script>) respectively. Top1 encodes that we want to kill off all connected components other than the most persistent component, while Top2 encodes that we want to kill off all connected components other than the <em>three</em> most persistent components. Figure 7 shows a table and definitions of all regularization terms.</p>

<figure class="figure"><div class="figure__main">
<div style="text-align: center; padding: 10px">
  <img src="/blog/assets/img/posts/2019-08-23-topologylayer/tablereg.png" width="500" />
</div>
<figcaption>
Figure 7: Defintions of regularization terms
</figcaption>
</div></figure>
<p><br /></p>

<p>In Figures 6 and 7, we compare different regularization schemes for several different linear regression problems. Data is generated as <script type="math/tex">y_i = X_i\beta_\ast + \epsilon_i</script>, where <script type="math/tex">X_i\sim N(0,I)</script>, and <script type="math/tex">\epsilon_i\sim N(0,0.05)</script>. <script type="math/tex">\beta_\ast</script> is a feature vector with <script type="math/tex">p=100</script> features, and an estimate <script type="math/tex">\hat{\beta}</script> is made from <script type="math/tex">n</script> samples by solving Equation (1) with the mean-squared error loss <script type="math/tex">\ell\big(y_i, f(\beta; X_i)\big) = (y_i - X_i \beta)^2</script> using different penalties, and <script type="math/tex">\lambda</script> is chosen from a logarithmically spaced grid on <script type="math/tex">[10^{-4},10^1]</script> via cross-validation for each penalty. We track the mean-squared prediction error for the estimate <script type="math/tex">\hat{\beta}</script> as the number of samples is increased. We also compare to the ordinary least-squares solution, with no regularization term, although if the solution is under-determined <script type="math/tex">% <![CDATA[
(n < p) %]]></script>, we take the smallest 2-norm solution.</p>

<p>In Figure 8, the features in <script type="math/tex">\beta_\ast</script> are chosen uniformly at random from three different values. On the left, those values are <script type="math/tex">\{-1,0,1\}</script>, and on the right, <script type="math/tex">\{1, 2, 3\}</script>.  We consider <script type="math/tex">L_1</script> and <script type="math/tex">L_2</script> penalties, as well as two topological penalties (Top1 and Top2) using a weak-alpha filtration.</p>

<figure class="figure"><div class="figure__main">
<div style="text-align: center; padding: 10px">
  <img src="/blog/assets/img/posts/2019-08-23-topologylayer/regalpha.png" width="700" />
  </div>
<p>Figure 8: MSE (mean squared error) of <script type="math/tex">\hat{\beta}</script> obtained using several regularization schemes as size of training set increases.  Left: entries of <script type="math/tex">\beta_\ast</script> are drawn i.i.d. from <script type="math/tex">\{-1,0,1\}</script>. Right: entries of <script type="math/tex">\beta_\ast</script> are drawn i.i.d. from <script type="math/tex">\{1,2,3\}</script>. <script type="math/tex">n</script>: number of samples, <script type="math/tex">p</script>: number of features.</p>

</div></figure>

<p>In Figure 9, the features in <script type="math/tex">\beta_\ast</script> are chosen to have three local maxima when the features are given the line topology: <script type="math/tex">\beta_\ast</script> consists of three piecewise-linear sawteeth.  The total variation penalty <script type="math/tex">P(\beta) = \sum_{i=1}^p \mid \beta_{i+1} - \beta_i \mid</script> and a smooth variant <script type="math/tex">P(\beta) = (\sum_{i=1}^p \mid \beta_{i+1} - \beta_i \mid^2)^{1/2}</script> are considered, as well as two topological penalties (Top1 and Top2). The parameters of the topological penalties are identical to the previous example, but the penalties are now imposed on superlevel set diagrams of <script type="math/tex">\beta</script>.  This means that instead of penalizing the number of clusters in the weights of <script type="math/tex">\beta</script>, we now penalize the number of local maxima.</p>

<figure class="figure"><div class="figure__main">
<div style="text-align: center; padding: 10px">
  <img src="/blog/assets/img/posts/2019-08-23-topologylayer/regsaw.png" width="700" />
</div>
<p>Figure 9: Sawtooth <script type="math/tex">\beta_\ast</script>. MSE (mean squared error) of linear prediction using <script type="math/tex">\hat{\beta}</script> obtained from several regularization schemes as size of training set increases. <script type="math/tex">n</script>: number of samples, <script type="math/tex">p</script>: number of features.</p>
</div></figure>

<p>These examples show that useful topological priors exist already in basic machine learning settings and how our <a style="text-decoration: underline; font-weight: bold" target="_blank" a_="" href="https://github.com/bruel-gabrielsson/TopologyLayer">TopologyLayer</a> can easily incorporate such priors to good use.</p>

<h4 id="topology-priors-for-generative-models">Topology Priors for Generative Models</h4>

<p>We now use the same topological priors to improve the quality of a deep generative neural network. Specifically, we want to improve its topological fidelity and the right number of local maxima. We start with a Baseline-Generator, pre-trained in a GAN-setup on MNIST, and by training it for
a few iterations (only 50 batch-iterations to be exact) with a topological loss, we arrive at an improved Topology-Generator. We use the same loss, <script type="math/tex">\mathcal{E}(1,0,2; PD_0)</script> (topology loss), as in the MNIST digit denoising in Figure 6. The setup looks as in Figure 10 and the qualitative results can be seen in Figure 11.</p>

<figure class="figure"><div class="figure__main">
<div style="text-align: center; padding: 10px">
  <img src="/blog/assets/img/posts/2019-08-23-topologylayer/costsetup2.png" width="800" />
</div>
<p>Figure 10: Setup for training generator with topology loss. <script type="math/tex">Loss(Dgm) = \mathcal{E}(1,0,2; PD_0) =</script> <script type="math/tex">\sum_{i=2}^{\infty} \mid  d_i - b_i \mid</script> over <script type="math/tex">PD_0</script>.</p>
</div></figure>

<figure class="figure"><div class="figure__main">
<div style="text-align: center; padding: 10px">
  <img src="/blog/assets/img/posts/2019-08-23-topologylayer/costdig.png" width="750" />
</div>
<figcaption>
Figure 11: Left: Before training with topology loss. Right: after training with topology loss.
</figcaption>
</div></figure>
<p><br /></p>

<p>The topology loss allows the generator to learn in only 50 batch iterations to produce images with a single connected component and the difference is visually significant. Furthermore, consider the linear interpolation in the latent space of the Baseline-Generator and Topology-Generator in Figure 12. The two different cases behave very differently with respect to the topology. The Baseline-Generator interpolates by letting a disconnected components appear and grow.  The Topology-Generator tries to interpolate by deforming the number without creating disconnected components. This might be most obvious in the interpolation from ‘1’ to ‘4’ (Figure 12, right hand side) where the appended structure of the ‘4’ appears as a disconnected component in the baseline but grows out continuously from the “1” in the topology-aware case.</p>

<figure class="figure"><div class="figure__main">
<div style="text-align: center; padding: 10px">
  <img src="/blog/assets/img/posts/2019-08-23-topologylayer/interpolation.png" width="800" />
</div>
<figcaption>
Figure 12: Bottom rows: Interpolation before training with topology loss. Top rows: Interpolation after training with topology loss.
</figcaption>
</div></figure>
<p><br /></p>

<p>We also quantitatively compare the Baseline-Generator and Topology-Generator to further investigate if any improvements have been made. We use the Minimal Matching Distance (MMD) and Coverage metric as advocated by <sup id="fnref:panos"><a href="#fn:panos" class="footnote">1</a></sup> as well as the <em>Inception score</em><sup id="fnref:inception"><a href="#fn:inception" class="footnote">2</a></sup> (a convolutional neural network with 99% test accuracy on MNIST was used instead of the Inception model). The results can be seen in Figure 13. MMD-Wass and COV-Wass use the same procedure as MMD-L2 and COV-L2 but instead of the L2 distance between images, the 1-Wasserstein distance between the 0-dimensional persistence diagrams of the images was used. The Topology-Generator shows improvements on all but one of these metrics.</p>

<figure class="figure"><div class="figure__main">
<div style="text-align: center; padding: 10px">
  <img src="/blog/assets/img/posts/2019-08-23-topologylayer/tablegen.png" width="900" />
</div>
<figcaption>
Figure 13: Showing metrics for generator evaluation
</figcaption>
</div></figure>
<p><br /></p>

<p>We extend this superlevel set filtration to 3D data in the form of voxel grids. As before, a baseline generator is obtained by training a GAN to generate voxel shapes as <sup id="fnref:3dgan"><a href="#fn:3dgan" class="footnote">3</a></sup> and its output after 1,000 epochs (or 333,000 batch iterations) can be seen in Figure 14 as the left hand members in each of the two pairs. The result of training with the topology loss (same as for images) for 20 batch iterations can be seen in Figure 14 as the right hand members in each of the two pairs. We claim no improvements on general metrics in this case but note that the generator is able to learn to generate output with far fewer connected components.</p>

<figure class="figure"><div class="figure__main">
<div style="text-align: center; padding: 10px">
  <img src="/blog/assets/img/posts/2019-08-23-topologylayer/cost3d2.png" width="800" />
</div>
<figcaption>
Figure 14: Left hand members of the pairs: before training with topology loss. Right hand members of pairs: after training with topology loss for 20 batch iterations.
</figcaption>
</div></figure>

<h4 id="topological-adversarial-attacks">Topological Adversarial Attacks</h4>

<p>Our topological layer may also be placed at the beginning of a deep network. In contrast to other approaches that use persistence features for deep learning, we can use the fact that our input layer is differentiable to perform adversarial attacks, i.e. we want to cause a trained neural network to misclassify input whose class to us is fairly obvious, and we do this by backpropagating from the predictions back to the input image, which is known as a gradient attack.</p>

<p>Since standard super-level set persistence is insufficient to classify MNIST digits, we include the orientation and direction information by computing the persistent homology during 8 directional sweeps. The model (TopModel) trained to classify the digits based on these topological features achieved 80-85% accuracy. Next we performed gradient attack <sup id="fnref:attack"><a href="#fn:attack" class="footnote">4</a></sup> to change the classification of the digit to another target class. We observe that it is harder to train adversarial images compared to CNNs and MLPs. The results are shown in Figure 15. A red outline indicates that the attack was successful. When the attack was conducted on 1,000 images, to retarget to a random class, it had 100% success rate on MLP and CNN models and 25.2% success rate on the TopModel.</p>

<figure class="figure"><div class="figure__main">
<div style="text-align: center; padding: 10px">
  <img src="/blog/assets/img/posts/2019-08-23-topologylayer/attacks.png" width="1000" />
</div>
<p>Figure 15: Topological adversarial attack on TopModel, MLPModel and CNNModel. Each <script type="math/tex">(i,j)</script>-cell with <script type="math/tex">i,j \in \{0, 1, \cdots, 9\}</script> represents an attack on an image with label <script type="math/tex">i</script> to be classified with label <script type="math/tex">j</script>. Red outline indicates successful attack.</p>
</div></figure>

<p>When the adversarial attacks succeed the results sometimes offer insight as to how the model classifies each digit. For example in Figure 16, the left image is the original image of the digit 4, the right was trained to be classified as an 8; notice that two small holes at the top and bottom were sufficient to misclassify the digit. Several instances of the topological attacks provide similar interpretation. Attacks on MLP and CNN are qualitatively different, but further work is needed to gauge the extent and utility of such distinctions.</p>

<figure class="figure"><div class="figure__main">
<div style="text-align: center; padding: 10px">
  <img src="/blog/assets/img/posts/2019-08-23-topologylayer/attack4.png" width="500" />
</div>
<figcaption>
Figure 16: Example of Topological adversarial attack. Left is the original image, right image was optimized to be classified to be an 8, which introduced two 1 pixel holes. This is qualitatively different form attacks on MLP and CNN, because to a topologist the misclassification is completely interpretable as well as in close coherence with persistent homology theory.
</figcaption>
</div></figure>

<h2 id="conclusion">Conclusion</h2>

<p>In this post we have introduced a general framework for incorporating global geometry in the form of topology into machine learning domains. In each of the examples we showcase how <em>global geometry</em>, through our <a style="text-decoration: underline; font-weight: bold" target="_blank" a_="" href="https://github.com/bruel-gabrielsson/TopologyLayer">TopologyLayer</a>, can be used in cases where <em>local geometry</em> is today usually solely relied upon. We present both quantitative and qualitative advantages that can be achieved by incorporating topology into these domains and hope this will inspire the machine learning community at large to embrace global geometry and topology.</p>

<p>This work only scratches the surface of the possible directions leveraging the differentiable properties of persistence. Without doubt such work will tackle problems beyond those we have presented here. Such work could include encouraging topological structure in intermediate activations of deep neural networks or using the layer in the middle of deep networks to extract persistence features where they may be more useful. However, many of the applications we have presented here also deserve further focus. For example, topological regularization, including the penalties we have presented, may have interesting theoretical properties, or closed form solutions. Furthermore, training autoencoders with distances such as the bottleneck or Wasserstein distance between persistence features might produce stronger results than the functions considered here. Finally, it might prove useful to use topological features to train deep networks that are more robust to adversarial attacks – however, as we show this will require additional work.</p>

<p>Topology, in contrast to local geometry, is generally underexploited in machine learning, but changing this could benefit the discipline. Go ahead and install the <a style="text-decoration: underline; font-weight: bold" target="_blank" a_="" href="https://github.com/bruel-gabrielsson/TopologyLayer">TopologyLayer</a>, play around with it, and see for yourself all kind of cool things it can do.</p>

<hr />

<p>Post based on preprint: <a target="_blank" a_="" href="https://arxiv.org/pdf/1905.12200.pdf">A Topology Layer for Machine Learning
</a> by Rickard Brüel-Gabrielsson, Bradley J. Nelson, Anjan Dwaraknath, Primoz Skraba, Leonidas J. Guibas, and Gunnar Carlsson.</p>

<p>Work supported by Altor Equity Partners AB through <a target="_blank" a_="" href="https://unboxai.org">Unbox AI</a> (unboxai.org) and by the US Department of Energy, Contract DE-AC02-76SF00515.</p>

<div class="footnotes">
  <ol>
    <li id="fn:panos">
      <p>Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, and Leonidas Guibas. <em>Learning representations and generative models for 3D point clouds</em>. In Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 40–49, Stockholmsmässan, Stockholm Sweden, 10–15 Jul 2018. <a href="#fnref:panos" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:inception">
      <p>Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. <em>Improved techniques for training gans</em>. In Proceedings of the 30th International Conference on Neural Information Processing Systems, NIPS’16, pages 2234–2242, 2016. <a href="#fnref:inception" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:3dgan">
      <p>Jiajun Wu, Chengkai Zhang, Tianfan Xue, Bill Freeman, and Josh Tenenbaum. <em>Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling</em>. In Advances in neural information processing systems, pages 82–90, 2016. <a href="#fnref:3dgan" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:attack">
      <p>Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. <em>Explaining and harnessing adversarial examples</em>. In International Conference on Learning Representations, 2015. <a href="#fnref:attack" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>

  
  </section>
  <hr>
  Keep on top of the latest SAIL Blog posts via <a class="social-icon" href="http://0.0.0.0:4000/blog/feed.xml">RSS <i class="fa fa-rss-square fa fa" title="Twitter"></i></a>, <a class="social-icon" href="https://twitter.com/StanfordAILab">Twitter <i class="fa fa-twitter-square fa fa" title="Twitter"></i></a>, or email:

<!-- Begin Mailchimp Signup Form -->
<link href="//cdn-images.mailchimp.com/embedcode/horizontal-slim-10_7.css" rel="stylesheet" type="text/css">
<style type="text/css">
	#mc_embed_signup{background:#fff; clear:left; font:14px Helvetica,Arial,sans-serif; width:100%;}
	/* Add your own Mailchimp form style overrides in your site stylesheet or in this style block.
	   We recommend moving this block and the preceding CSS link to the HEAD of your HTML file. */
</style>
<div id="mc_embed_signup">
<form action="https://stanford.us19.list-manage.com/subscribe/post?u=3a6484754abf2fc18724ec835&amp;id=028823e92b" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <div id="mc_embed_signup_scroll">
	
	<input type="email" value="" name="EMAIL" class="email" id="mce-EMAIL" placeholder="email address" required>
    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_3a6484754abf2fc18724ec835_028823e92b" tabindex="-1" value=""></div>
    <div class="clear"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button"></div>
    </div>
</form>
</div>

<!--End mc_embed_signup-->



  <!-- Social media shares -->
  <div class="share-buttons">
    <ul class="share-buttons">
        <div class="meta">Share</div>
        
        <li>
            <a href="https://www.facebook.com/sharer/sharer.php?u=http://0.0.0.0:4000/blog/topologylayer/" target="_blank" title="Share on Facebook">
			<i class="fa fa-facebook-square fa-2x" aria-hidden="true"></i>
			<span class="sr-only">Share on Facebook</span>
		</a>
        </li>
         
        <li>
            <a href="https://twitter.com/intent/tweet?source=http://0.0.0.0:4000/blog/topologylayer/&text=A+Topology+Layer+for+Machine+Learning%20%7C%20SAIL+Blog:%20http://0.0.0.0:4000/blog/topologylayer/" target="_blank" title="Tweet">
			<i class="fa fa-twitter-square fa-2x" aria-hidden="true"></i>
			<span class="sr-only">Tweet</span>
		</a>
        </li>
            
        <li>
            <a href="https://getpocket.com/save?url=http://0.0.0.0:4000/blog/topologylayer/&title=A+Topology+Layer+for+Machine+Learning%20%7C%20SAIL+Blog" target="_blank" title="Add to Pocket">
			<i class="fa fa fa-get-pocket fa-2x" aria-hidden="true"></i>
			<span class="sr-only">Add to Pocket</span>
		</a>
        </li>
         
        <li>
            <a href="http://www.reddit.com/submit?url=http://0.0.0.0:4000/blog/topologylayer/&title=A+Topology+Layer+for+Machine+Learning%20%7C%20SAIL+Blog" target="_blank" title="Share on Reddit">
			<i class="fa fa-reddit-square fa-2x" aria-hidden="true"></i>
			<span class="sr-only">Share on Reddit</span>
		</a>
        </li>
           
        <li>
            <a href="mailto:?subject=A+Topology+Layer+for+Machine+Learning%20%7C%20SAIL+Blog&body=:%20http://0.0.0.0:4000/blog/topologylayer/" target="_blank" title="Email">
			<i class="fa fa-envelope-square fa-2x" aria-hidden="true"></i>
			<span class="sr-only">Email</span>
		</a>
        </li>
        
    </ul>
</div>

  <!-- Tag list -->
  
  


<footer>
  <div class="tag-list">
    
      <div class="meta">Tags</div>
    

    
    <a class="button" href="/blog/tags#computational+geometry">
      <p><i class="fa fa-tag fa-fw"></i> computational geometry</p>
    </a>
    
    <a class="button" href="/blog/tags#computational+topology">
      <p><i class="fa fa-tag fa-fw"></i> computational topology</p>
    </a>
    
    <a class="button" href="/blog/tags#machine+learning">
      <p><i class="fa fa-tag fa-fw"></i> machine learning</p>
    </a>
    
  </div>
</footer>

</article>

<!-- Disqus -->


<!-- Post navigation -->

  <div id="post-nav">
  
  <div id="previous-post" class="post-nav-post">
      <a href="/blog/controllable-dialogue/">
      <p>Previous post</p>
        What makes a good conversation?
      </a>
  </div>
  
  
  <div id="next-post" class="post-nav-post">
      <a href="/blog/minimax-optimal-pac/">
      <p>Next post</p>
        Policy Certificates and Minimax-Optimal PAC Bounds for Episodic Reinforcement Learning
      </a>
  </div>
  
</div>



    </div>
    <hr>
<footer class="site-footer">
    
   <div class="footer-icons">
        <ul>
        <!-- Social icons from Font Awesome, if enabled -->
        
<li>
	<a href="http://0.0.0.0:4000/blog/feed.xml" title="Follow RSS feed">
		<span class="fa-stack fa-lg">
            <i class="fa fa-circle fa-stack-2x"></i>
            <i class="fa fa-rss fa-stack-1x fa-inverse"></i>
        </span>
	</a>
</li>







































<li>
	<a href="https://twitter.com/StanfordAILab" title="Follow on Twitter" class="type">
        <span class="fa-stack fa-lg">
            <i class="fa fa-circle fa-stack-2x"></i>
            <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
        </span>
	</a>
</li>








        </ul>
    </div>
    <p class="text">&copy; 2019 Stanford AI Lab
</p>
 
</footer>



  </body>
</html>
