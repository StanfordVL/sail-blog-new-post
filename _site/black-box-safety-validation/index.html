<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <!-- Main JS (navbar.js, katex_init.js and masonry_init.js)-->
    <script defer=true src="/blog/assets/js/main.min.js"></script>
    
    <!-- CSS -->
    <link rel="stylesheet" href="/blog/assets/css/main.css">

    <!--Favicon-->
    <link rel="icon" type="image/png" href="/blog/assets/img/favicon-32x32.png" sizes="32x32" />
    <link rel="icon" type="image/png" href="/blog/assets/img/favicon-16x16.png" sizes="16x16" />

    <!-- Canonical -->
    <link rel="canonical" href="http://0.0.0.0:4000/blog/black-box-safety-validation/">

    <!-- RSS -->
    <link rel="alternate" type="application/atom+xml" title="The Stanford AI Lab Blog" href="http://0.0.0.0:4000/blog/feed.xml"/>

    <!-- Font Awesome -->
    <!-- <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet"> -->
    <link rel="stylesheet" type="text/css" href="/blog/assets/css/vendor/font-awesome.min.css">
    
    <!-- Bootstrap-3.3.7 isolation CSS -->
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" integrity="sha384-JcKb8q3iqJ61gNV9KGb8thSsNjpSL0n8PARn9HuZOnIxN0hoP+VmmDGMN5t9UJ0Z" crossorigin="anonymous">


    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js" integrity="sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha384-B4gt1jrGC7Jh4AgTPSdUtOBvfO8shuf57BaghqFfPlYxofvL8/KUEfYiJOMMV+rV" crossorigin="anonymous"></script>
    
    <!-- JQuery 2.2.4 -->
    <script src="https://code.jquery.com/jquery-2.2.4.min.js" integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin="anonymous"></script>
    
    <link rel="stylesheet" type="text/css" href="/blog/assets/css/vendor/bigfoot-number.css">
    <script type="text/javascript" src="/blog/assets/js/vendor/bigfoot.min.js"></script>
    <script type="text/javascript">
    $(document).ready(function() { 
      $.bigfoot();
      
    window.onload = function() {

        var videos = document.getElementsByTagName("video"),
            fraction = 0.8;

        function checkScroll() {

            for (var i = 0; i < videos.length; i++) {

                var video = videos[i];

                var x = video.offsetLeft,
                    y = video.offsetTop,
                    w = video.offsetWidth,
                    h = video.offsetHeight,
                    r = x + w, //right
                    b = y + h, //bottom
                    visibleX, visibleY, visible;

                visibleX = Math.max(0, Math.min(w, window.pageXOffset + window.innerWidth - x, r - window.pageXOffset));
                visibleY = Math.max(0, Math.min(h, window.pageYOffset + window.innerHeight - y, b - window.pageYOffset));

                visible = visibleX * visibleY / (w * h);

                if (visible > fraction) {
                    video.play();
                } else {
                    video.pause();
                }

            }

        }

        window.addEventListener('scroll', checkScroll, false);
        window.addEventListener('resize', checkScroll, false);
    }
    }); 
    </script>
    


    <!-- Google Fonts -->
    
    <link href="//fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,700italic,400italic" rel="stylesheet" type="text/css"> 
    

    <!-- KaTeX 0.8.3 -->
    
    <link rel="stylesheet" type="text/css" href="/blog/assets/css/vendor/katex.min.css">
    <script src="/blog/assets/js/vendor/katex.min.js">
    </script>
    
    
    
    <!-- Google Analytics -->
    
    <script>
        (function(i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r;
            i[r] = i[r] || function() {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date();
            a = s.createElement(o),
                m = s.getElementsByTagName(o)[0];
            a.async = 1;
            a.src = g;
            m.parentNode.insertBefore(a, m)
        })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'UA-129018108-1', 'auto');
        ga('send', 'pageview');

    </script>
    
    
    <!-- seo tags -->
    <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Safety Validation of Black-Box Autonomous Systems | SAIL Blog</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Safety Validation of Black-Box Autonomous Systems" />
<meta name="author" content="<a href="http://anthonylcorso.com/">Anthony L. Corso</a>" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="With autonomous systems becoming more capable, they are entering into safety-critical domains such as autonomous driving, aircraft collision avoidance, and healthcare. Ensuring the safe operations of these systems is a crucial step before they can be deployed and accepted by our society. Failure to perform the proper degree of safety validation can risk the loss of property or even human life." />
<meta property="og:description" content="With autonomous systems becoming more capable, they are entering into safety-critical domains such as autonomous driving, aircraft collision avoidance, and healthcare. Ensuring the safe operations of these systems is a crucial step before they can be deployed and accepted by our society. Failure to perform the proper degree of safety validation can risk the loss of property or even human life." />
<link rel="canonical" href="http://0.0.0.0:4000/blog/black-box-safety-validation/" />
<meta property="og:url" content="http://0.0.0.0:4000/blog/black-box-safety-validation/" />
<meta property="og:site_name" content="SAIL Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-08-31T00:00:00-07:00" />
<script type="application/ld+json">
{"description":"With autonomous systems becoming more capable, they are entering into safety-critical domains such as autonomous driving, aircraft collision avoidance, and healthcare. Ensuring the safe operations of these systems is a crucial step before they can be deployed and accepted by our society. Failure to perform the proper degree of safety validation can risk the loss of property or even human life.","author":{"@type":"Person","name":"<a href=\"http://anthonylcorso.com/\">Anthony L. Corso</a>"},"@type":"BlogPosting","url":"http://0.0.0.0:4000/blog/black-box-safety-validation/","headline":"Safety Validation of Black-Box Autonomous Systems","dateModified":"2020-08-31T00:00:00-07:00","datePublished":"2020-08-31T00:00:00-07:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://0.0.0.0:4000/blog/black-box-safety-validation/"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    
    <!-- Manual seo tags -->
    <title>Safety Validation of Black-Box Autonomous Systems | The Stanford AI Lab Blog</title>
    <meta name="description" content="Approaches for uncovering failures of decision-making systems in safety-critical domain">
    
    <!-- Twitter Cards -->
    <meta name="twitter:title" content="Safety Validation of Black-Box Autonomous Systems">
    
    <meta name="twitter:description" content="Approaches for uncovering failures of decision-making systems in safety-critical domain">
    
    <meta name="twitter:creator" content="@StanfordAILab">
    
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:image" content="http://0.0.0.0:4000/blog//assets/img/posts/2020-08-25-black-box-safety-validation/formulation.jpg">
    <meta name="og:image" content="http://0.0.0.0:4000/blog//assets/img/posts/2020-08-25-black-box-safety-validation/formulation.jpg">
    
</head>

  <body class="post-body">
      <!-- Toggle menu -->
<header class="site-header">

<nav class="clear navbar navbar-expand-lg navbar-light bg-white flex-column flex-md-row bd-navbar fixed-top" id="main_nav">
  
  <div class="container">

    <a class="navbar-brand mr-0 mr-md-2 text-black d-flex align-items-center" href="/blog/" aria-label="Bootstrap">
    
	  <div class="branding">
	    <a href="http://ai.stanford.edu/">
	      <img class="avatar" src="/blog/assets/img/sail-logo.png" alt=""/>
		  </a>

      <a href="/blog/">
	      <h1 class="site-title">
			    The Stanford AI Lab Blog
		    </h1> 
		  </a>
	  </div>
    
    
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
    </button>
    
    <div class="collapse navbar-collapse" id="navbarResponsive">

      <ul class="navbar-nav ml-auto">
      
      
      <li class="nav-item">
      <a class="nav-link" href="/blog/about">About</a>
      </li>
      
      <li class="nav-item dropdown">
        <a class="nav-link dropdown-toggle" data-toggle="dropdown" href="#" role="button" aria-haspopup="true" aria-expanded="false">Posts</a>
        <div class="dropdown-menu">
        <a class="dropdown-item" href="/blog/">All</a>
        <a class="dropdown-item" href="/blog/conferences">Conferences</a>
        <a class="dropdown-item" href="/blog/vision">Computer Vision</a>
        <a class="dropdown-item" href="/blog/robotics">Robotics</a>
        <a class="dropdown-item" href="/blog/nlp">NLP</a>
        <a class="dropdown-item" href="/blog/ml">Machine Learning</a>
        <a class="dropdown-item" href="/blog/rl">Reinforcement Learning</a>

        </div>
      </li>
        
      
      <li class="nav-item">
      <a class="nav-link" href="/blog/subscribe">Subscribe</a>
      </li>
      
      
      <li class="nav-item">
      <a class="nav-link" href="http://ai.stanford.edu/">SAIL</a>
      </li>
      
      </ul> 

    </div>

  </div>
</nav>

</header>

  
    <div class="content">
      

<article>

  <header id="main">
    
    <h1 id="post_title">Safety Validation of Black-Box Autonomous Systems</h1>
    <p class="meta">
    <a href="http://anthonylcorso.com/">Anthony L. Corso</a>
    <div class="post-date">August 31, 2020</div>
    </p>
  <hr>
  </header>


  <section class="post-content">
  
    <p>With autonomous systems becoming more capable, they are entering into safety-critical domains such as autonomous driving, aircraft collision avoidance, and healthcare. Ensuring the safe operations of these systems is a crucial step before they can be deployed and accepted by our society. Failure to perform the proper degree of safety validation can risk the loss of property or even  human life.</p>

<figure class="figure"><div class="figure__main">
<p><img class="postimage" src="/blog/assets/img/posts/2020-08-25-black-box-safety-validation/design.png" /></p>
<figcaption>
The autonomous system design cycle.
</figcaption>
</div></figure>

<p>Safety can be incorporated at various stages of the development of an autonomous system.  Consider the above model for the design cycle of such a system. A necessary component of safety is the <strong>definition</strong> of a complete set of realistic and safe requirements such as the Responsibility-Sensitive Safety model<sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup> which encodes commonsense driving rules—such as <em>don’t rear end anyone</em> and <em>right of way is given, not taken</em>—into formal mathematical statements about what a vehicle is and is not allowed to do in a given driving scenario. Safety can also be incorporated directly into the <strong>design</strong> of the system through techniques such as safety-masked reinforcement learning (RL)<sup id="fnref:2"><a href="#fn:2" class="footnote">2</a></sup> where a driving agent learns how to drive under the constraint that it only takes actions that have a minimal likelihood of causing a collision. Compared to traditional reinforcement learning techniques which have no constraint on their exploratory actions, safety-masked RL results in a safer driving policy.</p>

<p>Once a prototype of a system is available, safety validation can be performed through <strong>testing</strong>, performance <strong>evaluation</strong>, and <strong>interpretation</strong> of the failure modes of the system. Testing can discover failures due to implementation bugs, missing requirements, and emergent behavior due to the complex interaction of subcomponents. For complex autonomous systems operating in physical environments, we can not guarantee safety in all situations, so performance evaluation techniques can determine if the system is acceptably safe. The failure examples generated from testing can then be used to understand flaws in the systems and help engineers to fix them in the next iteration. Even with safety embedded in the process of defining requirements and system design, safety validation is a critical part of ensuring safe autonomy.</p>

<p>There are multiple ways to go about safety validation. White-box approaches use knowledge of the design of the system to construct challenging scenarios and evaluate the behavior of the system. They are often interpretable and can give a high degree of confidence in a system, but can suffer from problems of scalability. Modern autonomous systems employ complex components such as deep neural networks for perception and decision making. Despite improvements to white-box approaches for small neural networks<sup id="fnref:3"><a href="#fn:3" class="footnote">3</a></sup>, they don’t scale to the large networks used in practice. We can, however, trade formal guarantees for scalability by employing algorithms that treat the autonomous system as a black-box.</p>

<p>Safety validation algorithms for black-box autonomous systems have become the preferred tool for validation since they scale to complex systems and can rely on the latest advancements in machine learning to become more effective. In this blog post we cover the latest research in algorithms for the safety validation of black box autonomous systems. For a more in-depth description of the following algorithms (including pseudocode) see our recent survey paper <em><a href="https://arxiv.org/abs/2005.02979">A Survey of Algorithms for Black-Box Safety Validation</a></em>.</p>

<figure class="figure"><div class="figure__main">
<p><img class="postimage_75" src="/blog/assets/img/posts/2020-08-25-black-box-safety-validation/formulation.jpg" /></p>
<figcaption>
The problem formulation for the safety validation of black-box autonomous systems.
</figcaption>
</div></figure>

<p>The setup for safety validation algorithms for  black-box systems is shown above. We have a black-box system that is going to be tested, such as an autonomous vehicle driving policy or an aircraft collision avoidance system. We assume we have a simulated environment in which the system takes actions after making observations with its sensors, while an adversary perturbs the environment through disturbances <script type="math/tex">x</script> in an effort to make the system fail. Disturbances could include sensor noise, the behavior of other agents in the environment, or environmental conditions such as weather. The adversary may have access to the state of the environment which, for example, may describe the positions and velocity of all the vehicles and pedestrians in a driving scenario. The systems we care about usually operate over time in a physical environment, in which case the adversary seeks to find the <em>sequence</em> of disturbances that leads to failure. Finding a disturbance trajectory <script type="math/tex">X = [x_1, \ldots, x_N]</script> that leads to failure, rather than just a single disturbance, makes the problem much more challenging. We may also have a model of the disturbances in the environment <script type="math/tex">p(X)</script> that describes which sequences of disturbances are most likely. The disturbance model can be constructed through expert knowledge or learned from real-world data.  The exact goal of the adversary may be</p>

<ol>
  <li><strong>Falsification</strong>: Find any disturbance trajectory <script type="math/tex">X</script> that leads to a failure.</li>
  <li><strong>Most likely failure analysis</strong>: Find the most likely disturbance trajectory that leads to a failure (i.e. maximize <script type="math/tex">p(X)</script>).</li>
  <li><strong>Estimation of the probability of failure</strong>: Determine how likely it is that <em>any</em> failure will occur based on knowledge of <script type="math/tex">p(X)</script>.</li>
</ol>

<p>The adversary can use a variety of algorithms to generate disturbances. We will cover 4 categories: optimization, path-planning, reinforcement learning, and importance sampling.</p>

<h2 id="optimization">Optimization</h2>

<p>Optimization approaches search over the space of possible disturbance trajectories to find those that lead to a system failure. Optimization techniques can involve adaptive sampling or a coordinated search, both of which are guided by a cost function <script type="math/tex">c(X)</script> which measures the level of safety for a particular disturbance trajectory. The lower the cost, the closer we are to a failure. Some common cost functions include</p>

<ul>
  <li><strong>Miss distance</strong>: Often a physically-motivated measure of safety such as the point of closest approach between two aircraft or two vehicles.</li>
  <li><strong>Temporal logic robustness</strong>: When the safety requirements of a system are expressed formally using temporal logic, a language used to reason about events over time, the <em>robustness</em><sup id="fnref:6"><a href="#fn:6" class="footnote">4</a></sup> measures how close a trajectory is to violating the specification<sup id="fnref:7"><a href="#fn:7" class="footnote">5</a></sup>.</li>
</ul>

<p>When performing most likely failure analysis, the probability of the disturbance trajectory is incorporated into the regular cost function to produce a new cost <script type="math/tex">c'(X)</script>. Ideally, probability can be incorporated as a piecewise objective where <script type="math/tex">c'(X) = c(X)</script> when <script type="math/tex">X</script> does not lead to failure and <script type="math/tex">c'(X) = -p(X)</script> when <script type="math/tex">X</script> does lead to a failure. In practice, however, using a penalty term <script type="math/tex">c'(X) = c(X) - \lambda p(X)</script> may be easier to optimize.</p>

<p>The upside of formulating safety validation as an optimization problem is the ability to use off-the-shelf optimizers and rely on the significant amount of optimization literature (see Kochenderfer and Wheeler<sup id="fnref:8"><a href="#fn:8" class="footnote">6</a></sup> for an overview). Approaches that have been successfully used for safety validation include simulated annealing<sup id="fnref:9"><a href="#fn:9" class="footnote">7</a></sup>, genetic algorithms<sup id="fnref:10"><a href="#fn:10" class="footnote">8</a></sup>, Bayesian optimization<sup id="fnref:11"><a href="#fn:11" class="footnote">9</a></sup>, extended ant-colony optimization<sup id="fnref:12"><a href="#fn:12" class="footnote">10</a></sup>, and genetic programming<sup id="fnref:13"><a href="#fn:13" class="footnote">11</a></sup>.</p>

<p>The downsides of optimization-based approaches are twofold. First, we are directly searching over the space of all possible disturbance <em>trajectories</em> which is exponential in the length of the trajectory. This can quickly get out of hand. Second, the state of the environment is not typically used when choosing the disturbance trajectory. The state of the environment may not be available for logistical or privacy reasons, but if it is, then the state can provide additional information to the adversary. The next two sections describe techniques to address these limitations by building the disturbance trajectories sequentially and using the state information to help guide the search.</p>

<h2 id="path-planning">Path Planning</h2>

<p>When the safety validation problem is cast as a path-planning problem, we search for failures by sequentially building disturbance trajectories that explore the state space of the environment.  There are several metrics of state-space coverage that can be used to guide the search and decide when the state space has been sufficiently explored<sup id="fnref:14"><a href="#fn:14" class="footnote">12</a></sup>.</p>

<figure class="figure"><div class="figure__main">
<p><img class="postimagesmaller" src="/blog/assets/img/posts/2020-08-25-black-box-safety-validation/rrt.png" /></p>
<figcaption>
Two sample trees generated by the RRT Algorithm.
</figcaption>
</div></figure>

<p>One of the most common path-planning algorithms that has been used for safety validation is the rapidly-exploring random tree (RRT) algorithm, depicted above<sup id="fnref:15"><a href="#fn:15" class="footnote">13</a></sup>. In RRT, a space-filling tree is iteratively constructed by choosing disturbances that bring the environment into unexplored regions of the state space. The RRT algorithm has been used to find failures of an adaptive cruise control system<sup id="fnref:16"><a href="#fn:16" class="footnote">14</a></sup> where failures involved complex motion of the lead vehicle (shown below) that would be rarely discovered by traditional sampling techniques.</p>

<figure class="figure"><div class="figure__main">
<p><img class="postimage" src="/blog/assets/img/posts/2020-08-25-black-box-safety-validation/acc_failure.png" /></p>
<figcaption>
Sample failure of an adaptive cruise control system.
</figcaption>
</div></figure>

<p>Many path planning approaches were designed to be used with white-box systems and environments where dynamics and gradient information is available. When applied to black-box safety validation, these algorithms need to be adapted to forego the use of such information. For example, in multiple shooting methods, a trajectory is constructed through disjoint segments, which are then joined using gradient descent. In the absence of gradient information, a black-box multiple shooting method was developed that connected segments by successively refining the segment inputs and outputs through full trajectory rollouts<sup id="fnref:17"><a href="#fn:17" class="footnote">15</a></sup>.</p>

<h2 id="reinforcement-learning">Reinforcement Learning</h2>

<p>The safety validation problem can be further simplified if we describe it as a <a href="https://en.wikipedia.org/wiki/Markov_decision_process">Markov decision process</a> where the next state of the environment is only a function of the current state and disturbance. The Markov assumption allows us to select disturbances based only on the current state and apply reinforcement learning (RL) algorithms such as Monte Carlo tree search (MCTS), and deep RL algorithms such as Deep Q-Networks or Proximal Policy Optimization.</p>

<p>Monte Carlo tree search is similar to RRT in that a search tree is iteratively created to find disturbance trajectories that end in failure. Unlike RRT, however, MCTS is designed for use with black-box systems. The trajectories are always rolled out from the initial state of the simulator and the search is guided by a reward function rather than a coverage of the state space. These modifications allow MCTS to be applied in the most information-poor environments.  Lee et. al<sup id="fnref:4"><a href="#fn:4" class="footnote">16</a></sup> used MCTS to find failures of an aircraft collision avoidance system (an example failure is depicted below) where they had no access to the simulator state and could only control actions through a pseudorandom seed. This approach may be preferred when organizations don’t want to expose any aspect of the functioning of their system.</p>

<p>Deep RL has seen a lot of success in recent years due to its ability to solve problems with large state spaces, complex dynamics, and large action spaces. The success of deep RL is due to the large representational capacity of neural networks and advanced optimization techniques, which make it a natural choice as a safety validation algorithm. For example, it has been used to find failures of autonomous driving policies<sup id="fnref:5"><a href="#fn:5" class="footnote">17</a></sup> where the state and action spaces are large and continuous—attributes that are difficult for other algorithms to handle well. A sample failure of an autonomous driving policy is demonstrated below<sup id="fnref:19"><a href="#fn:19" class="footnote">18</a></sup>.</p>

<figure class="figure"><div class="figure__main">
<p><img class="postimagethird" src="/blog/assets/img/posts/2020-08-25-black-box-safety-validation/vcas_failure.jpg" />
<img class="postimagehalf" src="/blog/assets/img/posts/2020-08-25-black-box-safety-validation/rss_failure.gif" /></p>
<figcaption>
(Left) Sample failure of an aircraft collision avoidance system, (right) sample failure of a driving policy.
</figcaption>
</div></figure>

<p>Optimization, path-planning and RL approaches all lend themselves to solving the problems of falsification and most likely failure analysis. However, when we need to evaluate the failure probability of a system, importance sampling approaches should be used.</p>

<h2 id="importance-sampling">Importance Sampling</h2>

<p>The final set of approaches are well-suited for the task of estimating the probability of failure of the system from many failure examples. Importance sampling approaches seek to learn a sampling distribution <script type="math/tex">q(X)</script> that reliably produces failures and can be used to estimate the probability of failure with the minimal number of samples. Some common approaches are the cross-entropy method<sup id="fnref:20"><a href="#fn:20" class="footnote">19</a></sup>, multilevel splitting<sup id="fnref:21"><a href="#fn:21" class="footnote">20</a></sup>, supervised learning<sup id="fnref:22"><a href="#fn:22" class="footnote">21</a></sup>, and approximate dynamic programming<sup id="fnref:23"><a href="#fn:23" class="footnote">22</a></sup>.</p>

<p>Most importance sampling approaches suffer the same drawback as optimization-based approaches: they are constructing a distribution across the entire disturbance trajectory <script type="math/tex">X</script>. If we can invoke the Markov assumption, however, then we can construct a good sampling distribution based only on the current state using dynamic programming. However, the downside to dynamic programming is its inability to scale to large state spaces and thus complex scenarios. Our recent work<sup id="fnref:25"><a href="#fn:25" class="footnote">23</a></sup> shows that we can overcome this scalability problem by decomposing the system into subproblems and combining the subproblem solutions. For example, in an autonomous driving scenario, each adversarial agent on the road is paired with the ego vehicle to create a smaller safety validation problem with just two agents. Each of these problems are solved and then recombined using a neural network based on the Attend, Adapt and Transfer (A2T) architecture<sup id="fnref:24"><a href="#fn:24" class="footnote">24</a></sup>. The combined solution is then refined using simulations of the full scenario. The decomposition strategy, network architecture and a sample failure for a 5-agent driving scenario is shown below. These types of hybrid approaches will be required to solve the most challenging safety validation problems.</p>

<figure class="figure"><div class="figure__main">
<p><img class="postimagehalf" src="/blog/assets/img/posts/2020-08-25-black-box-safety-validation/decomp.png" />
<img class="postimagehalf" src="/blog/assets/img/posts/2020-08-25-black-box-safety-validation/A2T.png" /></p>
<figcaption>
(Left) Decomposition into pairwise subproblems, each involving the blue ego vehicle. (Right) The network used to fuse the subproblem solutions based on A2T.
</figcaption>
</div></figure>

<figure class="figure"><div class="figure__main">
<p><img src="/blog/assets/img/posts/2020-08-25-black-box-safety-validation/A2T_failure.gif" width="91%" /></p>
<figcaption>
Sample failure for an autonomous driving policy in a complex environment.
</figcaption>
</div></figure>

<h2 id="the-future">The Future</h2>

<p>The validation of complex and safety-critical autonomous systems will likely involve many different techniques throughout the system design cycle, and black-box safety validation algorithms will play a crucial role. In particular, black-box algorithms are useful to the engineers who design safety-critical systems as well as third-party organizations that wish to validate the safety of such systems for regulatory or risk-assessment purposes. Although this post reviews many algorithms that will be of practical use for the validation of safety-critical autonomous systems, there are still areas that require more investigation. For example, we would like to be able to answer the question: if no failure has been found, how sure are we that the system is safe? This will require the development of algorithms that have formal or probabilistic guarantees of convergence. Scalability also remains a significant challenge. Autonomous systems can encounter a wide range of complex interactions, so safety validation algorithms must be able to efficiently discover failures in the most complex scenarios. The algorithms presented in this survey are a promising step toward safe and beneficial autonomy.</p>

<h5 id="acknowledgements">Acknowledgements</h5>

<p class="small-text">
Many thanks to Michelle Lee, Andrey Kurenkov, Robert Moss, Mark Koren, Ritchie Lee, and Mykel Kochenderfer for comments and edits on this blog post.
</p>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>Shalev-Shwartz, Shai, et al. “On a formal model of safe and scalable self-driving cars.” <em>arXiv preprint arXiv:1708.06374</em> (2017). <a href="#fnref:1" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>Bouton, Maxime, et al. “Reinforcement learning with probabilistic guarantees for autonomous driving.” <em>arXiv preprint arXiv:1904.07189</em> (2019). <a href="#fnref:2" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>Katz, Guy, et al. “Reluplex: An efficient SMT solver for verifying deep neural networks.” <em>International Conference on Computer Aided Verification</em>. Springer, 2017. <a href="#fnref:3" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:6">
      <p>Fainekos, Georgios E., et al. “Robustness of temporal logic specifications for continuous-time signals.” <em>Theoretical Computer Science</em> 410.42 (2009): 4262-4291. <a href="#fnref:6" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:7">
      <p>Mathesen, Logan, et al. “Falsification of cyber-physical systems with robustness uncertainty quantification through stochastic optimization with adaptive restart.” <em>International Conference on Automation Science and Engineering (CASE)</em>. IEEE, 2019. <a href="#fnref:7" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:8">
      <p>M. J. Kochenderfer and T. A. Wheeler, <em>Algorithms for optimization</em>. MIT Press, 2019. <a href="#fnref:8" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:9">
      <p>Abbas, Houssam, et al. “Probabilistic temporal logic falsification of cyber-physical systems.” <em>ACM Transactions on Embedded Computing Systems (TECS)</em> 12.2s (2013): 1-30. <a href="#fnref:9" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:10">
      <p>Zou, Xueyi, et al. “Safety validation of sense and avoid algorithms using simulation and evolutionary search.” <em>International Conference on Computer Safety, Reliability, and Security</em>. Springer, 2014. <a href="#fnref:10" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:11">
      <p>Mullins, Galen E., et al. “Adaptive generation of challenging scenarios for testing and evaluation of autonomous vehicles.” <em>Journal of Systems and Software</em> 137 (2018): 197-215. <a href="#fnref:11" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:12">
      <p>Annapureddy, Yashwanth Singh Rahul, et al. “Ant colonies for temporal logic falsification of hybrid systems.” <em>Annual Conference on IEEE Industrial Electronics Society (IECON)</em>. IEEE, 2010. <a href="#fnref:12" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:13">
      <p>Corso, Anthony, et al. “Interpretable safety validation for autonomous vehicles.” To appear in <em>International Conference on Intelligent Transportation Systems (ITSC)</em>. IEEE, 2020. <a href="#fnref:13" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:14">
      <p>Nahhal, Tarik, et al. “Test coverage for continuous and hybrid systems.” <em>International Conference on Computer Aided Verification</em>. Springer, Berlin, Heidelberg, 2007. <a href="#fnref:14" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:15">
      <p>LaValle, Steven M. <em>Planning algorithms</em>. Cambridge University Press, 2006. <a href="#fnref:15" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:16">
      <p>Koschi, Markus, et al. “Computationally efficient safety falsification of adaptive cruise control systems.”_ Intelligent Transportation Systems Conference (ITSC)_. IEEE, 2019. <a href="#fnref:16" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:17">
      <p>Zutshi, Aditya, et al. “Multiple shooting, cegar-based falsification for hybrid systems.” <em>International Conference on Embedded Software</em>. 2014. <a href="#fnref:17" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:4">
      <p>Lee, Ritchie, et al. “Adaptive stress testing of airborne collision avoidance systems.” <em>Digital Avionics Systems Conference (DASC)</em>. IEEE, 2015. <a href="#fnref:4" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:5">
      <p>Koren, Mark, et al. “Adaptive stress testing for autonomous vehicles.” <em>Intelligent Vehicles Symposium (IV)</em>. IEEE, 2018. <a href="#fnref:5" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:19">
      <p>Corso, Anthony, et al. “Adaptive stress testing with reward augmentation for autonomous vehicle validation.” <em>Intelligent Transportation Systems Conference (ITSC)</em>. IEEE, 2019. <a href="#fnref:19" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:20">
      <p>O’Kelly, Matthew, et al. “Scalable end-to-end autonomous vehicle testing via rare-event simulation.” <em>Advances in Neural Information Processing Systems</em>. 2018. <a href="#fnref:20" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:21">
      <p>Norden, Justin, et al. “Efficient black-box assessment of autonomous vehicle safety.” <em>arXiv preprint arXiv:1912.03618</em> (2019). <a href="#fnref:21" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:22">
      <p>Uesato, Jonathan, et al. “Rigorous agent evaluation: An adversarial approach to uncover catastrophic failures.” <em>arXiv preprint arXiv:1812.01647</em> (2018). <a href="#fnref:22" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:23">
      <p>Corso, Anthony, et al. “Scalable autonomous vehicle safety validation through dynamic programming and scene decomposition.” To appear in <em>International Conference on Intelligent Transportation Systems (ITSC)</em>. IEEE, 2020. <a href="#fnref:23" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:25">
      <p>Corso, Anthony, et al. “Scalable autonomous vehicle safety validation through dynamic programming and scene decomposition.” To appear in <em>International Conference on Intelligent Transportation Systems (ITSC)</em>. IEEE, 2020. <a href="#fnref:25" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:24">
      <p>Rajendran, Janarthanan, et al. “Attend, adapt and transfer: Attentive deep architecture for adaptive transfer from multiple sources in the same domain.” <em>arXiv preprint arXiv:1510.02879</em> (2015). <a href="#fnref:24" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>

  
  </section>
  <hr>
  Keep on top of the latest SAIL Blog posts via <a class="social-icon" href="http://0.0.0.0:4000/blog/feed.xml">RSS <i class="fa fa-rss-square fa fa" title="Twitter"></i></a>, <a class="social-icon" href="https://twitter.com/StanfordAILab">Twitter <i class="fa fa-twitter-square fa fa" title="Twitter"></i></a>, or email:

<!-- Begin Mailchimp Signup Form -->
<link href="//cdn-images.mailchimp.com/embedcode/horizontal-slim-10_7.css" rel="stylesheet" type="text/css">
<style type="text/css">
	#mc_embed_signup{background:#fff; clear:left; font:14px Helvetica,Arial,sans-serif; width:100%;}
	/* Add your own Mailchimp form style overrides in your site stylesheet or in this style block.
	   We recommend moving this block and the preceding CSS link to the HEAD of your HTML file. */
</style>
<div id="mc_embed_signup">
<form action="https://stanford.us19.list-manage.com/subscribe/post?u=3a6484754abf2fc18724ec835&amp;id=028823e92b" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <div id="mc_embed_signup_scroll">
	
	<input type="email" value="" name="EMAIL" class="email" id="mce-EMAIL" placeholder="email address" required>
    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_3a6484754abf2fc18724ec835_028823e92b" tabindex="-1" value=""></div>
    <div class="clear"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button"></div>
    </div>
</form>
</div>

<!--End mc_embed_signup-->



  <!-- Social media shares -->
  <div class="share-buttons">
    <ul class="share-buttons">
        <div class="meta">Share</div>
        
        <li>
            <a href="https://www.facebook.com/sharer/sharer.php?u=http://0.0.0.0:4000/blog/black-box-safety-validation/" target="_blank" title="Share on Facebook">
			<i class="fa fa-facebook-square fa-2x" aria-hidden="true"></i>
			<span class="sr-only">Share on Facebook</span>
		</a>
        </li>
         
        <li>
            <a href="https://twitter.com/intent/tweet?source=http://0.0.0.0:4000/blog/black-box-safety-validation/&text=Safety+Validation+of+Black-Box+Autonomous+Systems%20%7C%20SAIL+Blog:%20http://0.0.0.0:4000/blog/black-box-safety-validation/" target="_blank" title="Tweet">
			<i class="fa fa-twitter-square fa-2x" aria-hidden="true"></i>
			<span class="sr-only">Tweet</span>
		</a>
        </li>
            
        <li>
            <a href="https://getpocket.com/save?url=http://0.0.0.0:4000/blog/black-box-safety-validation/&title=Safety+Validation+of+Black-Box+Autonomous+Systems%20%7C%20SAIL+Blog" target="_blank" title="Add to Pocket">
			<i class="fa fa fa-get-pocket fa-2x" aria-hidden="true"></i>
			<span class="sr-only">Add to Pocket</span>
		</a>
        </li>
         
        <li>
            <a href="http://www.reddit.com/submit?url=http://0.0.0.0:4000/blog/black-box-safety-validation/&title=Safety+Validation+of+Black-Box+Autonomous+Systems%20%7C%20SAIL+Blog" target="_blank" title="Share on Reddit">
			<i class="fa fa-reddit-square fa-2x" aria-hidden="true"></i>
			<span class="sr-only">Share on Reddit</span>
		</a>
        </li>
           
        <li>
            <a href="mailto:?subject=Safety+Validation+of+Black-Box+Autonomous+Systems%20%7C%20SAIL+Blog&body=:%20http://0.0.0.0:4000/blog/black-box-safety-validation/" target="_blank" title="Email">
			<i class="fa fa-envelope-square fa-2x" aria-hidden="true"></i>
			<span class="sr-only">Email</span>
		</a>
        </li>
        
    </ul>
</div>

  <!-- Tag list -->
  
  


<footer>
  <div class="tag-list">
    
      <div class="meta">Tags</div>
    

    
    <a class="button" href="/blog/tags#autonomous+driving">
      <p><i class="fa fa-tag fa-fw"></i> autonomous driving</p>
    </a>
    
    <a class="button" href="/blog/tags#ml">
      <p><i class="fa fa-tag fa-fw"></i> ml</p>
    </a>
    
    <a class="button" href="/blog/tags#rl">
      <p><i class="fa fa-tag fa-fw"></i> rl</p>
    </a>
    
    <a class="button" href="/blog/tags#robotics">
      <p><i class="fa fa-tag fa-fw"></i> robotics</p>
    </a>
    
    <a class="button" href="/blog/tags#safety">
      <p><i class="fa fa-tag fa-fw"></i> safety</p>
    </a>
    
  </div>
</footer>

</article>

<!-- Disqus -->


<!-- Post navigation -->

  <div id="post-nav">
  
  <div id="previous-post" class="post-nav-post">
      <a href="/blog/meta-exploration/">
      <p>Previous post</p>
        Explore then Execute: Adapting without Rewards via Factorized Meta-Reinforcement Learning
      </a>
  </div>
  
  
  <div id="next-post" class="post-nav-post">
      <a href="/blog/infilling-by-language-modeling/">
      <p>Next post</p>
        How to Fill in the Blanks with Language Models
      </a>
  </div>
  
</div>



    </div>
    <hr>
<footer class="site-footer">
    
   <div class="footer-icons">
        <ul>
        <!-- Social icons from Font Awesome, if enabled -->
        
<li>
	<a href="http://0.0.0.0:4000/blog/feed.xml" title="Follow RSS feed">
		<span class="fa-stack fa-lg">
            <i class="fa fa-circle fa-stack-2x"></i>
            <i class="fa fa-rss fa-stack-1x fa-inverse"></i>
        </span>
	</a>
</li>







































<li>
	<a href="https://twitter.com/StanfordAILab" title="Follow on Twitter" class="type">
        <span class="fa-stack fa-lg">
            <i class="fa fa-circle fa-stack-2x"></i>
            <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
        </span>
	</a>
</li>








        </ul>
    </div>
    <p class="text">&copy; 2019 Stanford AI Lab
</p>
 
</footer>



  </body>
</html>
