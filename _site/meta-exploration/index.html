<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <!-- Main JS (navbar.js, katex_init.js and masonry_init.js)-->
    <script defer=true src="/blog/assets/js/main.min.js"></script>
    
    <!-- CSS -->
    <link rel="stylesheet" href="/blog/assets/css/main.css">

    <!--Favicon-->
    <link rel="icon" type="image/png" href="/blog/assets/img/favicon-32x32.png" sizes="32x32" />
    <link rel="icon" type="image/png" href="/blog/assets/img/favicon-16x16.png" sizes="16x16" />

    <!-- Canonical -->
    <link rel="canonical" href="http://0.0.0.0:4000/blog/meta-exploration/">

    <!-- RSS -->
    <link rel="alternate" type="application/atom+xml" title="The Stanford AI Lab Blog" href="http://0.0.0.0:4000/blog/feed.xml"/>

    <!-- Font Awesome -->
    <!-- <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet"> -->
    <link rel="stylesheet" type="text/css" href="/blog/assets/css/vendor/font-awesome.min.css">
    
    <!-- Bootstrap-3.3.7 isolation CSS -->
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" integrity="sha384-JcKb8q3iqJ61gNV9KGb8thSsNjpSL0n8PARn9HuZOnIxN0hoP+VmmDGMN5t9UJ0Z" crossorigin="anonymous">


    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js" integrity="sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha384-B4gt1jrGC7Jh4AgTPSdUtOBvfO8shuf57BaghqFfPlYxofvL8/KUEfYiJOMMV+rV" crossorigin="anonymous"></script>
    
    <!-- JQuery 2.2.4 -->
    <script src="https://code.jquery.com/jquery-2.2.4.min.js" integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin="anonymous"></script>
    
    <link rel="stylesheet" type="text/css" href="/blog/assets/css/vendor/bigfoot-number.css">
    <script type="text/javascript" src="/blog/assets/js/vendor/bigfoot.min.js"></script>
    <script type="text/javascript">
    $(document).ready(function() { 
      $.bigfoot();
      
    window.onload = function() {

        var videos = document.getElementsByTagName("video"),
            fraction = 0.8;

        function checkScroll() {

            for (var i = 0; i < videos.length; i++) {

                var video = videos[i];

                var x = video.offsetLeft,
                    y = video.offsetTop,
                    w = video.offsetWidth,
                    h = video.offsetHeight,
                    r = x + w, //right
                    b = y + h, //bottom
                    visibleX, visibleY, visible;

                visibleX = Math.max(0, Math.min(w, window.pageXOffset + window.innerWidth - x, r - window.pageXOffset));
                visibleY = Math.max(0, Math.min(h, window.pageYOffset + window.innerHeight - y, b - window.pageYOffset));

                visible = visibleX * visibleY / (w * h);

                if (visible > fraction) {
                    video.play();
                } else {
                    video.pause();
                }

            }

        }

        window.addEventListener('scroll', checkScroll, false);
        window.addEventListener('resize', checkScroll, false);
    }
    }); 
    </script>
    


    <!-- Google Fonts -->
    
    <link href="//fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,700italic,400italic" rel="stylesheet" type="text/css"> 
    

    <!-- KaTeX 0.8.3 -->
    
    <link rel="stylesheet" type="text/css" href="/blog/assets/css/vendor/katex.min.css">
    <script src="/blog/assets/js/vendor/katex.min.js">
    </script>
    
    
    
    <!-- Google Analytics -->
    
    <script>
        (function(i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r;
            i[r] = i[r] || function() {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date();
            a = s.createElement(o),
                m = s.getElementsByTagName(o)[0];
            a.async = 1;
            a.src = g;
            m.parentNode.insertBefore(a, m)
        })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'UA-129018108-1', 'auto');
        ga('send', 'pageview');

    </script>
    
    
    <!-- seo tags -->
    <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Explore then Execute: Adapting without Rewards via Factorized Meta-Reinforcement Learning | SAIL Blog</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Explore then Execute: Adapting without Rewards via Factorized Meta-Reinforcement Learning" />
<meta name="author" content="<a href='https://cs.stanford.edu/~evanliu'>Evan Zheran Liu</a>" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Activities more fulfilling than chores." />
<meta property="og:description" content="Activities more fulfilling than chores." />
<link rel="canonical" href="http://0.0.0.0:4000/blog/meta-exploration/" />
<meta property="og:url" content="http://0.0.0.0:4000/blog/meta-exploration/" />
<meta property="og:site_name" content="SAIL Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-08-26T00:00:00-07:00" />
<script type="application/ld+json">
{"description":"Activities more fulfilling than chores.","author":{"@type":"Person","name":"<a href='https://cs.stanford.edu/~evanliu'>Evan Zheran Liu</a>"},"@type":"BlogPosting","url":"http://0.0.0.0:4000/blog/meta-exploration/","headline":"Explore then Execute: Adapting without Rewards via Factorized Meta-Reinforcement Learning","dateModified":"2020-08-26T00:00:00-07:00","datePublished":"2020-08-26T00:00:00-07:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://0.0.0.0:4000/blog/meta-exploration/"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    
    <!-- Manual seo tags -->
    <title>Explore then Execute: Adapting without Rewards via Factorized Meta-Reinforcement Learning | The Stanford AI Lab Blog</title>
    <meta name="description" content="Adapting to new environments and tasks (e.g., cooking in a new kitchen) requires first gathering information via exploration (e.g., finding the ingredients)....">
    
    <!-- Twitter Cards -->
    <meta name="twitter:title" content="Explore then Execute: Adapting without Rewards via Factorized Meta-Reinforcement Learning">
    
    <meta name="twitter:description" content="Adapting to new environments and tasks (e.g., cooking in a new kitchen) requires first gathering information via exploration (e.g., finding the ingredients). We identify a chicken-and-egg coupling problem that prevents existing approaches from effectively exploring, and we solve this with DREAM.">
    
    <meta name="twitter:creator" content="@StanfordAILab">
    
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:image" content="http://0.0.0.0:4000/blog/assets/img/posts/2020-08-23-meta-exploration/coupling.svg">
    <meta name="og:image" content="http://0.0.0.0:4000/blog/assets/img/posts/2020-08-23-meta-exploration/coupling.svg">
    
</head>

  <body class="post-body">
      <!-- Toggle menu -->
<header class="site-header">

<nav class="clear navbar navbar-expand-lg navbar-light bg-white flex-column flex-md-row bd-navbar fixed-top" id="main_nav">
  
  <div class="container">

    <a class="navbar-brand mr-0 mr-md-2 text-black d-flex align-items-center" href="/blog/" aria-label="Bootstrap">
    
	  <div class="branding">
	    <a href="http://ai.stanford.edu/">
	      <img class="avatar" src="/blog/assets/img/sail-logo.png" alt=""/>
		  </a>

      <a href="/blog/">
	      <h1 class="site-title">
			    The Stanford AI Lab Blog
		    </h1> 
		  </a>
	  </div>
    
    
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
    </button>
    
    <div class="collapse navbar-collapse" id="navbarResponsive">

      <ul class="navbar-nav ml-auto">
      
      
      <li class="nav-item">
      <a class="nav-link" href="/blog/about">About</a>
      </li>
      
      <li class="nav-item dropdown">
        <a class="nav-link dropdown-toggle" data-toggle="dropdown" href="#" role="button" aria-haspopup="true" aria-expanded="false">Posts</a>
        <div class="dropdown-menu">
        <a class="dropdown-item" href="/blog/">All</a>
        <a class="dropdown-item" href="/blog/conferences">Conferences</a>
        <a class="dropdown-item" href="/blog/vision">Computer Vision</a>
        <a class="dropdown-item" href="/blog/robotics">Robotics</a>
        <a class="dropdown-item" href="/blog/nlp">NLP</a>
        <a class="dropdown-item" href="/blog/ml">Machine Learning</a>
        <a class="dropdown-item" href="/blog/rl">Reinforcement Learning</a>

        </div>
      </li>
        
      
      <li class="nav-item">
      <a class="nav-link" href="/blog/subscribe">Subscribe</a>
      </li>
      
      
      <li class="nav-item">
      <a class="nav-link" href="http://ai.stanford.edu/">SAIL</a>
      </li>
      
      </ul> 

    </div>

  </div>
</nav>

</header>

  
    <div class="content">
      

<article>

  <header id="main">
    
    <h1 id="post_title">Explore then Execute: Adapting without Rewards via Factorized Meta-Reinforcement Learning</h1>
    <p class="meta">
    <a href='https://cs.stanford.edu/~evanliu'>Evan Zheran Liu</a>
    <div class="post-date">August 26, 2020</div>
    </p>
  <hr>
  </header>


  <section class="post-content">
  
    <figure class="figure"><div class="figure__main">
<p><img class="postimage_75" src="/blog/assets/img/posts/2020-08-23-meta-exploration/fulfilling_activities.svg" /></p>
<figcaption>
Activities more fulfilling than chores.
</figcaption>
</div></figure>

<p>Nobody likes chores — can we build robots to do these chores, such as
cooking, for us? A common paradigm for training agents to perform
various tasks is to train a separate agent on each task, completely from
scratch, with reinforcement learning. However, training a robot to cook
with reinforcement learning from scratch in each person’s home would
completely fail, as it would result in many disasters (e.g., kitchen
fires), would require a lot of supervision from each person to reward
the robot for successfully cooking meals, and would take a long time
(learning even simple tasks from scratch can take reinforcement learning
agents millions of attempts).</p>

<p>Instead, it would be ideal if we could train a robot to be able to
quickly adapt to various home kitchens, after first training in many
kitchens in a robot chef factory. Intuitively, this should be possible
since different tasks and environments share considerable structure
(e.g., cooking pizza in one kitchen is similar to cooking a hamburger in
another kitchen), which can make learning each task easier and more
efficient.</p>

<p>Fortunately, meta-reinforcement learning seeks this exact goal of
training agents to adapt to new tasks from very few interactions on the
new task, after first training on many similar tasks. So, why aren’t
robots cooking in our kitchens today? To answer this question, we’ll
turn our attention to the problem of <em>meta-exploration</em>: how to best
spend these few interactions <em>exploring</em> the new task. For example, in
order to adapt to a new kitchen, a robot chef should ideally spend its
few interactions exploring the new kitchen to find the ingredients,
which will allow it to later cook a meal (solve the task). In this blog
post, we’ll cover and solve two key challenges about
meta-exploration that keep humans in the kitchen.</p>

<ul>
  <li>
    <p>First, we’ll show that existing meta-reinforcement learning
approaches suffer from a chicken-and-egg coupling problem:
learning to explore and find the ingredients only helps a robot
prepare a meal if it already knows how to cook, but the robot can
only learn to cook if it already knows where the ingredients are.
We’ll avoid this cyclic dependence of learning to explore and learning to
execute (solve the task) by proposing an objective to learn them
independently of each other.</p>
  </li>
  <li>
    <p>Second, we’ll observe that the standard meta-reinforcement learning
problem setting expects robots to cook the correct meal by
trial-and-error, without even being told what meal to cook, which
unnecessarily complicates the meta-exploration problem. To avoid
this, we propose instruction-based meta-reinforcement learning,
where the robot receives instructions specifying what meal to
cook.</p>
  </li>
</ul>

<h2 id="standard-meta-reinforcement-learning">Standard Meta-Reinforcement Learning</h2>

<figure class="figure"><div class="figure__main">
<p><img src="/blog/assets/img/posts/2020-08-23-meta-exploration/standard_setting.svg" /></p>
<figcaption>
Standard meta-RL setting.
</figcaption>
</div></figure>

<p>Before we dive in, let’s review the standard meta-reinforcement learning
(meta-RL) problem statement. In meta-reinforcement learning, an agent
(e.g., a robot chef) trains on many tasks (different recipes) and
environments (different kitchens), and then must accomplish a new task
in a new environment during meta-testing. When presented with a new task
and environment, the agent is allowed to first spend an episode
exploring, gathering any necessary information (e.g., locating the
ingredients), before execution episodes, where the agent must accomplish
the task (e.g., cook a meal).</p>

<p>In more formal language, standard meta-RL considers a family of
problems, where each problem <script type="math/tex">\mu</script> identifies a reward function
<script type="math/tex">\mathcal{R}_\mu</script> (e.g., cook a pizza) and transition dynamics
(e.g., a kitchen).
Using the terminology from Duan et al., 2016, we define a trial to consist of
several episodes in the same problem. The first episode is the exploration
episode, where the agent is allowed to gather information, without needing to
maximize returns. All subsequent episodes are execution episodes, where the
agent must accomplish the task. The goal is to maximize the returns achieved
during the execution episodes of meta-testing trials, after first training on
many trials during meta-training.</p>

<h2 id="decoupled-reward-free-exploration-and-execution-in-meta-reinforcement-learning-dream"><strong>D</strong>ecoupled <strong>R</strong>eward-free <strong>E</strong>xplor<strong>a</strong>tion and Execution in <strong>M</strong>eta-Reinforcement Learning (DREAM)</h2>

<p><strong>A chicken-and-egg coupling problem.</strong> A common approach (Wang et al.,
2016, Duan et al., 2016) for the meta-exploration problem is to optimize
a recurrent policy that performs both exploration and execution episodes
end-to-end based on the execution episode rewards. The hope is to
capture the information learned during the exploration episode in the
recurrent policy’s hidden state, which will then be useful for
execution episodes. However, this leads to a chicken-and-egg coupling
problem, where learning good exploration behaviors requires already
having learned good execution behaviors and vice-versa, which prevents
such an approach from learning.</p>

<p>For example, if a robot chef fails to discover the locations of
ingredients in a kitchen (bad exploration), then it cannot possibly
learn how to cook (bad execution). On the other hand, if the robot does
not know how to cook (bad execution), then no matter what it does during
the exploration episode, it will still not successfully cook a meal,
making learning exploration challenging. Since robots can’t cook or
explore at the beginning of training, they get stuck in this local
optimum and have a hard time learning either.</p>

<figure class="figure"><div class="figure__main">
<p><img class="postimagehalf" style="width:40%" src="/blog/assets/img/posts/2020-08-23-meta-exploration/coupling.svg" />
<img class="postimagehalf" style="width:40%" src="/blog/assets/img/posts/2020-08-23-meta-exploration/coupling_example.svg" /></p>
<figcaption>
The coupling problem. What came first: the chicken (good exploration) or
the egg (good execution)?
</figcaption>
</div></figure>

<p><strong>Avoiding the coupling problem with DREAM.</strong> To avoid the
chicken-and-egg coupling problem, we propose a method to break the
cyclic dependency between learning exploration and learning execution
behaviors, which we call DREAM. Intuitively, good exploration can be
learned by trying to recover the information necessary for executing
instructions. Therefore, from a high-level, DREAM consists of two main
steps: 1) simultaneously learn an execution policy independently from
exploration and learn what information is necessary for execution and 2)
learn an exploration policy to recover that information.</p>

<figure class="figure"><div class="figure__main">
<p><img src="/blog/assets/img/posts/2020-08-23-meta-exploration/dream_meta_training.svg" /></p>
<figcaption>
To answer the chicken-and-egg problem, DREAM manufactures its own egg,
and out comes the chicken.
</figcaption>
</div></figure>

<p>More concretely, in the first step, we train an execution policy
<script type="math/tex">\pi^\text{exec}</script>
conditioned on the problem identifier <script type="math/tex">\mu</script>, which in the cooking example,
may either directly identify attributes of the kitchen (e.g., wall color
or ingredient locations), or simply be a unique identifier (e.g., a
one-hot) for each kitchen. This problem identifier (directly or
indirectly) encodes all the information necessary to solve tasks in the
kitchen, allowing the execution policy to learn independently from
exploration, which avoids the coupling problem. At the same time, our
goal in the first step is to identify only the information necessary for
executing instructions, and the problem identifier may also encode
extraneous information, such as the wall color. To remove this, we apply
an information bottleneck to obtain a bottlenecked representation <script type="math/tex">z</script>,
which we use for training an exploration policy <script type="math/tex">\pi^\text{exp}</script>.</p>

<p>In the second step, once we’ve obtained a bottleneck representation <script type="math/tex">z</script>
that ideally contains only the information necessary for executing
instructions, we can train an exploration policy <script type="math/tex">\pi^\text{exp}</script> to recover this
information in the exploration episode. To do this, we roll-out the
exploration policy to obtain an episode <script type="math/tex">\tau</script> and then reward the policy
based on how well this episode encodes the information contained in <script type="math/tex">z</script>.
Roughly, this reward is the mutual information <script type="math/tex">I(z; \tau)</script> between the
bottlenecked representation <script type="math/tex">z</script> and the episode <script type="math/tex">\tau</script>.</p>

<figure class="figure"><div class="figure__main">
<p><img class="postimage_75" src="/blog/assets/img/posts/2020-08-23-meta-exploration/dream_meta_testing.svg" /></p>
<figcaption>
DREAM meta-testing.
</figcaption>
</div></figure>

<p>The problem identifier <script type="math/tex">\mu</script> is easy to provide during meta-training by
simply assigning each problem a unique one-hot, but is typically
unavailable or unhelpful during meta-testing (e.g., if <script type="math/tex">\mu</script> is a completely
new one-hot). This might seem concerning, since, during meta-training,
the execution policy conditions on <script type="math/tex">z</script>, which requires knowing <script type="math/tex">\mu</script>. However,
since the exploration policy is trained to produce exploration
trajectories <script type="math/tex">\tau</script> that contain the same information as <script type="math/tex">z</script>, we can directly
swap <script type="math/tex">\tau</script> for <script type="math/tex">z</script> at meta-test time by rolling out the exploration policy.
See our paper for the details!</p>

<h2 id="instruction-based-meta-reinforcement-learning-imrl">Instruction-based Meta-Reinforcement Learning (IMRL)</h2>

<p><strong>Improving the standard meta-RL setting.</strong> A second meta-exploration
challenge concerns the meta-reinforcement learning setting itself.
While the above standard
meta-RL setting is a useful problem formulation, we observe two areas
that can be made more realistic. First, the standard setting requires
the agent to infer the task (e.g., the meal to cook) from reward
observations, which can be needlessly inefficient. In more realistic
situations, the user would just tell the agent what they want, instead.</p>

<figure class="figure"><div class="figure__main">
<p><img src="/blog/assets/img/posts/2020-08-23-meta-exploration/instructions.svg" /></p>
<figcaption>
Open and honest communication is important for your robots too.
</figcaption>
</div></figure>

<p>Second, while the standard meta-RL setting leverages shared structure
between different problems (environment and task pairs), it does not
capture shared structure between different tasks in the same
environment. More concretely, the task is fixed across all episodes in a
trial, and in order to perform a new task (e.g., cook a new meal), the
agent requires another exploration episode, even when the underlying
environment (e.g., the kitchen) stays the same. Instead, an agent would
ideally be able to perform many tasks after a single exploration
episode. For example, after exploring the kitchen to find any
ingredients, an ideal robot chef would be able to then cook <em>any</em> meal
involving those ingredients, whereas an agent trained in the standard
meta-reinforcement learning setting would only be able to cook a single meal.</p>

<figure class="figure"><div class="figure__main">
<p><img class="postimage_50" style="width:66%" src="/blog/assets/img/posts/2020-08-23-meta-exploration/multitask.svg" /></p>
<figcaption>
Dinner schedule according to a robot chef trained in the standard
meta-reinforcement learning setting.
</figcaption>
</div></figure>

<p>These two areas can obscure the meta-exploration problem of how to
optimally spend the exploration episode, as the former requires
unnecessary exploration to infer the task, while the latter only
requires the agent to explore to discover information relevant to a
single task. While intuitively, the agent should spend the exploration
episode gathering useful information for later execution episodes, in
many cases, optimal exploration collapses to simply solving the task.
For example, the agent can only discover that the task is to cook pizza
by successfully cooking pizza and receiving positive rewards, only to do
the same thing again and again on future execution episodes. This can
make the exploration episode nearly useless.</p>

<p><strong>Instruction-based meta-RL (IMRL).</strong> To make the meta-RL setting more
realistic, we propose a new setting called instruction-based meta-RL
(IMRL), which addresses the two above areas by (i) providing the agent
with instructions (e.g., "cook pizza" or a one-hot representation)
that specify the task during execution episodes and (ii) varying the
task by providing a different instruction on each execution episode.
Then, for example, after meta-training in different kitchens at a
factory, a robot chef could begin cooking many different meals specified
by a human in a new home kitchen, after a single setup period
(exploration episode).</p>

<figure class="figure"><div class="figure__main">
<p><img src="/blog/assets/img/posts/2020-08-23-meta-exploration/imrl.svg" /></p>
<figcaption>
Instruction-based meta-RL: The task, which changes each execution
episode, is conveyed to the agent via instructions. The environment
still stays the same within a trial.
</figcaption>
</div></figure>

<p><strong>Reward-free adaptation.</strong> In the standard meta-RL setting, the agent
requires reward observations during exploration episodes in order to
infer the task. However, by receiving instructions that specify the task
in IMRL, a further benefit is that the agent no longer requires observing
rewards to adapt to new tasks and environments. Concretely, IMRL enables
reward-free adaptation, where during meta-training, the agent uses reward
observations during execution episodes to learn to solve the task, but does
not observe rewards during exploration episodes. During meta-testing, the
agent never observes any rewards. This enables modeling real-world deployment
situations where gathering reward supervision is really expensive. For
example, a robot chef would ideally be able to adapt to a home kitchen without
any supervision from a human.</p>

<p><strong>Is IMRL general?</strong> Importantly, setting the instruction to always be
some "empty" instruction recovers the standard meta-RL setting. In
other words, standard meta-RL is just IMRL, where the user's desires
are fixed within a trial and the user says nothing for the instructions.
Therefore, algorithms developed for IMRL can also be directly applied to
the standard setting and vice-versa.</p>

<h2 id="results">Results</h2>

<figure class="figure"><div class="figure__main">
<figure class="postfigurehalf">
  <img src="/blog/assets/img/posts/2020-08-23-meta-exploration/blue.png" />
  <figcaption>
  Sign reads <b style="color:blue">blue</b>.
  </figcaption>
</figure>
<figure class="postfigurehalf">
  <img src="/blog/assets/img/posts/2020-08-23-meta-exploration/red.png" />
  <figcaption>
  Sign reads <b style="color:red">red</b>.
  </figcaption>
</figure>
</div></figure>

<p><strong>Sparse-reward 3D visual navigation.</strong> In one experiment from our
paper, we evaluate DREAM on the sparse-reward 3D visual navigation
problem family proposed by Kamienny et al., 2020 (pictured above), which
we've made harder by including a visual sign and more objects. We use
the IMRL setting with reward-free adaptation. During execution episodes,
the agent receives an instruction to go to an object: a ball, block or
key. The agent starts episodes on the far side of the barrier, and must
walk around the barrier to read the sign
(highlighted in <b style="color:#e6e600">yellow</b>),
which in the two versions of the problem, either specify going to the
<b style="color:blue">blue</b> or <b style="color:red">red</b> version of the
object. The agent receives 80x60 RGB images as observations
and can turn left or right, or move forward. Going to the correct object gives
reward <b>+1</b> and going to the wrong object gives reward <b>-1</b>.</p>

<p>DREAM learns near-optimal exploration and execution behaviors on this
task, which are pictured below. On the left, DREAM spends the
exploration episode walking around the barrier to read the sign, which
says <b style="color:blue">blue</b>. On the right, during an execution
episode, DREAM receives an instruction to go to the key. Since DREAM already
read that the sign said <b style="color:blue">blue</b> during the exploration
episode, it goes to the <b style="color:blue">blue</b> key.</p>

<h3 id="behaviors-learned-by-dream">Behaviors learned by DREAM</h3>

<figure class="figure"><div class="figure__main">
<figure class="postfigurehalf">
  <img src="/blog/assets/img/posts/2020-08-23-meta-exploration/resize_explore.gif" />
  <figcaption>
  Exploration.
  </figcaption>
</figure>
<figure class="postfigurehalf">
  <img src="/blog/assets/img/posts/2020-08-23-meta-exploration/resize_execute.gif" />
  <figcaption>
  Execution: go to the key.
  </figcaption>
</figure>
</div></figure>

<p><strong>Comparisons.</strong> Broadly, prior meta-RL approaches fall into two main
groups: (i) end-to-end approaches, where exploration and execution are
optimized end-to-end based on execution rewards, and (ii) decoupled
approaches, where exploration and execution are optimized with separate
objectives. We compare DREAM with state-of-the-art approaches from both
categories. In the end-to-end category, we compare with:</p>

<ul>
  <li>
    <p>RL<script type="math/tex">^2</script><sup id="fnref:duan2016fast"><a href="#fn:duan2016fast" class="footnote">1</a></sup><sup id="fnref:wang2016learning"><a href="#fn:wang2016learning" class="footnote">2</a></sup>, the canonical
end-to-end approach, which learns a recurrent policy conditioned
on the entire sequence of past state and reward observations.</p>
  </li>
  <li>
    <p>VariBAD<sup id="fnref:zintgraf2019varibad"><a href="#fn:zintgraf2019varibad" class="footnote">3</a></sup>, which additionally adds auxiliary
loss functions to the hidden state of the recurrent policy to
predict the rewards and dynamics of the current problem. This can
be viewed as learning the belief state<sup id="fnref:kaelbling1998planning"><a href="#fn:kaelbling1998planning" class="footnote">4</a></sup>, a
sufficient summary of all of its past observations.</p>
  </li>
  <li>
    <p>IMPORT<sup id="fnref:kamienny2020learning"><a href="#fn:kamienny2020learning" class="footnote">5</a></sup>, which additionally leverages the
problem identifier to help learn execution behaviors.</p>
  </li>
</ul>

<p>Additionally, in the decoupled category, we compare with:</p>

<ul>
  <li>PEARL-UB, an upperbound on PEARL<sup id="fnref:rakelly2019efficient"><a href="#fn:rakelly2019efficient" class="footnote">6</a></sup>. We
analytically compute the expected rewards achieved by the optimal
problem-specific policy that explores with
Thompson sampling<sup id="fnref:thompson1933likelihood"><a href="#fn:thompson1933likelihood" class="footnote">7</a></sup> using the true posterior
distribution over problems.</li>
</ul>

<p><strong>Quantitative results.</strong> Below, we plot the returns achieved by all
approaches. In contrast to DREAM, which achieves near-optimal returns,
we find that the end-to-end approaches never read the sign, and
consequently avoid all objects, in fear of receiving negative reward for
going to the wrong object. This happens even when they are allowed to
observe rewards in the exploration episode (dotted lines). Therefore,
they achieve no rewards, which is indicative of the coupling problem.</p>

<p>On the other hand, while existing approaches in the decoupled category
avoid the coupling problem, optimizing their objectives does not lead to
the optimal exploration policy. For example, Thompson sampling
approaches (PEARL-UB) do not achieve optimal reward, even with the
optimal problem-specific execution policy and access to the true
posterior distribution over problems. To see this, recall that Thompson
sampling explores by sampling a problem from the posterior distribution
and following the execution policy for that problem. Since the optimal
execution policy directly goes to the correct object, and never reads
the sign, Thompson sampling never reads the sign during exploration. In
contrast, a nice property of DREAM is that with enough data and
expressive-enough policy classes, it theoretically learns optimal
exploration and execution.</p>

<figure class="figure"><div class="figure__main">
<p><img src="/blog/assets/img/posts/2020-08-23-meta-exploration/results.png" /></p>
<figcaption>
Training curves with (dotted lines) and without (solid lines) rewards
during exploration. Only DREAM reads the sign and solves the task. And
it does it without needing rewards during exploration!
</figcaption>
</div></figure>

<p><strong>Additional results.</strong> In our paper, we also evaluate DREAM on
additional didactic problems, designed to to answer the following
questions:</p>

<ul>
  <li>
    <p>Can DREAM efficiently explore to discover only the information
required to execute instructions?</p>
  </li>
  <li>
    <p>Can DREAM generalize to unseen instructions and environments?</p>
  </li>
  <li>
    <p>Does DREAM also show improved results in the standard meta-RL
setting, as well as instruction-based meta-RL?</p>
  </li>
</ul>

<p>Broadly, the answer is yes to all of these questions. Check out our
paper for detailed results!</p>

<h2 id="conclusion">Conclusion</h2>

<p><strong>Summary.</strong> In this blog post, we tackled the problem of
meta-exploration: how to best gather information in a new environment in
order to perform a task. To do this, we examined and addressed two key
challenges.</p>

<ul>
  <li>
    <p>First, we saw how existing meta-RL approaches that optimize both
exploration and execution <em>end-to-end</em> to maximize reward fall
prey to a chicken-and-egg problem. If the agent hasn’t learned to
explore yet, then it can’t gather key information (e.g., the
location of ingredients) required for learning to solve tasks
(e.g., cook a meal). On the other hand, if the agent hasn’t
learned to solve tasks yet, then there’s no signal for learning to
explore, as it’ll fail to solve the task no matter what. We
avoided this problematic cycle by proposing a <em>decoupled</em>
objective (DREAM), which learns to explore and learns to solve
tasks independently from each other.</p>
  </li>
  <li>
    <p>Second, we saw how the standard meta-RL setting captures the notion
of adapting to a new environment and task, but requires the agent
to unnecessarily explore to infer the task (e.g., what meal to
cook) and doesn’t leverage the shared structure between different
tasks in the <em>same</em> environment (e.g., cooking different meals in
the same kitchen). We addressed this by proposing
instruction-based meta-RL (IMRL), which provides the agent with an
instruction that specifies the task and requires the agent to
explore and gather information useful for <em>many</em> tasks.</p>
  </li>
</ul>

<p>DREAM and IMRL combine quite nicely: IMRL enables reward-free adaptation
in principle, and DREAM achieves it in practice. Other state-of-the-art
approaches we tested weren’t able to achieve reward-free adaptation, due
to the chicken-and-egg coupling problem.</p>

<p><strong>What’s next?</strong> There’s a lot of room for future work — here are a
few directions to explore.</p>

<ul>
  <li>
    <p><em>More sophisticated instruction and problem ID representations.</em>
This work examines the case where the instructions and problem IDs
are represented as unique one-hots, as a proof of concept. Of
course, in the real world, instructions and problem IDs might be
better represented with natural language, or images (e.g., a
picture of the meal to cook).</p>
  </li>
  <li>
    <p><em>Applying DREAM to other meta-RL settings.</em> DREAM applies generally
to any meta-RL setting where some information is conveyed to the
agent and the rest must be discovered via exploration. In this
work, we studied two such instances — in IMRL, the instruction
conveys the task and in the standard meta-RL setting, everything
must be discovered via exploration — but there are other settings
worth examining, too. For example, we might want to convey
information about the environment to the agent, such as the
locations of some ingredients, or that the left burner is broken,
so the robot chef should use the right one.</p>
  </li>
  <li>
    <p><em>Seamlessly integrating exploration and execution.</em> In the most
commonly studied meta-RL setting, the agent is allowed to first
gather information via exploration (exploration episode) before
then solving tasks (execution episodes). This is also the setting
we study, and it can be pretty realistic. For example, a robot
chef might require a setup phase, where it first explores a home
kitchen, before it can start cooking meals. On the other hand, a
few works, such as Zintgraf et al., 2019, require the agent to
start solving tasks from the get-go: there are no exploration
episodes and all episodes are execution episodes. DREAM can
already operate in this setting, by just ignoring the rewards and
exploring in the first execution episode, and trying to make up
for the first execution episode with better performance in later
execution episodes. This works surprisingly well, but it’d be nice
to more elegantly integrate exploration and execution.</p>
  </li>
</ul>

<h2 id="acknowledgements">Acknowledgements</h2>

<p>This is work done with my fantastic collaborators
<a href="https://stanford.edu/~aditir/">Aditi Raghunathan</a>,
<a href="https://cs.stanford.edu/~pliang/">Percy Liang</a>,
and <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a>.
You can check out our <a href="https://arxiv.org/abs/2008.02790">full paper on ArXiv</a>
and our <a href="https://github.com/ezliu/dream">source code on GitHub</a>.
You can also find a short talk on this work <a href="https://youtu.be/EiIC0Rkz8-s">here</a>.</p>

<p>Many thanks to <a href="https://www.andreykurenkov.com/">Andrey Kurenkov</a>
for comments and edits on this blog post!</p>

<p>The icons used in the above figures were made by Freepik, ThoseIcons,
dDara, mynamepong, Icongeek26, photo3idea_studio and Vitaly Gorbachev
from <a href="https://www.flaticon.com/">flaticon.com</a>.</p>

<figure class="figure"><div class="figure__main">
<p><img src="/blog/assets/img/posts/2020-08-23-meta-exploration/collaborators.svg" /></p>
</div></figure>

<div class="footnotes">
  <ol>
    <li id="fn:duan2016fast">
      <p>Y. Duan, J. Schulman, X. Chen, P. L. Bartlett, I. Sutskever, and P. Abbeel. RL2: Fast reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779, 2016. <a href="#fnref:duan2016fast" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:wang2016learning">
      <p>J. X. Wang, Z. Kurth-Nelson, D. Tirumala, H. Soyer, J. Z. Leibo, R. Munos, C. Blundell, D. Kumaran, and M. Botvinick. Learning to reinforcement learn. arXiv preprint arXiv:1611.05763, 2016. <a href="#fnref:wang2016learning" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:zintgraf2019varibad">
      <p>L. Zintgraf, K. Shiarlis, M. Igl, S. Schulze, Y. Gal, K. Hofmann, and S. Whiteson.  VariBAD A very good method for bayes-adaptive deep RL via meta-learning. arXiv preprint arXiv:1910.08348, 2019. <a href="#fnref:zintgraf2019varibad" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:kaelbling1998planning">
      <p>L. P. Kaelbling, M. L. Littman, and A. R. Cassandra. Planning and acting in partially observable stochastic domains. Artificial intelligence, 101(1):99–134, 1998. <a href="#fnref:kaelbling1998planning" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:kamienny2020learning">
      <p>P. Kamienny, M. Pirotta, A. Lazaric, T. Lavril, N. Usunier, and L. Denoyer. Learning adaptive exploration strategies in dynamic environments through informed policy regularization. arXiv preprint arXiv:2005.02934, 2020. <a href="#fnref:kamienny2020learning" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:rakelly2019efficient">
      <p>K. Rakelly, A. Zhou, D. Quillen, C. Finn, and S. Levine. Efficient off-policy meta-reinforcement learning via probabilistic context variables. arXiv preprint arXiv:1903.08254, 2019. <a href="#fnref:rakelly2019efficient" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:thompson1933likelihood">
      <p>W. R. Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3):285–294, 1933. <a href="#fnref:thompson1933likelihood" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>

  
  </section>
  <hr>
  Keep on top of the latest SAIL Blog posts via <a class="social-icon" href="http://0.0.0.0:4000/blog/feed.xml">RSS <i class="fa fa-rss-square fa fa" title="Twitter"></i></a>, <a class="social-icon" href="https://twitter.com/StanfordAILab">Twitter <i class="fa fa-twitter-square fa fa" title="Twitter"></i></a>, or email:

<!-- Begin Mailchimp Signup Form -->
<link href="//cdn-images.mailchimp.com/embedcode/horizontal-slim-10_7.css" rel="stylesheet" type="text/css">
<style type="text/css">
	#mc_embed_signup{background:#fff; clear:left; font:14px Helvetica,Arial,sans-serif; width:100%;}
	/* Add your own Mailchimp form style overrides in your site stylesheet or in this style block.
	   We recommend moving this block and the preceding CSS link to the HEAD of your HTML file. */
</style>
<div id="mc_embed_signup">
<form action="https://stanford.us19.list-manage.com/subscribe/post?u=3a6484754abf2fc18724ec835&amp;id=028823e92b" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <div id="mc_embed_signup_scroll">
	
	<input type="email" value="" name="EMAIL" class="email" id="mce-EMAIL" placeholder="email address" required>
    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_3a6484754abf2fc18724ec835_028823e92b" tabindex="-1" value=""></div>
    <div class="clear"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button"></div>
    </div>
</form>
</div>

<!--End mc_embed_signup-->



  <!-- Social media shares -->
  <div class="share-buttons">
    <ul class="share-buttons">
        <div class="meta">Share</div>
        
        <li>
            <a href="https://www.facebook.com/sharer/sharer.php?u=http://0.0.0.0:4000/blog/meta-exploration/" target="_blank" title="Share on Facebook">
			<i class="fa fa-facebook-square fa-2x" aria-hidden="true"></i>
			<span class="sr-only">Share on Facebook</span>
		</a>
        </li>
         
        <li>
            <a href="https://twitter.com/intent/tweet?source=http://0.0.0.0:4000/blog/meta-exploration/&text=Explore+then+Execute%3A+Adapting+without+Rewards+via+Factorized+Meta-Reinforcement+Learning%20%7C%20SAIL+Blog:%20http://0.0.0.0:4000/blog/meta-exploration/" target="_blank" title="Tweet">
			<i class="fa fa-twitter-square fa-2x" aria-hidden="true"></i>
			<span class="sr-only">Tweet</span>
		</a>
        </li>
            
        <li>
            <a href="https://getpocket.com/save?url=http://0.0.0.0:4000/blog/meta-exploration/&title=Explore+then+Execute%3A+Adapting+without+Rewards+via+Factorized+Meta-Reinforcement+Learning%20%7C%20SAIL+Blog" target="_blank" title="Add to Pocket">
			<i class="fa fa fa-get-pocket fa-2x" aria-hidden="true"></i>
			<span class="sr-only">Add to Pocket</span>
		</a>
        </li>
         
        <li>
            <a href="http://www.reddit.com/submit?url=http://0.0.0.0:4000/blog/meta-exploration/&title=Explore+then+Execute%3A+Adapting+without+Rewards+via+Factorized+Meta-Reinforcement+Learning%20%7C%20SAIL+Blog" target="_blank" title="Share on Reddit">
			<i class="fa fa-reddit-square fa-2x" aria-hidden="true"></i>
			<span class="sr-only">Share on Reddit</span>
		</a>
        </li>
           
        <li>
            <a href="mailto:?subject=Explore+then+Execute%3A+Adapting+without+Rewards+via+Factorized+Meta-Reinforcement+Learning%20%7C%20SAIL+Blog&body=:%20http://0.0.0.0:4000/blog/meta-exploration/" target="_blank" title="Email">
			<i class="fa fa-envelope-square fa-2x" aria-hidden="true"></i>
			<span class="sr-only">Email</span>
		</a>
        </li>
        
    </ul>
</div>

  <!-- Tag list -->
  
  


<footer>
  <div class="tag-list">
    
      <div class="meta">Tags</div>
    

    
    <a class="button" href="/blog/tags#meta-learning">
      <p><i class="fa fa-tag fa-fw"></i> meta-learning</p>
    </a>
    
    <a class="button" href="/blog/tags#ml">
      <p><i class="fa fa-tag fa-fw"></i> ml</p>
    </a>
    
    <a class="button" href="/blog/tags#rl">
      <p><i class="fa fa-tag fa-fw"></i> rl</p>
    </a>
    
  </div>
</footer>

</article>

<!-- Disqus -->


<!-- Post navigation -->

  <div id="post-nav">
  
  <div id="previous-post" class="post-nav-post">
      <a href="/blog/eccv-2020/">
      <p>Previous post</p>
        Stanford AI Lab Papers and Talks at ECCV 2020
      </a>
  </div>
  
  
  <div id="next-post" class="post-nav-post">
      <a href="/blog/black-box-safety-validation/">
      <p>Next post</p>
        Safety Validation of Black-Box Autonomous Systems
      </a>
  </div>
  
</div>



    </div>
    <hr>
<footer class="site-footer">
    
   <div class="footer-icons">
        <ul>
        <!-- Social icons from Font Awesome, if enabled -->
        
<li>
	<a href="http://0.0.0.0:4000/blog/feed.xml" title="Follow RSS feed">
		<span class="fa-stack fa-lg">
            <i class="fa fa-circle fa-stack-2x"></i>
            <i class="fa fa-rss fa-stack-1x fa-inverse"></i>
        </span>
	</a>
</li>







































<li>
	<a href="https://twitter.com/StanfordAILab" title="Follow on Twitter" class="type">
        <span class="fa-stack fa-lg">
            <i class="fa fa-circle fa-stack-2x"></i>
            <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
        </span>
	</a>
</li>








        </ul>
    </div>
    <p class="text">&copy; 2019 Stanford AI Lab
</p>
 
</footer>



  </body>
</html>
