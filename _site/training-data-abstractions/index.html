<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <!-- Main JS (navbar.js, katex_init.js and masonry_init.js)-->
    <script defer=true src="/blog/assets/js/main.min.js"></script>
    
    <!-- CSS -->
    <link rel="stylesheet" href="/blog/assets/css/main.css">

    <!--Favicon-->
    <link rel="icon" type="image/png" href="/blog/assets/img/favicon-32x32.png" sizes="32x32" />
    <link rel="icon" type="image/png" href="/blog/assets/img/favicon-16x16.png" sizes="16x16" />

    <!-- Canonical -->
    <link rel="canonical" href="http://0.0.0.0:4000/blog/training-data-abstractions/">

    <!-- RSS -->
    <link rel="alternate" type="application/atom+xml" title="The Stanford AI Lab Blog" href="http://0.0.0.0:4000/blog/feed.xml"/>

    <!-- Font Awesome -->
    <!-- <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet"> -->
    <link rel="stylesheet" type="text/css" href="/blog/assets/css/vendor/font-awesome.min.css">
    
    <!-- Bootstrap-3.3.7 isolation CSS -->
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" integrity="sha384-JcKb8q3iqJ61gNV9KGb8thSsNjpSL0n8PARn9HuZOnIxN0hoP+VmmDGMN5t9UJ0Z" crossorigin="anonymous">


    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js" integrity="sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha384-B4gt1jrGC7Jh4AgTPSdUtOBvfO8shuf57BaghqFfPlYxofvL8/KUEfYiJOMMV+rV" crossorigin="anonymous"></script>
    
    <!-- JQuery 2.2.4 -->
    <script src="https://code.jquery.com/jquery-2.2.4.min.js" integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin="anonymous"></script>
    
    <link rel="stylesheet" type="text/css" href="/blog/assets/css/vendor/bigfoot-number.css">
    <script type="text/javascript" src="/blog/assets/js/vendor/bigfoot.min.js"></script>
    <script type="text/javascript">
    $(document).ready(function() { 
      $.bigfoot();
      
    window.onload = function() {

        var videos = document.getElementsByTagName("video"),
            fraction = 0.8;

        function checkScroll() {

            for (var i = 0; i < videos.length; i++) {

                var video = videos[i];

                var x = video.offsetLeft,
                    y = video.offsetTop,
                    w = video.offsetWidth,
                    h = video.offsetHeight,
                    r = x + w, //right
                    b = y + h, //bottom
                    visibleX, visibleY, visible;

                visibleX = Math.max(0, Math.min(w, window.pageXOffset + window.innerWidth - x, r - window.pageXOffset));
                visibleY = Math.max(0, Math.min(h, window.pageYOffset + window.innerHeight - y, b - window.pageYOffset));

                visible = visibleX * visibleY / (w * h);

                if (visible > fraction) {
                    video.play();
                } else {
                    video.pause();
                }

            }

        }

        window.addEventListener('scroll', checkScroll, false);
        window.addEventListener('resize', checkScroll, false);
    }
    }); 
    </script>
    


    <!-- Google Fonts -->
    
    <link href="//fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,700italic,400italic" rel="stylesheet" type="text/css"> 
    

    <!-- KaTeX 0.8.3 -->
    
    <link rel="stylesheet" type="text/css" href="/blog/assets/css/vendor/katex.min.css">
    <script src="/blog/assets/js/vendor/katex.min.js">
    </script>
    
    
    
    <!-- Google Analytics -->
    
    <script>
        (function(i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r;
            i[r] = i[r] || function() {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date();
            a = s.createElement(o),
                m = s.getElementsByTagName(o)[0];
            a.async = 1;
            a.src = g;
            m.parentNode.insertBefore(a, m)
        })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'UA-129018108-1', 'auto');
        ga('send', 'pageview');

    </script>
    
    
    <!-- seo tags -->
    <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Powerful Abstractions for Programmatically Building and Managing Training Sets | SAIL Blog</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Powerful Abstractions for Programmatically Building and Managing Training Sets" />
<meta name="author" content="<a href='https://vincentsc.com'>Vincent S. Chen</a>, <a href='https://stanford.edu/~senwu/'>Sen Wu</a>, <a href='https://www.bradenhancock.com'>Braden Hancock</a>, <a href='https://ajratner.github.io'>Alex Ratner</a>, <a href='https://cs.stanford.edu/people/chrismre/'>Chris Ré</a>, and&nbsp;<a href='https://cs.stanford.edu/people/chrismre/#students'>other&nbsp;members&nbsp;of&nbsp;Hazy&nbsp;Lab</a>" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Overview" />
<meta property="og:description" content="Overview" />
<link rel="canonical" href="http://0.0.0.0:4000/blog/training-data-abstractions/" />
<meta property="og:url" content="http://0.0.0.0:4000/blog/training-data-abstractions/" />
<meta property="og:site_name" content="SAIL Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-06-21T00:00:00-07:00" />
<script type="application/ld+json">
{"description":"Overview","author":{"@type":"Person","name":"<a href='https://vincentsc.com'>Vincent S. Chen</a>, <a href='https://stanford.edu/~senwu/'>Sen Wu</a>, <a href='https://www.bradenhancock.com'>Braden Hancock</a>, <a href='https://ajratner.github.io'>Alex Ratner</a>, <a href='https://cs.stanford.edu/people/chrismre/'>Chris Ré</a>, and&nbsp;<a href='https://cs.stanford.edu/people/chrismre/#students'>other&nbsp;members&nbsp;of&nbsp;Hazy&nbsp;Lab</a>"},"@type":"BlogPosting","url":"http://0.0.0.0:4000/blog/training-data-abstractions/","headline":"Powerful Abstractions for Programmatically Building and Managing Training Sets","dateModified":"2019-06-21T00:00:00-07:00","datePublished":"2019-06-21T00:00:00-07:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://0.0.0.0:4000/blog/training-data-abstractions/"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    
    <!-- Manual seo tags -->
    <title>Powerful Abstractions for Programmatically Building and Managing Training Sets | The Stanford AI Lab Blog</title>
    <meta name="description" content="Machine learning practitioners are spending less time on model architectures and hardware optimizations and, instead, focusing on training data. We describe ...">
    
    <!-- Twitter Cards -->
    <meta name="twitter:title" content="Powerful Abstractions for Programmatically Building and Managing Training Sets">
    
    <meta name="twitter:description" content="Machine learning practitioners are spending less time on model architectures and hardware optimizations and, instead, focusing on training data. We describe three powerful abstractions that practitioners can use to programmatically build and manage their training data.">
    
    <meta name="twitter:creator" content="@StanfordAILab">
    
    <meta name="twitter:card" content="summary">
    <meta name="twitter:image" content="http://0.0.0.0:4000/blog/assets/img/posts/2019-06-21-training-data-abstractions/fig_abstractions_thumbnail.png">
    <meta name="og:image" content="http://0.0.0.0:4000/blog/assets/img/posts/2019-06-21-training-data-abstractions/fig_abstractions_thumbnail.png">
    
</head>

  <body class="post-body">
      <!-- Toggle menu -->
<header class="site-header">

<nav class="clear navbar navbar-expand-lg navbar-light bg-white flex-column flex-md-row bd-navbar fixed-top" id="main_nav">
  
  <div class="container">

    <a class="navbar-brand mr-0 mr-md-2 text-black d-flex align-items-center" href="/blog/" aria-label="Bootstrap">
    
	  <div class="branding">
	    <a href="http://ai.stanford.edu/">
	      <img class="avatar" src="/blog/assets/img/sail-logo.png" alt=""/>
		  </a>

      <a href="/blog/">
	      <h1 class="site-title">
			    The Stanford AI Lab Blog
		    </h1> 
		  </a>
	  </div>
    
    
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
    </button>
    
    <div class="collapse navbar-collapse" id="navbarResponsive">

      <ul class="navbar-nav ml-auto">
      
      
      <li class="nav-item">
      <a class="nav-link" href="/blog/about">About</a>
      </li>
      
      <li class="nav-item dropdown">
        <a class="nav-link dropdown-toggle" data-toggle="dropdown" href="#" role="button" aria-haspopup="true" aria-expanded="false">Posts</a>
        <div class="dropdown-menu">
        <a class="dropdown-item" href="/blog/">All</a>
        <a class="dropdown-item" href="/blog/conferences">Conferences</a>
        <a class="dropdown-item" href="/blog/vision">Computer Vision</a>
        <a class="dropdown-item" href="/blog/robotics">Robotics</a>
        <a class="dropdown-item" href="/blog/nlp">NLP</a>
        <a class="dropdown-item" href="/blog/ml">Machine Learning</a>
        <a class="dropdown-item" href="/blog/rl">Reinforcement Learning</a>

        </div>
      </li>
        
      
      <li class="nav-item">
      <a class="nav-link" href="/blog/subscribe">Subscribe</a>
      </li>
      
      
      <li class="nav-item">
      <a class="nav-link" href="http://ai.stanford.edu/">SAIL</a>
      </li>
      
      </ul> 

    </div>

  </div>
</nav>

</header>

  
    <div class="content">
      

<article>

  <header id="main">
    
    <h1 id="post_title">Powerful Abstractions for Programmatically Building and Managing Training Sets</h1>
    <p class="meta">
    <a href='https://vincentsc.com'>Vincent S. Chen</a>, <a href='https://stanford.edu/~senwu/'>Sen Wu</a>, <a href='https://www.bradenhancock.com'>Braden Hancock</a>, <a href='https://ajratner.github.io'>Alex Ratner</a>, <a href='https://cs.stanford.edu/people/chrismre/'>Chris Ré</a>, and&nbsp;<a href='https://cs.stanford.edu/people/chrismre/#students'>other&nbsp;members&nbsp;of&nbsp;Hazy&nbsp;Lab</a>
    <div class="post-date">June 21, 2019</div>
    </p>
  <hr>
  </header>


  <section class="post-content">
  
    <h2 id="overview"><strong>Overview</strong></h2>

<p>Machine learning practitioners are spending less time on model architectures and hardware optimizations and, instead, focusing on training data. As a result, programmers are relying on different abstractions—high-level design patterns—to build machine learning pipelines for their applications. In this post, we describe three powerful abstractions that practitioners can use to programmatically build and manage their training data.</p>

<p>We ran an experiment to test the effectiveness of basic training data operations—applying a handful of these using our framework, <a href="http://snorkel.stanford.edu">Snorkel</a>, and a standard NLP model (i.e. BERT) yields a state-of-the-art result on <a href="https://super.gluebenchmark.com/">SuperGLUE</a><sup id="fnref:superglue"><a href="#fn:superglue" class="footnote">1</a></sup>—a newly curated benchmark with six tasks for evaluating “general-purpose language understanding technologies”. Compared to the recent advances in natural language pretraining (i.e. BERT), we achieve a <em><mark>new state-of-the-art score overall and the highest reported score anywhere on a majority of component tasks</mark></em>.</p>

<p>Beyond SuperGLUE, we also highlight updates on Snorkel’s use in the real world with even more applications—from industrial scale at <a href="https://ai.googleblog.com/2019/03/harnessing-organizational-knowledge-for.html">Google’s Snorkel Drybell</a> to scientific work in <a href="https://nature-research-under-consideration.nature.com/users/37265-nature-communications/posts/38921-weakly-supervised-classification-of-rare-aortic-valve-malformations-using-unlabeled-cardiac-mri-sequences">MRI classification</a> and <a href="https://ai.stanford.edu/~kuleshov/papers/gwaskb-manuscript.pdf">automated Genome-wide association study (GWAS) curation</a> (both accepted in <a href="https://www.nature.com/ncomms/">Nature Comms</a>)!</p>

<p>We will be releasing code in the <a href="https://github.com/HazyResearch/snorkel">Snorkel repo</a> for reproducing and building on our results in conjunction with a 2-day <strong>Snorkel workshop</strong> during the last week  of June with collaborators from science, industry, and government. This workshop is unfortunately already completely full, but if you would like to be notified of future Snorkel workshops, please provide your name and contact information <a href="https://docs.google.com/forms/d/e/1FAIpQLScOpiImyBA3uk_CnJ03R1b7Ese9VA3XjfLnemCO76WyTwrO5Q/viewform?usp=sf_link">here</a>.</p>

<h2 id="three-key-abstractions"><strong>Three key abstractions</strong></h2>

<p>In our SuperGLUE result, as well as more generally, we find that spending our time programmatically building and manipulating the training data—rather than the models— provides a powerful and effective strategy to achieve high performance in ML pipelines. In a past <a href="https://dawn.cs.stanford.edu/2019/03/22/glue/">post</a>, we talked about the value of incorporating more supervision signal from more sources, e.g. multi-task learning and transfer learning, as we achieved state-of-the-art results on the GLUE Benchmark (a precursor to SuperGLUE). In this post, we focus on three key abstractions for building and modifying training datasets:</p>

<ol>
  <li><strong>Labeling data</strong> with labeling functions (LFs) <sup id="fnref:dp"><a href="#fn:dp" class="footnote">2</a></sup></li>
  <li><strong>Transforming data</strong> with transformation functions (TFs) <sup id="fnref:tanda"><a href="#fn:tanda" class="footnote">3</a></sup> <sup id="fnref:autoaugment"><a href="#fn:autoaugment" class="footnote">4</a></sup></li>
  <li><strong>Slicing data</strong> with slicing functions (SFs) [<em>technical report + blog post coming soon!</em>]</li>
</ol>

<figure class="figure"><div class="figure__main">
<p><a href="/blog/assets/img/posts/2019-06-21-training-data-abstractions/fig_abstractions.png"><img class="postimage_75" src="/blog/assets/img/posts/2019-06-21-training-data-abstractions/fig_abstractions.png" /></a></p>
</div></figure>

<h3 id="running-example"><strong>Running Example</strong></h3>

<p>For the remainder of this post, we use a running example from the Words in Context (WiC) task from SuperGLUE: <em>is the target word being used in the same way in both sentences?</em></p>

<figure class="figure"><div class="figure__main">
<p><a href="/blog/assets/img/posts/2019-06-21-training-data-abstractions/example.png"><img src="/blog/assets/img/posts/2019-06-21-training-data-abstractions/example.png" /></a></p>
</div></figure>

<h2 id="1-weak-labeling-with-labeling-functions"><strong>1. Weak labeling with labeling functions</strong></h2>

<p>In many applications, unlabeled data is abundant—it may come from fleets of autonomous vehicles, or large corpora of unstructured data. Modern architectures are largely unable to take advantage of such potentially rich datasets because labeling them is intractable due to time or cost. With <a href="https://hazyresearch.github.io/snorkel/">Snorkel</a>, we’ve studied for years the use of <strong>labeling functions (LFs)</strong> for heuristically labeling training examples. LFs provide domain experts or machine learning practitioners with an intuitive interface for denoising and combining supervision sources from existing datasets, models, or crowd labelers.</p>

<figure class="figure"><div class="figure__main">
<p><a href="/blog/assets/img/posts/2019-06-21-training-data-abstractions/lf_ex.png"><img src="/blog/assets/img/posts/2019-06-21-training-data-abstractions/lf_ex.png" /></a></p>
<figcaption>
  For the WiC task (identifying whether a target word is used with the same "sense" in two sentences) we might consider weakly labeling examples based on whether or not they share a trigram including the target word.
</figcaption>
</div></figure>

<h2 id="2-augmenting-data-with-transformation-functions"><strong>2. Augmenting data with transformation functions</strong></h2>

<p>Often, people think about data augmentation in terms of simple transformations—randomly rotating or stretching images—but they can refer to much more diverse range of operations. We see <strong>transformation functions (TFs)</strong> as a powerful abstraction that heuristically generates new, modified examples from existing ones. For instance, for a medical imaging task, we might write TFs to perform transformations that are specific to our imaging modality—e.g. resampling segmenting tumor masses or resampling background tissue. We have explored this abstraction in our own work, TANDA <sup id="fnref:tanda-2"><a href="#fn:tanda-2" class="footnote">5</a></sup>, which seeks to learn compositions of transformations across domain-specific tasks. AutoAugment <sup id="fnref:autoaugment-2"><a href="#fn:autoaugment-2" class="footnote">6</a></sup> from Google builds on this work to automatically learn policies for augmentation strategies.</p>

<figure class="figure"><div class="figure__main">
<p><a href="/blog/assets/img/posts/2019-06-21-training-data-abstractions/tf_ex.png"><img src="/blog/assets/img/posts/2019-06-21-training-data-abstractions/tf_ex.png" /></a></p>
<figcaption>
  Given that “Sunday” does not change the word sense of “invite”, we can transform an example that includes the word “Sunday” into many copies of that example with different days of the week so that our model is likely to overfit.
</figcaption>
</div></figure>

<h2 id="3-slicing-data-with-slicing-functions-new-idea"><strong>3. Slicing data with slicing functions (<mark>new idea</mark>!)</strong></h2>

<p>In many datasets, especially in real-world applications, there are subsets of the data that our model underperforms on, or that we care more about performing well on than others. For example, a model may underperform on lower-frequency healthcare demographics (e.g. younger patients with certain cancers) or we may care extra about model performance on safety-critical but rare scenarios in an autonomous driving setting, such as detecting cyclists. We call these data subsets <em>slices</em>. The technical challenge often faced by practitioners is to improve performance on these slices while maintaining overall performance.</p>

<p><strong>Slicing functions (SFs)</strong> provide an interface for users to coarsely identify data subsets for which the model should commit additional representational capacity. To address slice-specific representations, practitioners might train many models that each specialize on particular subsets, and then combine these with a mixture-of-experts (MoE) approach <sup id="fnref:moe"><a href="#fn:moe" class="footnote">7</a></sup>. However, with the growing size of ML models, MoE is often impractical. Another strategy would be to train a single model in the style of multi-task learning (MTL) with hard parameter sharing <sup id="fnref:mtl"><a href="#fn:mtl" class="footnote">8</a></sup>. While more computationally efficient, this approach expects representation bias across many slice-specific tasks to improve performance—an often unreliable approach. As a quick overview (<em>technical report + blog post coming soon!</em>)— we model slices in the style of multi-task learning, in which slice-based “expert-heads” are used to learn slice-specific representations. Then, an attention mechanism is learned over expert heads to determine when and how to combine the representations learned by these slice heads on a per-example basis.</p>

<p>We consider the following properties of our approach:</p>

<ul>
  <li>Our approach is <strong>model-agnostic</strong> — expert heads are learned on top of any backbone architecture (e.g. BERT, ResNET). As a result, practitioners improving performance with slicing functions can focus on the data rather than the model architecture.</li>
  <li>By learning in a multi-task fashion, we <strong>efficiently learn representations</strong> without the need to make many copies of the model (i.e. MoE requires too much memory)!</li>
  <li>By incorporating the attention mechanism, we <strong>avoid manual tuning</strong> of expert-heads—an otherwise significant developer cost.</li>
</ul>

<figure class="figure"><div class="figure__main">
<p><a href="/blog/assets/img/posts/2019-06-21-training-data-abstractions/sf_ex.png"><img src="/blog/assets/img/posts/2019-06-21-training-data-abstractions/sf_ex.png" /></a></p>
<figcaption>
  From WiC error analysis, we might find that our model appears to perform worse on examples where the target word is a noun instead of a verb. Using an SF, we tell the model to pay attention to the differences between these slices and use a slightly different representation when making predictions for target words that it believes are nouns.
</figcaption>
</div></figure>

<h2 id="key-properties-of-lfs-tfs-and-sfs"><strong>Key properties of LFs, TFs, and SFs</strong></h2>

<ul>
  <li><strong>Intuitive interfaces</strong>: These abstractions provide intuitive interfaces to existing practitioner workflows. They allow insights from debugging/error analysis to be directly encoded to improve models.</li>
  <li><strong>Programming abstractions as weak supervision</strong>: In practice, many of these techniques can be viewed as a form of weak supervision, as users specify them in noisy, heuristic, and imprecise ways. Dealing with this is one of the core technical challenges we tackle with Snorkel.</li>
  <li><strong>Supervision as code</strong>: These types of inputs are ways of supervising a model (i.e. they specify training sets). Concretely, they are also code, and thus carry many of the advantages of code—reusability, modifiability, etc.</li>
</ul>

<h2 id="superglue-results"><strong>SuperGLUE Results</strong></h2>

<p>Using these programming abstractions, we achieved a new state-of-the-art score on the SuperGLUE Benchmark and 4 of its components tasks. SuperGLUE is similar to <a href="https://gluebenchmark.com/">GLUE</a>, but contains “more difficult tasks…chosen to maximize difficulty and diversity, and…selected to show a substantial headroom gap between a strong BERT-based baseline and human performance.” After reproducing the BERT++ baselines, we minimally tuned these models (baseline models, default learning rate, etc.) and found that with a handful of applications of the above programming abstractions, we saw improvements of +4.0 points on the SuperGLUE benchmark (21% reduction of the gap to human performance).</p>

<h2 id="snorkel-in-the-real-world"><strong>Snorkel in the Real World</strong></h2>

<p>These Snorkel programming abstractions have also been used to fuel progress in high-impact real-world applications.</p>

<p>In March of this year, we published a <a href="https://arxiv.org/pdf/1812.00417.pdf">paper</a> and <a href="https://ai.googleblog.com/2019/03/harnessing-organizational-knowledge-for.html">blog post</a> with Google on the lessons learned from deploying Snorkel at industrial scale. Relying on diverse sources of knowledge across the organization—heuristics, taggers, knowledge graphs, legacy systems, etc.—they saw significant improvements in quality, by as much as 17.5 F1 points.</p>

<figure class="figure"><div class="figure__main">
<p><a href="/blog/assets/img/posts/2019-06-21-training-data-abstractions/bav.jpg"><img src="/blog/assets/img/posts/2019-06-21-training-data-abstractions/bav.jpg" /></a></p>
<figcaption>
  The Snorkel pipeline, deployed on the BAV classification task for large collections of up to 4,000 unlabeled MRI sequences. Figure credit to Fries et. al 2018.
</figcaption>
</div></figure>

<p>In <a href="https://www.biorxiv.org/content/10.1101/339630v4.full">recent work</a> that was accepted to Nature Communications, Snorkel was deployed in an ongoing collaboration with <a href="https://priestlab.stanford.edu/">Stanford University Pediatric Cardiology</a>, where labeled training data is a significant practical roadblock to developing automated methods. We focused on bicuspid aortic valve (BAV), the most common congenital heart malformation (with an incidence rate of 0.5-2% in the general population), with risk of adverse downstream health effects. Instead of relying on costly MRI labels from cardiologists, we worked directly with domain experts to develop LFs to generate large-scale training sets for downstream deep learning models. In patients identified by our end-to-end approach, an independent evaluation determined a 1.8-fold increase in risk for major adverse cardiac events.</p>

<p>In another forthcoming Nature Communications <a href="https://ai.stanford.edu/~kuleshov/papers/gwaskb-manuscript.pdf">paper</a>, we showed how Snorkel can be used to automate Gene-Wide Association Study (GWAS) curation. On a collection of hundreds of previously published studies reporting significant genotype-phenotype pairs, we auto-labeled a large training set using only labeling functions. The resulting classifier applied to a collection of 598 studies recovered over 3,000 previously documented open-access relations (with an estimated recall of 60-80%) as well as over 2,000 associations not present in existing human curated repositories (with an estimated precision of 82-89%). The resulting database is available for exploration with a user interface at <a href="http://gwaskb.stanford.edu/">http://gwaskb.stanford.edu/</a>.</p>

<h2 id="stay-tuned"><strong>Stay Tuned</strong></h2>

<p>The Snorkel project is active and ongoing! We have a number of exciting, ongoing collaborations—from follow-on work at Stanford’s School of Medicine, to deployments at the <a href="https://www.icij.org/blog/2019/03/how-artificial-intelligence-can-help-us-crack-more-panama-papers-stories/">International Consortium of Investigative Journalists (ICIJ)</a> to help journalists organize, index, and understand millions of unstructured documents.</p>

<p>A code release later this month will include significant infrastructural improvements and tutorials for how to apply LFs, TFs, and SFs to SuperGLUE and other tasks. If you’ve used Snorkel for your own applications, we’d love to hear about it! For updates on Snorkel developments and applications, you can always visit the Snorkel <a href="http://snorkel.stanford.edu/">landing page</a> or <a href="https://github.com/HazyResearch/snorkel">open-source repository</a>.</p>

<h2 id="acknowledgements">Acknowledgements</h2>
<p>The authors would like to thank Feng Niu and Charles Srisuwananukorn for many helpful discussions, tests, and collaborations throughout the development of slicing!</p>

<!-- ##### Footnotes -->
<!-- * footnotes will be placed here. This line is necessary -->
<!-- {:footnotes} -->

<div class="footnotes">
  <ol>
    <li id="fn:superglue">
      <p>Wang, Alex, et al. <a href="https://arxiv.org/abs/1905.00537">“SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems.”</a>. 2019.  <em>SuperGLUE</em> consists of 6 datasets: the Commitment Bank (CB, <a href="https://github.com/mcdm/CommitmentBank/">De Marneffe et al., 2019</a>, Choice Of Plausible Alternatives (COPA, <a href="https://www.aaai.org/ocs/index.php/SSS/SSS11/paper/viewPaper/2418">Roemmele et al., 2011</a>), the Multi-Sentence Reading Comprehension dataset (MultiRC, <a href="https://www.aclweb.org/anthology/papers/N/N18/N18-1023/">Khashabi et al., 2018</a>), Recognizing Textual Entailment (merged from RTE1, <a href="https://link.springer.com/chapter/10.1007/11736790_9">Dagan et al. 2006</a>, RTE2, <a href="http://u.cs.biu.ac.il/~nlp/downloads/publications/RTE2-organizers.pdf">Bar Haim et al., 2006</a>, RTE3, <a href="https://dl.acm.org/citation.cfm?id=1654538">Giampiccolo et al., 2007</a>, and RTE5, <a href="http://www.cs.utexas.edu/users/pclark/papers/RTE6_overview.proceedings.pdf">Bentivogli et al., 2009</a>), Word in Context (WiC, <a href="https://www.aclweb.org/anthology/papers/N/N19/N19-1128">Pilehvar and Camacho-Collados, 2019</a>), and the Winograd Schema Challenge (WSC, <a href="https://www.aaai.org/ocs/index.php/KR/KR12/paper/viewPaper/4492">Levesque et al., 2012</a>). <a href="#fnref:superglue" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:dp">
      <p>Ratner, Alexander J., et al. <a href="http://papers.nips.cc/paper/6523-data-programming-creating-large-training-sets-quickly">“Data programming: Creating large training sets, quickly.”</a> 2016. <a href="#fnref:dp" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:tanda">
      <p>Ratner, Alexander J., et al. <a href="http://papers.nips.cc/paper/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation">“Learning to compose domain-specific transformations for data augmentation.”</a> 2017. <a href="#fnref:tanda" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:autoaugment">
      <p>Cubuk, Ekin D., et al. <a href="https://arxiv.org/abs/1805.09501">“Autoaugment: Learning augmentation policies from data.”</a> 2018. <a href="#fnref:autoaugment" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:tanda-2">
      <p>Ratner, Alexander J., et al. <a href="http://papers.nips.cc/paper/6916-learning-to-compose-domain-specific-transformations-for-data-augmentation">“Learning to compose domain-specific transformations for data augmentation.”</a> 2017. <a href="#fnref:tanda-2" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:autoaugment-2">
      <p>Cubuk, Ekin D., et al. <a href="https://arxiv.org/abs/1805.09501">“Autoaugment: Learning augmentation policies from data.”</a> 2018. <a href="#fnref:autoaugment-2" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:moe">
      <p>Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. <a href="[http://www.csri.utoronto.ca/~hinton/absps/jjnh91.ps](http://www.csri.utoronto.ca/~hinton/absps/jjnh91.ps)">“Adaptive mixtures of local experts.”</a> 1991. <a href="#fnref:moe" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:mtl">
      <p>Rich Caruana. <a href="https://link.springer.com/article/10.1023/A:1007379606734">“Multitask learning.”</a> 1997. <a href="#fnref:mtl" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>

  
  </section>
  <hr>
  Keep on top of the latest SAIL Blog posts via <a class="social-icon" href="http://0.0.0.0:4000/blog/feed.xml">RSS <i class="fa fa-rss-square fa fa" title="Twitter"></i></a>, <a class="social-icon" href="https://twitter.com/StanfordAILab">Twitter <i class="fa fa-twitter-square fa fa" title="Twitter"></i></a>, or email:

<!-- Begin Mailchimp Signup Form -->
<link href="//cdn-images.mailchimp.com/embedcode/horizontal-slim-10_7.css" rel="stylesheet" type="text/css">
<style type="text/css">
	#mc_embed_signup{background:#fff; clear:left; font:14px Helvetica,Arial,sans-serif; width:100%;}
	/* Add your own Mailchimp form style overrides in your site stylesheet or in this style block.
	   We recommend moving this block and the preceding CSS link to the HEAD of your HTML file. */
</style>
<div id="mc_embed_signup">
<form action="https://stanford.us19.list-manage.com/subscribe/post?u=3a6484754abf2fc18724ec835&amp;id=028823e92b" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <div id="mc_embed_signup_scroll">
	
	<input type="email" value="" name="EMAIL" class="email" id="mce-EMAIL" placeholder="email address" required>
    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_3a6484754abf2fc18724ec835_028823e92b" tabindex="-1" value=""></div>
    <div class="clear"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button"></div>
    </div>
</form>
</div>

<!--End mc_embed_signup-->



  <!-- Social media shares -->
  <div class="share-buttons">
    <ul class="share-buttons">
        <div class="meta">Share</div>
        
        <li>
            <a href="https://www.facebook.com/sharer/sharer.php?u=http://0.0.0.0:4000/blog/training-data-abstractions/" target="_blank" title="Share on Facebook">
			<i class="fa fa-facebook-square fa-2x" aria-hidden="true"></i>
			<span class="sr-only">Share on Facebook</span>
		</a>
        </li>
         
        <li>
            <a href="https://twitter.com/intent/tweet?source=http://0.0.0.0:4000/blog/training-data-abstractions/&text=Powerful+Abstractions+for+Programmatically+Building+and+Managing+Training+Sets%20%7C%20SAIL+Blog:%20http://0.0.0.0:4000/blog/training-data-abstractions/" target="_blank" title="Tweet">
			<i class="fa fa-twitter-square fa-2x" aria-hidden="true"></i>
			<span class="sr-only">Tweet</span>
		</a>
        </li>
            
        <li>
            <a href="https://getpocket.com/save?url=http://0.0.0.0:4000/blog/training-data-abstractions/&title=Powerful+Abstractions+for+Programmatically+Building+and+Managing+Training+Sets%20%7C%20SAIL+Blog" target="_blank" title="Add to Pocket">
			<i class="fa fa fa-get-pocket fa-2x" aria-hidden="true"></i>
			<span class="sr-only">Add to Pocket</span>
		</a>
        </li>
         
        <li>
            <a href="http://www.reddit.com/submit?url=http://0.0.0.0:4000/blog/training-data-abstractions/&title=Powerful+Abstractions+for+Programmatically+Building+and+Managing+Training+Sets%20%7C%20SAIL+Blog" target="_blank" title="Share on Reddit">
			<i class="fa fa-reddit-square fa-2x" aria-hidden="true"></i>
			<span class="sr-only">Share on Reddit</span>
		</a>
        </li>
           
        <li>
            <a href="mailto:?subject=Powerful+Abstractions+for+Programmatically+Building+and+Managing+Training+Sets%20%7C%20SAIL+Blog&body=:%20http://0.0.0.0:4000/blog/training-data-abstractions/" target="_blank" title="Email">
			<i class="fa fa-envelope-square fa-2x" aria-hidden="true"></i>
			<span class="sr-only">Email</span>
		</a>
        </li>
        
    </ul>
</div>

  <!-- Tag list -->
  
  


<footer>
  <div class="tag-list">
    
      <div class="meta">Tags</div>
    

    
    <a class="button" href="/blog/tags#ml">
      <p><i class="fa fa-tag fa-fw"></i> ml</p>
    </a>
    
    <a class="button" href="/blog/tags#research">
      <p><i class="fa fa-tag fa-fw"></i> research</p>
    </a>
    
    <a class="button" href="/blog/tags#systems">
      <p><i class="fa fa-tag fa-fw"></i> systems</p>
    </a>
    
  </div>
</footer>

</article>

<!-- Disqus -->


<!-- Post navigation -->

  <div id="post-nav">
  
  <div id="previous-post" class="post-nav-post">
      <a href="/blog/controllable-fairness/">
      <p>Previous post</p>
        Controllable Fairness in Machine&nbsp;Learning
      </a>
  </div>
  
  
  <div id="next-post" class="post-nav-post">
      <a href="/blog/dempref/">
      <p>Next post</p>
        Learning Reward Functions by Integrating Human Demonstrations and Preferences
      </a>
  </div>
  
</div>



    </div>
    <hr>
<footer class="site-footer">
    
   <div class="footer-icons">
        <ul>
        <!-- Social icons from Font Awesome, if enabled -->
        
<li>
	<a href="http://0.0.0.0:4000/blog/feed.xml" title="Follow RSS feed">
		<span class="fa-stack fa-lg">
            <i class="fa fa-circle fa-stack-2x"></i>
            <i class="fa fa-rss fa-stack-1x fa-inverse"></i>
        </span>
	</a>
</li>







































<li>
	<a href="https://twitter.com/StanfordAILab" title="Follow on Twitter" class="type">
        <span class="fa-stack fa-lg">
            <i class="fa fa-circle fa-stack-2x"></i>
            <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
        </span>
	</a>
</li>








        </ul>
    </div>
    <p class="text">&copy; 2019 Stanford AI Lab
</p>
 
</footer>



  </body>
</html>
