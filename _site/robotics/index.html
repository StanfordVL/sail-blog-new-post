<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <!-- Main JS (navbar.js, katex_init.js and masonry_init.js)-->
    <script defer=true src="/blog/assets/js/main.min.js"></script>
    
    <!-- CSS -->
    <link rel="stylesheet" href="/blog/assets/css/main.css">

    <!--Favicon-->
    <link rel="icon" type="image/png" href="/blog/assets/img/favicon-32x32.png" sizes="32x32" />
    <link rel="icon" type="image/png" href="/blog/assets/img/favicon-16x16.png" sizes="16x16" />

    <!-- Canonical -->
    <link rel="canonical" href="http://0.0.0.0:4000/blog/robotics/">

    <!-- RSS -->
    <link rel="alternate" type="application/atom+xml" title="The Stanford AI Lab Blog" href="http://0.0.0.0:4000/blog/feed.xml"/>

    <!-- Font Awesome -->
    <!-- <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet"> -->
    <link rel="stylesheet" type="text/css" href="/blog/assets/css/vendor/font-awesome.min.css">
    
    <!-- Bootstrap-3.3.7 isolation CSS -->
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" integrity="sha384-JcKb8q3iqJ61gNV9KGb8thSsNjpSL0n8PARn9HuZOnIxN0hoP+VmmDGMN5t9UJ0Z" crossorigin="anonymous">


    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js" integrity="sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha384-B4gt1jrGC7Jh4AgTPSdUtOBvfO8shuf57BaghqFfPlYxofvL8/KUEfYiJOMMV+rV" crossorigin="anonymous"></script>
    
    <!-- JQuery 2.2.4 -->
    <script src="https://code.jquery.com/jquery-2.2.4.min.js" integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin="anonymous"></script>
    
    <link rel="stylesheet" type="text/css" href="/blog/assets/css/vendor/bigfoot-number.css">
    <script type="text/javascript" src="/blog/assets/js/vendor/bigfoot.min.js"></script>
    <script type="text/javascript">
    $(document).ready(function() { 
      $.bigfoot();
      
    window.onload = function() {

        var videos = document.getElementsByTagName("video"),
            fraction = 0.8;

        function checkScroll() {

            for (var i = 0; i < videos.length; i++) {

                var video = videos[i];

                var x = video.offsetLeft,
                    y = video.offsetTop,
                    w = video.offsetWidth,
                    h = video.offsetHeight,
                    r = x + w, //right
                    b = y + h, //bottom
                    visibleX, visibleY, visible;

                visibleX = Math.max(0, Math.min(w, window.pageXOffset + window.innerWidth - x, r - window.pageXOffset));
                visibleY = Math.max(0, Math.min(h, window.pageYOffset + window.innerHeight - y, b - window.pageYOffset));

                visible = visibleX * visibleY / (w * h);

                if (visible > fraction) {
                    video.play();
                } else {
                    video.pause();
                }

            }

        }

        window.addEventListener('scroll', checkScroll, false);
        window.addEventListener('resize', checkScroll, false);
    }
    }); 
    </script>
    


    <!-- Google Fonts -->
    
    <link href="//fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,700italic,400italic" rel="stylesheet" type="text/css"> 
    

    <!-- KaTeX 0.8.3 -->
    
    <link rel="stylesheet" type="text/css" href="/blog/assets/css/vendor/katex.min.css">
    <script src="/blog/assets/js/vendor/katex.min.js">
    </script>
    
    
    
    <!-- Google Analytics -->
    
    <script>
        (function(i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r;
            i[r] = i[r] || function() {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date();
            a = s.createElement(o),
                m = s.getElementsByTagName(o)[0];
            a.async = 1;
            a.src = g;
            m.parentNode.insertBefore(a, m)
        })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'UA-129018108-1', 'auto');
        ga('send', 'pageview');

    </script>
    
    
    <!-- seo tags -->
    <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Robotics Posts | SAIL Blog</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Robotics Posts" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="The official Stanford AI Lab blog" />
<meta property="og:description" content="The official Stanford AI Lab blog" />
<link rel="canonical" href="http://0.0.0.0:4000/blog/robotics/" />
<meta property="og:url" content="http://0.0.0.0:4000/blog/robotics/" />
<meta property="og:site_name" content="SAIL Blog" />
<link rel="next" href="http://0.0.0.0:4000/blog/robotics/page/2/index.html" />
<script type="application/ld+json">
{"description":"The official Stanford AI Lab blog","@type":"WebPage","url":"http://0.0.0.0:4000/blog/robotics/","headline":"Robotics Posts","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    
    <!-- Manual seo tags -->
    <title>Robotics Posts | The Stanford AI Lab Blog</title>
    <meta name="description" content="The Stanford AI Lab (SAIL) Blog is a place for SAIL students, faculty, and researchers to share our work with the general public.">
    
    <!-- Twitter Cards -->
    <meta name="twitter:title" content="Robotics Posts">
    
    <meta name="twitter:description" content="The Stanford AI Lab (SAIL) Blog is a place for SAIL students, faculty, and researchers to share our work with the general public.">
    
    <meta name="twitter:creator" content="@StanfordAILab">
     
    <meta name="twitter:card" content="summary">
    <meta name="twitter:image" content="http://0.0.0.0:4000/blog/assets/img/sail-logo.png">
    <meta name="og:image" content="http://0.0.0.0:4000/blog/assets/img/sail-logo.png">
    
</head>

  <body class="category_posts-body">
      <!-- Toggle menu -->
<header class="site-header">

<nav class="clear navbar navbar-expand-lg navbar-light bg-white flex-column flex-md-row bd-navbar fixed-top" id="main_nav">
  
  <div class="container">

    <a class="navbar-brand mr-0 mr-md-2 text-black d-flex align-items-center" href="/blog/" aria-label="Bootstrap">
    
	  <div class="branding">
	    <a href="http://ai.stanford.edu/">
	      <img class="avatar" src="/blog/assets/img/sail-logo.png" alt=""/>
		  </a>

      <a href="/blog/">
	      <h1 class="site-title">
			    The Stanford AI Lab Blog
		    </h1> 
		  </a>
	  </div>
    
    
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
    </button>
    
    <div class="collapse navbar-collapse" id="navbarResponsive">

      <ul class="navbar-nav ml-auto">
      
      
      <li class="nav-item">
      <a class="nav-link" href="/blog/about">About</a>
      </li>
      
      <li class="nav-item dropdown">
        <a class="nav-link dropdown-toggle" data-toggle="dropdown" href="#" role="button" aria-haspopup="true" aria-expanded="false">Posts</a>
        <div class="dropdown-menu">
        <a class="dropdown-item" href="/blog/">All</a>
        <a class="dropdown-item" href="/blog/conferences">Conferences</a>
        <a class="dropdown-item" href="/blog/vision">Computer Vision</a>
        <a class="dropdown-item" href="/blog/robotics">Robotics</a>
        <a class="dropdown-item" href="/blog/nlp">NLP</a>
        <a class="dropdown-item" href="/blog/ml">Machine Learning</a>
        <a class="dropdown-item" href="/blog/rl">Reinforcement Learning</a>

        </div>
      </li>
        
      
      <li class="nav-item">
      <a class="nav-link" href="/blog/subscribe">Subscribe</a>
      </li>
      
      
      <li class="nav-item">
      <a class="nav-link" href="http://ai.stanford.edu/">SAIL</a>
      </li>
      
      </ul> 

    </div>

  </div>
</nav>

</header>

  
    <div class="content">
      <div class="home">
  <div class="posts">
  <br>
    <header id="main">
  <h1 id="Robotics+Posts" class="title">
      Robotics Posts
  </h1>
  
  
  <hr>
</header>

    
      
      <div class="post-teaser">
        
        <div class="post-img">

        
          <img src="https://ai.stanford.edu/blog//assets/img/posts/2020-11-14-lili/front.png">
        

        </div>
        
        <div class="info">
          <header>
            <a class="post-link" href="/blog/lili/">
              <h2> Learning to Influence Multi-Agent Interaction </h2>
            </a>

             <p class="meta">
              <a href="https://anxie.github.io/">Annie Xie</a>
              <!-- November 14, 2020 -->
            </p>
          </header>
          <div class="excerpt">
            <!--  -->
            <div class="excerpt-text">
                
                  We introduce a framework for multi-agent interaction that represents the low-level policies of non-stationary agents with high-level latent strategies.
                
            </div>
            <div class="excerpt-continue">
              <a class="button" href="/blog/lili/">
                Continue reading
              </a>
            </div>
          </div>
        </div>
      </div>
      
    
      
      <div class="post-teaser">
        
        <div class="post-img">

        
          <img src="https://ai.stanford.edu/blog/assets/img/posts/2020-10-07-gti/thumbnail.png">
        

        </div>
        
        <div class="info">
          <header>
            <a class="post-link" href="/blog/gti/">
              <h2> GTI: Learning to Generalize Across Long-Horizon Tasks from Human Demonstrations </h2>
            </a>

             <p class="meta">
              <a href="http://web.stanford.edu/~amandlek/">Ajay Mandlekar</a>, <a href="https://cs.stanford.edu/~danfei/">Danfei Xu</a>
              <!-- October 7, 2020 -->
            </p>
          </header>
          <div class="excerpt">
            <!--  -->
            <div class="excerpt-text">
                
                  We developed Generalization Through Imitation (GTI) - an algorithm for learning visuomotor control from human demonstrations and generalizing to new long-horizon tasks by leveraging latent compositional structures.
                
            </div>
            <div class="excerpt-continue">
              <a class="button" href="/blog/gti/">
                Continue reading
              </a>
            </div>
          </div>
        </div>
      </div>
      
    
      
      <div class="post-teaser">
        
        <div class="post-img">

        
          <img src="https://ai.stanford.edu/blog//assets/img/posts/2020-08-25-black-box-safety-validation/formulation.jpg">
        

        </div>
        
        <div class="info">
          <header>
            <a class="post-link" href="/blog/black-box-safety-validation/">
              <h2> Safety Validation of Black-Box Autonomous Systems </h2>
            </a>

             <p class="meta">
              <a href="http://anthonylcorso.com/">Anthony L. Corso</a>
              <!-- August 31, 2020 -->
            </p>
          </header>
          <div class="excerpt">
            <!--  -->
            <div class="excerpt-text">
                
                  Validation of safety-critical autonomous systems is necessary before deployment and is made possible by black-box validation strategies. In this blog post, we provide a model for black-box safety validation and survey the literature to uncover the most effective validation techniques.
                
            </div>
            <div class="excerpt-continue">
              <a class="button" href="/blog/black-box-safety-validation/">
                Continue reading
              </a>
            </div>
          </div>
        </div>
      </div>
      
    
      
      <div class="post-teaser">
        
        <div class="post-img">

        
          <img src="https://ai.stanford.edu/blog/assets/img/posts/2020-06-25-trajectory-forecasting/thumbnail.png">
        

        </div>
        
        <div class="info">
          <header>
            <a class="post-link" href="/blog/trajectory-forecasting/">
              <h2> Back to the Future: Planning-Aware Trajectory Forecasting for Autonomous Driving </h2>
            </a>

             <p class="meta">
              <a href="http://www.borisivanovic.com">Boris Ivanovic</a>
              <!-- June 25, 2020 -->
            </p>
          </header>
          <div class="excerpt">
            <!--  -->
            <div class="excerpt-text">
                
                  Recent advances in deep generative modeling have brought forth a paradigm shift in trajectory forecasting. In this blog post, we provide an overview of existing work, highlight new opportunities, and present our latest work on developing methods that are cognizant of their downstream use cases.
                
            </div>
            <div class="excerpt-continue">
              <a class="button" href="/blog/trajectory-forecasting/">
                Continue reading
              </a>
            </div>
          </div>
        </div>
      </div>
      
    
      
      <div class="post-teaser">
        
        <div class="post-img">

        
          <img src="https://ai.stanford.edu/blog//assets/img/posts/2020-05-18-selfsupervised-multimodal/intro.png">
        

        </div>
        
        <div class="info">
          <header>
            <a class="post-link" href="/blog/selfsupervised-multimodal/">
              <h2> Making Sense of Vision and Touch: Multimodal Representations for Contact-Rich Tasks  </h2>
            </a>

             <p class="meta">
              <a href="http://stanford.edu/~mishlee/">Michelle A. Lee</a>
              <!-- May 18, 2020 -->
            </p>
          </header>
          <div class="excerpt">
            <!--  -->
            <div class="excerpt-text">
                
                  Contact-rich manipulation tasks in unstructured environments often require both tactile and visual feedback. In this blog post, we introduce how to use self-supervision to learn a compact and multimodal representation of vision and touch.
                
            </div>
            <div class="excerpt-continue">
              <a class="button" href="/blog/selfsupervised-multimodal/">
                Continue reading
              </a>
            </div>
          </div>
        </div>
      </div>
      
    
      
      <div class="post-teaser">
        
        <div class="post-img">

        
          <img src="https://ai.stanford.edu/blog/assets/img/posts/2020-05-06-ntp-ntg/thumbnail.png">
        

        </div>
        
        <div class="info">
          <header>
            <a class="post-link" href="/blog/ntp-ntg/">
              <h2> Leveraging Compositionality for One-Shot Imitation Learning </h2>
            </a>

             <p class="meta">
              <a href="http://ai.stanford.edu/~dahuang/">De-An Huang</a>, <a href="https://cs.stanford.edu/~danfei/">Danfei Xu</a>
              <!-- May 6, 2020 -->
            </p>
          </header>
          <div class="excerpt">
            <!--  -->
            <div class="excerpt-text">
                
                  We introduce a class of algorithms that solve long-horizon one-shot imitation learning by leveraging compositional priors.
                
            </div>
            <div class="excerpt-continue">
              <a class="button" href="/blog/ntp-ntg/">
                Continue reading
              </a>
            </div>
          </div>
        </div>
      </div>
      
    
      
      <div class="post-teaser">
        
        <div class="post-img">

        
          <img src="https://ai.stanford.edu/blog//assets/img/posts/2020-04-08-cavin/pull_figure.png">
        

        </div>
        
        <div class="info">
          <header>
            <a class="post-link" href="/blog/cavin/">
              <h2> Sequential Problem Solving by Hierarchical Planning in Latent Spaces </h2>
            </a>

             <p class="meta">
              <a href='http://ai.stanford.edu/~kuanfang/'>Kuan Fang</a>
              <!-- April 10, 2020 -->
            </p>
          </header>
          <div class="excerpt">
            <!--  -->
            <div class="excerpt-text">
                
                  We propose a hierarchical planning algorithm in learned latent spaces. Our method uses deep generative models to prioritize promising actions for sampling-based planning.
                
            </div>
            <div class="excerpt-continue">
              <a class="button" href="/blog/cavin/">
                Continue reading
              </a>
            </div>
          </div>
        </div>
      </div>
      
    
      
      <div class="post-teaser">
        
        <div class="post-img">

        
          <img src="https://ai.stanford.edu/blog//assets/img/posts/2020-03-17-modeling-risky-humans/image1.png">
        

        </div>
        
        <div class="info">
          <header>
            <a class="post-link" href="/blog/modeling-risky-humans/">
              <h2> When Humans Aren’t Optimal: Robots that Collaborate with Risk-Aware Humans </h2>
            </a>

             <p class="meta">
              <a href='https://github.com/minaek'>Minae Kwon</a> and <a href='https://www.dylanlosey.com/'>Dylan Losey</a>
              <!-- March 17, 2020 -->
            </p>
          </header>
          <div class="excerpt">
            <!--  -->
            <div class="excerpt-text">
                
                  To create human-like robots, we need to understand how humans behave. We present a modeling approach enables robots to anticipate that humans will make suboptimal choices when risk and uncertainty are involved.
                
            </div>
            <div class="excerpt-continue">
              <a class="button" href="/blog/modeling-risky-humans/">
                Continue reading
              </a>
            </div>
          </div>
        </div>
      </div>
      
    
      
      <div class="post-teaser">
        
        <div class="post-img">

        
          <img src="https://ai.stanford.edu/blog/assets/img/posts/2019-11-26-robonet/bg-masthead.jpg">
        

        </div>
        
        <div class="info">
          <header>
            <a class="post-link" href="/blog/robonet/">
              <h2> RoboNet: A Dataset for Large-Scale Multi-Robot Learning </h2>
            </a>

             <p class="meta">
              <a href="https://sudeepdasari.github.io/"> Sudeep Dasari </a>
              <!-- November 26, 2019 -->
            </p>
          </header>
          <div class="excerpt">
            <!--  -->
            <div class="excerpt-text">
                
                  We introduce RoboNet, an open database for sharing robotic experience, and study how this data can be used to learn generalizable models for vision-based robotic manipulation. We find that pre-training on RoboNet enables faster learning in new environments compared to learning from scratch.
                
            </div>
            <div class="excerpt-continue">
              <a class="button" href="/blog/robonet/">
                Continue reading
              </a>
            </div>
          </div>
        </div>
      </div>
      
    
      
      <div class="post-teaser">
        
        <div class="post-img">

        
          <img src="https://ai.stanford.edu/blog//assets/img/posts/2019-11-10-assistive-latent-spaces/image0.png">
        

        </div>
        
        <div class="info">
          <header>
            <a class="post-link" href="/blog/assistive-latent-spaces/">
              <h2> Controlling Assistive Robots with Learned Latent Actions </h2>
            </a>

             <p class="meta">
              <a href='https://www.dylanlosey.com/'>Dylan Losey</a>
              <!-- November 11, 2019 -->
            </p>
          </header>
          <div class="excerpt">
            <!--  -->
            <div class="excerpt-text">
                
                  We want to make it easier for humans to teleoperate dexterous robots. We present a learning approach that embeds high-dimensional robot actions into an intuitive, human-controllable, and low-dimensional latent space.
                
            </div>
            <div class="excerpt-continue">
              <a class="button" href="/blog/assistive-latent-spaces/">
                Continue reading
              </a>
            </div>
          </div>
        </div>
      </div>
      
    
  </div>

  
  <div class="paginate pager">

      

      
          <a href="/blog/robotics/page/2/index.html" class="button" >
          Next
          <i class="fa fa-chevron-right"></i>
          </a>
      

  </div>
  

</div>


    </div>
    <hr>
<footer class="site-footer">
    
   <div class="footer-icons">
        <ul>
        <!-- Social icons from Font Awesome, if enabled -->
        
<li>
	<a href="http://0.0.0.0:4000/blog/feed.xml" title="Follow RSS feed">
		<span class="fa-stack fa-lg">
            <i class="fa fa-circle fa-stack-2x"></i>
            <i class="fa fa-rss fa-stack-1x fa-inverse"></i>
        </span>
	</a>
</li>







































<li>
	<a href="https://twitter.com/StanfordAILab" title="Follow on Twitter" class="type">
        <span class="fa-stack fa-lg">
            <i class="fa fa-circle fa-stack-2x"></i>
            <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
        </span>
	</a>
</li>








        </ul>
    </div>
    <p class="text">&copy; 2019 Stanford AI Lab
</p>
 
</footer>



  </body>
</html>
