<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>The Stanford AI Lab Blog</title>
        <atom:link href="/blog/feed.xml" rel="self" type="application/rss+xml"/>
        <link>http://0.0.0.0:4000/blog/</link>
        <description>The Stanford AI Lab (SAIL) Blog is a place for SAIL students, faculty, and researchers to share our work with the general public.</description>
        <pubDate>Mon, 07 Dec 2020 11:10:49 -0800</pubDate>
        
          
          <item>
              <title>Learning from Language Explanations</title>
              <link>/blog/learning-from-language/</link>
              <guid isPermaLink="true">/blog/learning-from-language/</guid>
              <description>&lt;p&gt;Imagine you’re a machine learning practitioner and you want to solve some classification problem, like classifying groups of colored squares as being either 1s or 0s. Here’s what you would typically do: collect a large dataset of examples, label the data, and train a classifier:&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage_unpadded&quot; style=&quot;max-width: 700px&quot; src=&quot;/blog/assets/img/posts/2020-11-23-learning-from-language/examples.jpg&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&lt;em&gt;But humans don’t learn like this&lt;/em&gt;. We have a very powerful and intuitive mechanism for communicating information about the world - &lt;strong&gt;language&lt;/strong&gt;!&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage_unpadded&quot; style=&quot;max-width: 500px&quot; src=&quot;/blog/assets/img/posts/2020-11-23-learning-from-language/language.jpg&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;With just the phrase &lt;em&gt;at least 2 red squares&lt;/em&gt;, we’ve summarized the entire dataset presented above in a much more efficient manner.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Language is a crucial medium for human learning:&lt;/strong&gt; we use it to &lt;a href=&quot;https://www.npr.org/2010/01/18/122701268/i-have-a-dream-speech-in-its-entirety&quot;&gt;convey beliefs&lt;/a&gt; about the world, &lt;a href=&quot;https://www.nature.com/articles/ncomms7029&quot;&gt;teach others&lt;/a&gt;, and describe things that are hard to &lt;a href=&quot;https://en.wikipedia.org/wiki/Saturn&quot;&gt;experience directly&lt;/a&gt;. Thus, language ought to be a simple and effective way to supervise machine learning models. Yet past approaches to learning from language have struggled to scale up to the general tasks targeted by modern deep learning systems and the freeform language explanations used in these domains. In two short papers presented at ACL 2020 this year, we use deep neural models to learn from language explanations to help tackle a variety of challenging tasks in natural language processing (NLP) and computer vision.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2005.01932&quot;&gt;ExpBERT: Representation Engineering with Natural Language Explanations&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1911.02683&quot;&gt;Shaping Visual Representations with Language for Few-shot Classification&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;whats-the-challenge&quot;&gt;&lt;strong&gt;What’s the challenge?&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Given that language is such an intuitive interface for humans to teach others,
why is it so hard to use language for machine learning?&lt;/p&gt;

&lt;p&gt;The principal challenge is the &lt;a href=&quot;https://arxiv.org/html/cs/9906002&quot;&gt;grounding
problem&lt;/a&gt;: understanding language
explanations in the context of other inputs. Building models that can
understand rich and ambiguous language is tricky enough, but building models
that can relate language to the surrounding world is even more challenging. For
instance, given the explanation &lt;em&gt;at least two red squares&lt;/em&gt;, a model must not
only understand the terms &lt;em&gt;red&lt;/em&gt; and &lt;em&gt;square&lt;/em&gt;, but also how they refer to
particular parts of (often complex) inputs.&lt;/p&gt;

&lt;p&gt;Past work (&lt;a href=&quot;https://www.aclweb.org/anthology/D17-1161&quot;&gt;1&lt;/a&gt;,
&lt;a href=&quot;https://www.aclweb.org/anthology/P18-1029.pdf&quot;&gt;2&lt;/a&gt;,
&lt;a href=&quot;https://arxiv.org/abs/1805.03818&quot;&gt;3&lt;/a&gt;) has relied on &lt;a href=&quot;https://cs.stanford.edu/~pliang/papers/executable-cacm2016.pdf&quot;&gt;semantic
parsers&lt;/a&gt; which
convert natural language statements (e.g. &lt;em&gt;at least two red squares&lt;/em&gt;) to formal
logical representations (e.g. &lt;code class=&quot;highlighter-rouge&quot;&gt;Count(Square AND Red) &amp;gt; 2&lt;/code&gt;). If we can easily
check whether explanations apply to our inputs by executing these logical
formulas, we can use our explanations as features to train our model.
However, semantic parsers only work on simple domains
where we can hand-engineer a logical grammar of explanations we might expect to
see. They struggle to handle richer and vaguer language or scale up to more
complex inputs, such as images.&lt;/p&gt;

&lt;p&gt;Fortunately, modern deep neural language models such as
&lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot;&gt;BERT&lt;/a&gt; are beginning to show promise at
solving many language understanding tasks. Our papers propose to alleviate the
grounding problem by using neural language models that are either trained to
ground language explanations in the domain of interest, or come pre-trained
with general-purpose “knowledge” that can be used to interpret explanations. We
will show that these neural models allow us to learn from richer and more
diverse language for more challenging settings.&lt;/p&gt;

&lt;h3 id=&quot;representation-engineering-with-natural-language-explanations&quot;&gt;&lt;strong&gt;Representation Engineering with Natural Language Explanations&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;In our &lt;a href=&quot;https://arxiv.org/abs/2005.01932&quot;&gt;first paper&lt;/a&gt;, we examine how to build text classifiers with language
explanations.
Consider the task of &lt;em&gt;relation extraction&lt;/em&gt;, where we are given a
short paragraph and must identify whether two people mentioned in the
paragraph are &lt;strong&gt;married&lt;/strong&gt;. While state-of-the-art NLP models can likely solve
this task from data alone, humans might use language to describe ways to tell
whether two people are married—for example, &lt;em&gt;people who go on honeymoons are
typically married&lt;/em&gt;. Can such language explanations be used to train better
classifiers?&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage_unpadded&quot; style=&quot;max-width: 700px&quot; src=&quot;/blog/assets/img/posts/2020-11-23-learning-from-language/expbert_dataset.jpg&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;In the same way that we might take an input &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;, and extract features (e.g.
the presence of certain words) to train a model, we can use explanations to
provide additional features. For example, knowing that honeymoons are relevant
for this task, if we can create a honeymoon feature that reliably activates
whenever the two people in a paragraph are described as going on a honeymoon,
this should be useful signal for training a better model.&lt;/p&gt;

&lt;p&gt;But creating such features requires some sort of explanation &lt;strong&gt;interpretation&lt;/strong&gt;
mechanism that tells us whether an explanation is true for an input. Semantic
parsers are one such tool: given &lt;em&gt;&lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;B&lt;/script&gt; went on honeymoon&lt;/em&gt;, we could
parse this explanation into a logical form which, when run on an input,
produces 1 if the word &lt;em&gt;honeymoon&lt;/em&gt; appears between &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;B&lt;/script&gt;. But what about
a vaguer explanation like &lt;em&gt;&lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;B&lt;/script&gt; are in love&lt;/em&gt;? How can we parse this?&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage_unpadded&quot; style=&quot;max-width: 800px&quot; src=&quot;/blog/assets/img/posts/2020-11-23-learning-from-language/semantic_parsing_examples.jpg&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;While semantic parsing is efficient and accurate in small domains, it can be
overly &lt;em&gt;brittle&lt;/em&gt;, as it can only interpret explanations which adhere to a fixed
set of grammatical rules and functions that we must specify in advance (e.g.
&lt;code class=&quot;highlighter-rouge&quot;&gt;contains&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;extract_text&lt;/code&gt;).
Instead, we turn to the soft reasoning
capabilities of &lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot;&gt;BERT&lt;/a&gt;, a neural language model. BERT is particularly effective
at the task of &lt;em&gt;textual entailment&lt;/em&gt;: determining whether a sentence implies or
contradicts another sentence (e.g. does &lt;em&gt;She ate pizza&lt;/em&gt; imply that &lt;em&gt;She ate
food?&lt;/em&gt; Yes!). In our proposed &lt;strong&gt;ExpBERT&lt;/strong&gt; model, we take a BERT model
trained for textual entailment, and instead ask it to identify whether a
paragraph in our task &lt;em&gt;entails&lt;/em&gt; an explanation. The features produced by BERT
during this process replace the indicator features produced by the semantic
parser above.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;video class=&quot;postimage_unpadded&quot; style=&quot;max-width: 800px&quot; autoplay=&quot;&quot; muted=&quot;&quot; loop=&quot;&quot; playsinline=&quot;&quot;&gt;
    &lt;source src=&quot;/blog/assets/img/posts/2020-11-23-learning-from-language/expbert.webm&quot; type=&quot;video/webm&quot; /&gt;
    &lt;source src=&quot;/blog/assets/img/posts/2020-11-23-learning-from-language/expbert.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    &lt;p&gt;Your browser doesn't support HTML5 video. Here is a &lt;a href=&quot;/blog/assets/img/posts/2020-11-23-learning-from-language/expbert.mp4&quot;&gt;link to the video&lt;/a&gt; instead, which you can download and run with a player like &lt;a href=&quot;https://www.videolan.org/vlc/index.html&quot;&gt;VLC&lt;/a&gt;&lt;/p&gt;
&lt;/video&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Does the soft reasoning power of BERT improve over semantic parsing? On the
marriage identification task, we find that &lt;strong&gt;ExpBERT&lt;/strong&gt; leads to substantial
improvements over a classifier that is trained on the input features only (No
Explanations). Importantly, using a semantic parser to try to parse
explanations doesn’t help much, since there are general explanations (&lt;em&gt;in
love&lt;/em&gt;) that are difficult to convert to logical forms.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage_unpadded&quot; style=&quot;max-width: 285px&quot; src=&quot;/blog/assets/img/posts/2020-11-23-learning-from-language/expbert_results.jpg&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;In the full paper, we compare to more baselines, explore larger relation
extraction tasks (e.g. &lt;a href=&quot;https://nlp.stanford.edu/projects/tacred/&quot;&gt;TACRED&lt;/a&gt;),
conduct ablation studies to understand what kinds of explanations are
important, and examine how much more efficient explanations are compared to
additional data.&lt;/p&gt;

&lt;h3 id=&quot;shaping-visual-representations-with-language&quot;&gt;&lt;strong&gt;Shaping Visual Representations with Language&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;The work we’ve just described uses natural language explanations for a single
task like marriage identification.  However, &lt;a href=&quot;https://plato.stanford.edu/entries/language-thought/&quot;&gt;work in cognitive
science&lt;/a&gt; suggests that
language also equips us with the right features and abstractions that help us
solve &lt;em&gt;future&lt;/em&gt; tasks.
For example, explanations that indicate whether person &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; is married to
&lt;script type=&quot;math/tex&quot;&gt;B&lt;/script&gt; also highlight other concepts that are crucial to human relationships:
&lt;em&gt;children&lt;/em&gt;, &lt;em&gt;daughters&lt;/em&gt;, &lt;em&gt;honeymoons&lt;/em&gt;, and more. Knowing these additional
concepts are not just useful for identifying married people; they are also
important if we would later like to identify other relationships
(e.g. &lt;em&gt;siblings&lt;/em&gt;, &lt;em&gt;mother&lt;/em&gt;, &lt;em&gt;father&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;In machine learning, we might ask: how can language point out the right
features for challenging and underspecified domains, if we
ultimately wish to solve &lt;em&gt;new tasks&lt;/em&gt; where no language is available? In our
&lt;a href=&quot;https://arxiv.org/abs/1911.02683&quot;&gt;second paper&lt;/a&gt;, we explore this setting,
additionally increasing the challenge by seeing whether language can improve
the learning of representations across modalities—here, vision.&lt;/p&gt;

&lt;p&gt;We’re specifically interested in few-shot visual reasoning tasks like the following (here, from the &lt;a href=&quot;https://arxiv.org/abs/1704.04517&quot;&gt;ShapeWorld&lt;/a&gt; dataset):&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;video class=&quot;postimage_unpadded&quot; style=&quot;max-width: 500px&quot; autoplay=&quot;&quot; muted=&quot;&quot; loop=&quot;&quot; playsinline=&quot;&quot;&gt;
    &lt;source src=&quot;/blog/assets/img/posts/2020-11-23-learning-from-language/shapeworld.webm&quot; type=&quot;video/webm&quot; /&gt;
    &lt;source src=&quot;/blog/assets/img/posts/2020-11-23-learning-from-language/shapeworld.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    &lt;p&gt;Your browser doesn't support HTML5 video. Here is a &lt;a href=&quot;/blog/assets/img/posts/2020-11-23-learning-from-language/shapeworld.mp4&quot;&gt;link to the video&lt;/a&gt; instead, which you can download and run with a player like &lt;a href=&quot;https://www.videolan.org/vlc/index.html&quot;&gt;VLC&lt;/a&gt;&lt;/p&gt;
&lt;/video&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Given a small training set of examples of a visual concept, the task is to
determine whether a held-out test image expresses the same concept.  Now, what
if we assume access to language explanations of the relevant visual concepts at
training time? Can we use these to learn a better model, &lt;em&gt;even if no language
is available at test time&lt;/em&gt;?&lt;/p&gt;

&lt;p&gt;We frame this as a &lt;a href=&quot;https://arxiv.org/abs/1904.04232&quot;&gt;&lt;em&gt;meta-learning&lt;/em&gt;&lt;/a&gt; task:
instead of training and testing a model on a single task, we
train a model on a &lt;em&gt;set&lt;/em&gt; of tasks, each with a small training set and
an accompanying language description (the &lt;em&gt;meta-train&lt;/em&gt; set). We then test
generalization to a &lt;em&gt;meta-test&lt;/em&gt; set of unseen tasks, for which no language is
available:&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage_unpadded&quot; style=&quot;max-width: 760px&quot; src=&quot;/blog/assets/img/posts/2020-11-23-learning-from-language/metalearning.jpg&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;First, let’s look at how we might solve this task without language. One typical
approach is &lt;strong&gt;Prototype Networks&lt;/strong&gt;, where we learn some model &lt;script type=&quot;math/tex&quot;&gt;f_\theta&lt;/script&gt;
(here, a &lt;a href=&quot;https://arxiv.org/abs/1409.1556&quot;&gt;deep convolutional neural network&lt;/a&gt;)
that embeds the training images, averages them, and compares to an embedding of
the test image:&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;video class=&quot;postimage_unpadded&quot; style=&quot;max-width: 800px&quot; autoplay=&quot;&quot; muted=&quot;&quot; loop=&quot;&quot; playsinline=&quot;&quot;&gt;
    &lt;source src=&quot;/blog/assets/img/posts/2020-11-23-learning-from-language/lsl.webm&quot; type=&quot;video/webm&quot; /&gt;
    &lt;source src=&quot;/blog/assets/img/posts/2020-11-23-learning-from-language/lsl.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    &lt;p&gt;Your browser doesn't support HTML5 video. Here is a &lt;a href=&quot;/blog/assets/img/posts/2020-11-23-learning-from-language/lsl.mp4&quot;&gt;link to the video&lt;/a&gt; instead, which you can download and run with a player like &lt;a href=&quot;https://www.videolan.org/vlc/index.html&quot;&gt;VLC&lt;/a&gt;&lt;/p&gt;
&lt;/video&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;To use language, we propose a simple approach called &lt;strong&gt;Language Shaped Learning&lt;/strong&gt;
(LSL): if we have access to explanations at training time, we encourage the
model to learn representations that are not only helpful for classification,
but are &lt;em&gt;predictive of the language explanations&lt;/em&gt;. We do this by introducing an
&lt;em&gt;auxiliary&lt;/em&gt; training objective (i.e. it is not related to the ultimate task of
interest), where we simultaneously train a recurrent neural network (RNN)
decoder to predict the explanation(s) from the representation of the
input images. Crucially, training this decoder depends on the
parameters of our image model &lt;script type=&quot;math/tex&quot;&gt;f_\theta&lt;/script&gt;, so this process should encourage
&lt;script type=&quot;math/tex&quot;&gt;f_\theta&lt;/script&gt; to better encode the features and abstractions exposed in
language.&lt;/p&gt;

&lt;p&gt;In effect, we are training the model to “think out loud” when representing
concepts at training time. At test time, we simply discard the RNN decoder, and
do classification as normal with the “language-shaped” image embeddings.&lt;/p&gt;

&lt;p&gt;We apply this model to both the ShapeWorld dataset described above, and a more
realistic &lt;a href=&quot;http://www.vision.caltech.edu/visipedia/CUB-200-2011.html&quot;&gt;Birds&lt;/a&gt;
dataset, with real images and human language:&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage_unpadded&quot; style=&quot;max-width: 800px&quot; src=&quot;/blog/assets/img/posts/2020-11-23-learning-from-language/birds.jpg&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;In both cases, this auxiliary training objective improves performance over a
no-explanation baseline (&lt;strong&gt;Meta&lt;/strong&gt;), and &lt;a href=&quot;https://arxiv.org/abs/1711.00482&quot;&gt;&lt;em&gt;Learning with Latent
Language&lt;/em&gt;&lt;/a&gt; (&lt;strong&gt;L3&lt;/strong&gt;), a similar model proposed
for this setting that uses language as a discrete bottleneck (see the paper for
details):&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage_unpadded&quot; style=&quot;max-width: 400px&quot; src=&quot;/blog/assets/img/posts/2020-11-23-learning-from-language/lsl_results.jpg&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;In the full paper, we also explore which &lt;em&gt;parts&lt;/em&gt; of language are most important
(spoiler: a little bit of everything), and &lt;em&gt;how much&lt;/em&gt; language is needed for
LSL to improve over models that don’t use language (spoiler: surprisingly little!)&lt;/p&gt;

&lt;h3 id=&quot;moving-forward&quot;&gt;&lt;strong&gt;Moving Forward&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;As NLP systems grow in their ability to understand and produce language, so too
grows the potential for machine learning systems to &lt;em&gt;learn from language&lt;/em&gt; to
solve other challenging tasks. In the papers above, we’ve shown that deep
neural language models can be used to successfully learn from language
explanations to improve generalization across a variety of tasks in vision and
NLP.&lt;/p&gt;

&lt;p&gt;We think this is an exciting new avenue for training machine learning models,
and similar ideas are already being explored in areas such as reinforcement
learning (&lt;a href=&quot;https://arxiv.org/abs/1910.08210&quot;&gt;4&lt;/a&gt;,
&lt;a href=&quot;https://arxiv.org/abs/1906.03926&quot;&gt;5&lt;/a&gt;). We envision a future where in order to
solve a machine learning task, we no longer have to collect a large labeled
dataset, but instead interact naturally and expressively with a model in the
same way that humans have interacted with each other for millennia—&lt;em&gt;through
language&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&quot;acknowledgments&quot;&gt;&lt;strong&gt;Acknowledgments&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Thanks to our coauthors (Pang Wei Koh, Percy Liang, and Noah Goodman), and to
Nelson Liu, Pang Wei Koh, and the rest of the SAIL blog team for reviewing and
publishing this blog post. This research was supported in part by the &lt;a href=&quot;https://research.fb.com/fellowship/&quot;&gt;Facebook
Fellowship&lt;/a&gt; (to Pang Wei Koh), the &lt;a href=&quot;https://www.nsfgrfp.org/&quot;&gt;NSF Graduate Research Fellowship&lt;/a&gt; (to Jesse Mu), &lt;a href=&quot;https://www.tri.global/&quot;&gt;Toyota Research
Institute&lt;/a&gt;, and the &lt;a href=&quot;https://www.onr.navy.mil/&quot;&gt;Office of Naval Research&lt;/a&gt;.&lt;/p&gt;
</description>
              <pubDate>Mon, 23 Nov 2020 00:00:00 -0800</pubDate>
          </item>
          
        
          
          <item>
              <title>Stanford AI Lab Papers and Talks at CoRL 2020</title>
              <link>/blog/corl-2020/</link>
              <guid isPermaLink="true">/blog/corl-2020/</guid>
              <description>&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimagethird&quot; src=&quot;/blog/assets/img/posts/2020-11-16-corl-2020/logo.png&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;The &lt;a href=&quot;https://www.robot-learning.org/&quot;&gt;Conference on Robot Learning&lt;/a&gt; (CoRL) 2020 is being hosted virtually from November 16th - November 18th. We’re excited to share all the work from SAIL that’s being presented, and you’ll find links to papers, videos and blogs below. Feel free to reach out to the contact authors directly to learn more about the work that’s happening at Stanford!&lt;/p&gt;

&lt;h2 id=&quot;list-of-accepted-papers&quot;&gt;List of Accepted Papers&lt;/h2&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;learning-3d-dynamic-scene-representations-for-robot-manipulation&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2011.01968.pdf&quot;&gt;Learning 3D Dynamic Scene Representations for Robot Manipulation&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-11-16-corl-2020/img0&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Zhenjia Xu, Zhanpeng He, Jiajun Wu, Shuran Song
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: jiajunwu@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/pdf/2011.01968.pdf&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://www.youtube.com/watch?v=GQjYG3nQJ80&quot;&gt;Video&lt;/a&gt; | &lt;a href=&quot;https://dsr-net.cs.columbia.edu/&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: scene representations, 3d perception, robot manipulation&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;learning-latent-representations-to-influence-multi-agent-interaction&quot;&gt;&lt;a href=&quot;https://drive.google.com/file/d/1_ezqLLEv4HLtj9vflRj0sq3PNOhaSnJm/view&quot;&gt;Learning Latent Representations to Influence Multi-Agent Interaction&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-11-16-corl-2020/img6&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Annie Xie, Dylan P. Losey, Ryan Tolsma, Chelsea Finn, Dorsa Sadigh
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: anniexie@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://drive.google.com/file/d/1_ezqLLEv4HLtj9vflRj0sq3PNOhaSnJm/view&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://ai.stanford.edu/blog/lili/&quot;&gt;Blog Post&lt;/a&gt; | &lt;a href=&quot;https://sites.google.com/view/latent-strategies&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: multi-agent systems, human-robot interaction, reinforcement learning&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;learning-object-conditioned-exploration-using-distributed-soft-actor-critic&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2007.14545.pdf&quot;&gt;Learning Object-conditioned Exploration using Distributed Soft Actor Critic&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-11-16-corl-2020/img1&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Ayzaan Wahid (Google), Austin Stone (Google), Brian Ichter (Google Brain), Kevin Chen (Stanford), Alexander Toshev (Google)
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: ayzaan@google.com
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/pdf/2007.14545.pdf&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: object navigation, visual navigation&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;mats-an-interpretable-trajectory-forecasting-representation-for-planning-and-control-&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2009.07517&quot;&gt;MATS: An Interpretable Trajectory Forecasting Representation for Planning and Control &lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-11-16-corl-2020/img2&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Boris Ivanovic, Amine Elhafsi, Guy Rosman, Adrien Gaidon, Marco Pavone
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: borisi@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2009.07517&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://www.youtube.com/watch?v=q6hMY2y-BcQ&quot;&gt;Video&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: trajectory forecasting, learning dynamical systems, motion planning, autonomous vehicles&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;model-based-reinforcement-learning-for-decentralized-multiagent-rendezvous&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2003.06906&quot;&gt;Model-based Reinforcement Learning for Decentralized Multiagent Rendezvous&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-11-16-corl-2020/img3&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Rose E. Wang, J. Chase Kew, Dennis Lee, Tsang-Wei Edward Lee, Tingnan Zhang, Brian Ichter, Jie Tan, Aleksandra Faust
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: rewang@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2003.06906&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://youtu.be/HqeYcO1DBUU&quot;&gt;Video&lt;/a&gt; | &lt;a href=&quot;https://sites.google.com/view/multiagent-hpp/home&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: multiagent systems; model-based reinforcement learning&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;reinforcement-learning-with-videos--combining-offline-observations-with-interaction&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2011.06507&quot;&gt;Reinforcement Learning with Videos:  Combining Offline Observations with Interaction&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-11-16-corl-2020/img4&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Karl Schmeckpeper, Oleh Rybkin, Kostas Daniilidis, Sergey Levine, Chelsea Finn
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: karls@seas.upenn.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2011.06507&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://sites.google.com/view/rl-with-videos&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: reinforcement learning, learning from observation&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;sampling-based-reachability-analysis-a-random-set-theory-approach-with-adversarial-sampling&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2008.10180&quot;&gt;Sampling-based Reachability Analysis: A Random Set Theory Approach with Adversarial Sampling&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-11-16-corl-2020/img5&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Thomas Lew, Marco Pavone
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: thomas.lew@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2008.10180&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: reachability analysis, robust planning and control, neural networks&lt;/p&gt;

&lt;h2 id=&quot;keynote&quot;&gt;Keynote&lt;/h2&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;walking-the-boundary-of-learning-and-interaction-dorsa-sadigh&quot;&gt;Walking the Boundary of Learning and Interaction (Dorsa Sadigh)&lt;/h4&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimagethird&quot; src=&quot;/blog/assets/img/posts/2020-11-16-corl-2020/keynote.png&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Overview:&lt;/strong&gt; There have been significant advances in the field of robot learning in the past decade. However, many challenges still remain when considering how robot learning can advance interactive agents such as robots that collaborate with humans. This includes autonomous vehicles that interact with human-driven vehicles or pedestrians, service robots collaborating with their users at homes over short or long periods of time, or assistive robots helping patients with disabilities. This introduces an opportunity for developing new robot learning algorithms that can help advance interactive autonomy.&lt;/p&gt;

&lt;p&gt;In this talk, I will discuss a formalism for human-robot interaction built upon ideas from representation learning. Specifically, I will first discuss the notion of latent strategies— low dimensional representations sufficient for capturing non-stationary interactions. I will then talk about the challenges of learning such representations when interacting with humans, and how we can develop data-efficient techniques that enable actively learning computational models of human behavior from demonstrations, preferences, or physical corrections. Finally, I will introduce an intuitive controlling paradigm that enables seamless collaboration based on learned representations, and further discuss how that can be used for further influencing humans.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Live Event:&lt;/strong&gt; November 17th, 7:00AM - 7:45AM PST&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;We look forward to seeing you at CoRL!&lt;/p&gt;
</description>
              <pubDate>Mon, 16 Nov 2020 00:00:00 -0800</pubDate>
          </item>
          
        
          
          <item>
              <title>Stanford AI Lab Papers and Talks at EMNLP 2020</title>
              <link>/blog/emnlp-2020/</link>
              <guid isPermaLink="true">/blog/emnlp-2020/</guid>
              <description>&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-11-15-emnlp-2020/logo.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://2020.emnlp.org/&quot;&gt;Conference on Empirical Methods in Natural Language Processing&lt;/a&gt; (EMNLP) 2020 is being hosted virtually from November 16th - November 20th. We’re excited to share all the work from SAIL that’s being presented, and you’ll find links to papers, videos and blogs below. Feel free to reach out to the contact authors directly to learn more about the work that’s happening at Stanford!&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#main-conference&quot;&gt;Main Conference&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#findings-of-emnlp&quot;&gt;Findings of EMNLP&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#workshops-and-co-located-conferences&quot;&gt;Workshops and Co-Located Conferences&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;main-conference&quot;&gt;Main Conference&lt;/h2&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;pre-training-transformers-as-energy-based-cloze-models&quot;&gt;&lt;a href=&quot;https://www.aclweb.org/anthology/2020.emnlp-main.20.pdf&quot;&gt;Pre-Training Transformers as Energy-Based Cloze Models&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-11-15-emnlp-2020/img19&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: kevclark@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://www.aclweb.org/anthology/2020.emnlp-main.20.pdf&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: representation learning, self-supervised learning, energy-based models&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;alice-active-learning-with-contrastive-natural-language-explanations&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2009.10259&quot;&gt;ALICE: Active Learning with Contrastive Natural Language Explanations&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-11-15-emnlp-2020/img8&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Weixin Liang, James Zou, Zhou Yu
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: wxliang@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2009.10259&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: natural language explanation, class-based active learning, contrastive explanation&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;chexbert-combining-automatic-labelers-and-expert-annotations-for-accurate-radiology-report-labeling-using-bert&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2004.09167&quot;&gt;CheXbert: Combining Automatic Labelers and Expert Annotations for Accurate Radiology Report Labeling Using BERT&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-11-15-emnlp-2020/img1&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Akshay Smit, Saahil Jain, Pranav Rajpurkar, Anuj Pareek, Andrew Y. Ng, Matthew P. Lungren
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: akshaysm@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2004.09167&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://virtual.2020.emnlp.org/paper_main.55.html&quot;&gt;Virtual Conference Room&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: bert, natural language processing, radiology, medical imaging, deep learning&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;autoqa-from-databases-to-qa-semantic-parsers-with-only-synthetic-training-data&quot;&gt;&lt;a href=&quot;https://www.aclweb.org/anthology/2020.emnlp-main.31/&quot;&gt;AutoQA: From Databases To QA Semantic Parsers With Only Synthetic Training Data&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-11-15-emnlp-2020/img21&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Silei Xu, Sina J. Semnani, Giovanni Campagna, Monica S. Lam
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: silei@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://www.aclweb.org/anthology/2020.emnlp-main.31/&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://virtual.2020.emnlp.org/paper_main.3506.html&quot;&gt;Virtual Conference Room&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: question answering, semantic parsing, language models, synthetic training data, data augmentation&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;data-and-representation-for-turkish-natural-language-inference&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2004.14963.pdf&quot;&gt;Data and Representation for Turkish Natural Language Inference&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-11-15-emnlp-2020/img14&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Emrah Budur, Rıza Özçelik, Tunga Güngör, Christopher Potts
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: emrah.budur@boun.edu.tr
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/pdf/2004.14963.pdf&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://github.com/boun-tabi/NLI-TR&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: sentence-level semantics, natural language inference, neural machine translation, morphologically rich language&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;intrinsic-evaluation-of-summarization-datasets&quot;&gt;&lt;a href=&quot;https://github.com/rishibommasani/rishibommasani.github.io/blob/master/papers/EMNLP2020.pdf&quot;&gt;Intrinsic Evaluation of Summarization Datasets&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-11-15-emnlp-2020/img6&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Rishi Bommasani, Claire Cardie
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: nlprishi@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://github.com/rishibommasani/rishibommasani.github.io/blob/master/papers/EMNLP2020.pdf&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://slideslive.com/38938755&quot;&gt;Video&lt;/a&gt; | &lt;a href=&quot;https://rishibommasani.github.io/&quot;&gt;Website&lt;/a&gt; | &lt;a href=&quot;https://virtual.2020.emnlp.org/paper_main.675.html&quot;&gt;Virtual Conference Room&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: summarization, datasets, evaluation&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;learning-music-helps-you-read-using-transfer-to-study-linguistic-structure-in-language-models&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2004.14601&quot;&gt;Learning Music Helps You Read: Using Transfer to Study Linguistic Structure in Language Models&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-11-15-emnlp-2020/img22&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Isabel Papadimitriou, Dan Jurafsky
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: isabelvp@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2004.14601&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: transfer learning, analysis, music, hierarchical structure&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;localizing-open-ontology-qa-semantic-parsers-in-a-day-using-machine-translation&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2010.05106.pdf&quot;&gt;Localizing Open-Ontology QA Semantic Parsers in a Day Using Machine Translation&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-11-15-emnlp-2020/img5&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Mehrad Moradshahi, Giovanni Campagna, Sina J. Semnani, Silei Xu, Monica S. Lam
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: mehrad@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/pdf/2010.05106.pdf&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://github.com/stanford-oval/SPL&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: machine translation, semantic parsing, localization&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;slm-learning-a-discourse-language-representation-with-sentence-unshuffling&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2010.16249.pdf&quot;&gt;SLM: Learning a Discourse Language Representation with Sentence Unshuffling&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-11-15-emnlp-2020/img18&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Haejun Lee, Drew A. Hudson, Kangwook Lee, Christopher D. Manning
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: dorarad@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/pdf/2010.16249.pdf&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: transformer, bert, language, understanding, nlp, squad, glue, sentences, discourse&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;utility-is-in-the-eye-of-the-user-a-critique-of-nlp-leaderboards&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2009.13888.pdf&quot;&gt;Utility is in the Eye of the User: A Critique of NLP Leaderboards&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-11-15-emnlp-2020/img0&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Kawin Ethayarajh, Dan Jurafsky
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: kawin@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/pdf/2009.13888.pdf&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://kawine.github.io/&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: nlp, leaderboard, utility, benchmark, fairness, efficiency&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;with-little-power-comes-great-responsibility&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2010.06595&quot;&gt;With Little Power Comes Great Responsibility&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-11-15-emnlp-2020/img4&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Dallas Card, Peter Henderson, Urvashi Khandelwal, Robin Jia, Kyle Mahowald, Dan Jurafsky
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: dcard@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2010.06595&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://github.com/dallascard/NLP-power-analysis&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: statistical power, experimental methodology, leaderboards, machine translation, human evaluation&lt;/p&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;findings-of-emnlp&quot;&gt;Findings of EMNLP&lt;/h2&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;desmog-detecting-stance-in-media-on-global-warming&quot;&gt;&lt;a href=&quot;https://www.aclweb.org/anthology/2020.findings-emnlp.296.pdf&quot;&gt;DeSMOG: Detecting Stance in Media On Global Warming&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-11-15-emnlp-2020/img16&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Yiwei Luo, Dallas Card, Dan Jurafsky
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: yiweil@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://www.aclweb.org/anthology/2020.findings-emnlp.296.pdf&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;http://stanford.edu/~yiweil/webpage.html&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: computational social science; framing; argumentation; stance; bias; climate change&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;investigating-transferability-in-pretrained-language-models&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2004.14975&quot;&gt;Investigating Transferability in Pretrained Language Models&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-11-15-emnlp-2020/img15&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Alex Tamkin, Trisha Singh, Davide Giovanardi, Noah Goodman
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: atamkin@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2004.14975&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;http://alextamkin.com&quot;&gt;Website&lt;/a&gt; | &lt;a href=&quot;https://virtual.2020.emnlp.org/paper_WS-1.1165_F.html&quot;&gt;Virtual Conference Room&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: finetuning, transfer learning, language models, bert, probing&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;stay-hungry-stay-focused-generating-informative-and-specific-questions-in-information-seeking-conversations&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2004.14530.pdf&quot;&gt;Stay Hungry, Stay Focused: Generating Informative and Specific Questions in Information-Seeking Conversations&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-11-15-emnlp-2020/img2&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Peng Qi, Yuhao Zhang, Christopher D. Manning
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: pengqi@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/pdf/2004.14530.pdf&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://qipeng.me/blog/learning-to-ask/&quot;&gt;Blog Post&lt;/a&gt; | &lt;a href=&quot;https://virtual.2020.emnlp.org/paper_WS-1.69_F.html&quot;&gt;Virtual Conference Room&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: conversational agents, question generation, natural language generation&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;do-language-embeddings-capture-scales&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2010.05345&quot;&gt;Do Language Embeddings Capture Scales?&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-11-15-emnlp-2020/img11&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Xikun Zhang*, Deepak Ramachandran*, Ian Tenney, Yanai Elazar, Dan Roth
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: xikunz2@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2010.05345&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://virtual.2020.emnlp.org/paper_findings.439.html&quot;&gt;Virtual Conference Room&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: probing, analysis, bertology, scales, common sense knowledge&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;on-the-importance-of-adaptive-data-collection-for-extremely-imbalanced-pairwise-tasks&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2010.05103&quot;&gt;On the Importance of Adaptive Data Collection for Extremely Imbalanced Pairwise Tasks&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-11-15-emnlp-2020/img7&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Stephen Mussmann, Robin Jia, Percy Liang
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: robinjia@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2010.05103&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://worksheets.codalab.org/worksheets/0x39ba5559790b4099a7ff75f916ce19a4&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: active learning, robustness, label imbalance&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;pragmatic-issue-sensitive-image-captioning&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2004.14451&quot;&gt;Pragmatic Issue-Sensitive Image Captioning&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-11-15-emnlp-2020/img12&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Allen Nie, Reuben Cohn-Gordon, Christopher Potts
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: anie@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2004.14451&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://slideslive.com/38940644/pragmatic-issuesensitive-image-captioning&quot;&gt;Video&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: controllable caption generation, question under discussion, discourse, pragmatics&lt;/p&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;workshops-and-co-located-conferences&quot;&gt;Workshops and Co-Located Conferences&lt;/h2&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;bleu-neighbors-a-reference-less-approach-to-automatic-evaluation&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2004.12726.pdf&quot;&gt;BLEU Neighbors: A Reference-less Approach to Automatic Evaluation&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-11-15-emnlp-2020/img3&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Kawin Ethayarajh, Dorsa Sadigh
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: kawin@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/pdf/2004.12726.pdf&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://kawine.github.io/&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: nlp, bleu, evaluation, nearest neighbors, dialogue&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;determining-question-answer-plausibility-in-crowdsourced-datasets-using-multi-task-learning&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2011.04883&quot;&gt;Determining Question-Answer Plausibility in Crowdsourced Datasets Using Multi-Task Learning&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-11-15-emnlp-2020/img17&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Rachel Gardner, Maya Varma, Clare Zhu, Ranjay Krishna
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: rachel0@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2011.04883&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: noisy text, bert, plausibility, multi-task learning&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;explaining-the-trump-gap-in-social-distancing-using-covid-discourse&quot;&gt;&lt;a href=&quot;https://openreview.net/pdf/baa636711f681ae8664818f378d565b17065c604.pdf&quot;&gt;Explaining the ‘Trump Gap’ in Social Distancing Using COVID Discourse&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-11-15-emnlp-2020/img20&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Austin van Loon, Sheridan Stewart, Brandon Waldon, Shrinidhi K. Lakshmikanth, Ishan Shah, Sharath Chandra Guntuku, Garrick Sherman, James Zou, Johannes Eichstaedt
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: avanloon@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://openreview.net/pdf/baa636711f681ae8664818f378d565b17065c604.pdf&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: computational social science, social distancing, word2vec, vector semantics, twitter, bert&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;learning-adaptive-language-interfaces-through-decomposition&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2010.05190&quot;&gt;Learning Adaptive Language Interfaces through Decomposition&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-11-15-emnlp-2020/img10&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Siddharth Karamcheti, Dorsa Sadigh, Percy Liang
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: skaramcheti@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2010.05190&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://virtual.2020.emnlp.org/paper_WS-6.10.html&quot;&gt;Virtual Conference Room&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: semantic parsing, interaction, decomposition&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;modeling-subjective-assessments-of-guilt-in-newspaper-crime-narratives&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2006.09589&quot;&gt;Modeling Subjective Assessments of Guilt in Newspaper Crime Narratives&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-11-15-emnlp-2020/img23&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Elisa Kreiss*, Zijian Wang*, Christopher Potts
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: ekreiss@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2006.09589&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://github.com/zijwang/modeling_guilt&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: psycholinguistics, pragmatics, token-level supervision, model attribution, news, guilt, hedges, corpus, subjectivity&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;neural-natural-language-inference-models-partially-embed-theories-of-lexical-entailment-and-negation&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2004.14623&quot;&gt;Neural Natural Language Inference Models Partially Embed Theories of Lexical Entailment and Negation&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-11-15-emnlp-2020/img13&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Atticus Geiger, Kyle Richardson, Chris Potts
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: atticusg@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2004.14623&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://atticusg.github.io/&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: entailment intervention causality systematic generalization&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;structured-self-attention-weights-encode-semantics-in-sentiment-analysis&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2010.04922&quot;&gt;Structured Self-Attention Weights Encode Semantics in Sentiment Analysis&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-11-15-emnlp-2020/img9&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Zhengxuan Wu, Thanh-Son Nguyen, Desmond C. Ong
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: wuzhengx@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2010.04922&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: attention, explainability, sentiment analysis&lt;/p&gt;
&lt;hr /&gt;

&lt;p&gt;We look forward to seeing you at EMNLP 2020!&lt;/p&gt;
</description>
              <pubDate>Sun, 15 Nov 2020 00:00:00 -0800</pubDate>
          </item>
          
        
          
          <item>
              <title>Learning to Influence Multi-Agent Interaction</title>
              <link>/blog/lili/</link>
              <guid isPermaLink="true">/blog/lili/</guid>
              <description>&lt;p&gt;Interaction with others is an important part of everyday life. No matter
the situation – whether it be playing a game of chess, carrying a
box together, or navigating lanes of traffic – we’re able to
seamlessly compete against, collaborate with, and acclimate to other
people.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimagethird&quot; src=&quot;/blog/assets/img/posts/2020-11-14-lili/motiv0.jpg&quot; /&gt;
&lt;img class=&quot;postimagethird&quot; src=&quot;/blog/assets/img/posts/2020-11-14-lili/motiv1.jpg&quot; /&gt;
&lt;img class=&quot;postimagethird&quot; src=&quot;/blog/assets/img/posts/2020-11-14-lili/motiv2.png&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Likewise, as robots become increasingly prevalent and capable, their
interaction with humans and other robots is inevitable. However, despite
the many advances in robot learning, most current algorithms are
designed for robots that act in isolation. These methods miss out on the
fact that other agents are also learning and changing – and so the
behavior the robot learns for the current interaction may not work
during the next one! Instead, can robots learn to seamlessly interact
with humans and other robots by taking their changing strategies into
account? In our new work (&lt;a href=&quot;http://iliad.stanford.edu/pdfs/publications/xie2020learning.pdf&quot;&gt;paper&lt;/a&gt;,
&lt;a href=&quot;https://sites.google.com/view/latent-strategies/&quot;&gt;website&lt;/a&gt;), we
begin to investigate this question.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-11-14-lili/hockey_sac.gif&quot; /&gt;&lt;/p&gt;
&lt;figcaption&gt;
A standard reinforcement learning agent (left) based on &lt;a href=&quot;https://arxiv.org/abs/1801.01290&quot;&gt;Soft
Actor-Critic&lt;/a&gt; (&lt;b&gt;SAC&lt;/b&gt;) assumes that
the opponent (right) follows a fixed strategy, and only blocks on its
left side.
&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Interactions with humans are difficult for robots because humans and
other intelligent agents don’t have fixed behavior – their
strategies and habits change over time. In other words, they update
their actions in response to the robot and thus continually change the
robot’s learning environment. Consider the robot on the left (the agent)
learning to play air hockey against the non-stationary robot on the
right. Rather than hitting the same shot every time, the other robot
modifies its policy between interactions to exploit the agent’s
weaknesses. If the agent ignores how the other robot changes, then it
will fail to adapt accordingly and learn a poor policy.&lt;/p&gt;

&lt;p&gt;The best defense for the agent is to block where it thinks the opponent
will next target. The robot therefore needs to anticipate how the
behavior of the other agent will change, and model how its own actions
affect the other’s behavior. People can deal with these scenarios on a
daily basis (e.g., driving, walking), and they do so without explicitly
modeling every low-level aspect of each other’s policy.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-11-14-lili/motiv3.gif&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Humans tend to be bounded-rational (i.e., their rationality is limited
by knowledge and computational capacity), and so likely keep track of
much less complex entities during interaction. Inspired by how humans
solve these problems, we recognize that robots also do not need to
explicitly model every low-level action another agent will make.
Instead, we can capture the hidden, underlying intent – what we call
latent strategy (in the sense that it underlies the actions of the
agent) – of other agents through learned low-dimensional
representations. These representations are learned by optimizing neural
networks based on experience interacting with these other agents.&lt;/p&gt;

&lt;h3 id=&quot;learning-and-influencing-latent-intent&quot;&gt;Learning and Influencing Latent Intent&lt;/h3&gt;

&lt;p&gt;We propose a framework for learning latent representations of another
agent’s policy: &lt;strong&gt;Learning and Influencing Latent Intent (LILI)&lt;/strong&gt;. The
agent of our framework identifies the relationship between its behavior
and the other agent’s future strategy, and then leverages these latent
dynamics to influence the other agent, purposely guiding them towards
policies suitable for co-adaptation. At a high level, the robot learns
two things: a way to predict latent strategy, and a policy for
responding to that strategy. The robot learns these during interaction
by “thinking back” to prior experiences, and figuring out what
strategies and policies it should have used.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/blog/assets/img/posts/2020-11-14-lili/method.png&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;h4 id=&quot;modeling-agent-strategies&quot;&gt;Modeling Agent Strategies&lt;/h4&gt;

&lt;p&gt;The first step, shown in the left side of the diagram above, is to learn
to represent the behavior of other agents. Many prior works assume
access to the underlying intentions or actions of other agents, which
can be a restrictive assumption. We instead recognize that a
low-dimensional representation of their behavior, i.e., their latent
strategy, can be inferred from the dynamics and rewards experienced by
the agent during the current interaction. Therefore, given a sequence of
interactions, we can train an
&lt;a href=&quot;https://en.wikipedia.org/wiki/Autoencoder&quot;&gt;encoder-decoder&lt;/a&gt;
model; the encoder embeds interaction &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; and predicts the next
latent strategy &lt;script type=&quot;math/tex&quot;&gt;z^{k+1}&lt;/script&gt;, and the decoder takes this prediction
and reconstructs the transitions and rewards observed during interaction
&lt;script type=&quot;math/tex&quot;&gt;k+1&lt;/script&gt;.&lt;/p&gt;

&lt;h4 id=&quot;influencing-by-optimizing-for-long-term-rewards&quot;&gt;Influencing by Optimizing for Long-Term Rewards&lt;/h4&gt;

&lt;p&gt;Given a prediction of what strategy the other agent will follow next,
the agent can learn how to &lt;em&gt;react&lt;/em&gt; to it, as illustrated on the right
side of the diagram above. Specifically, we train an agent policy
&lt;script type=&quot;math/tex&quot;&gt;\pi_\theta(a | s, z^i)&lt;/script&gt; with reinforcement learning (RL) to
make decisions conditioned on the latent strategy &lt;script type=&quot;math/tex&quot;&gt;z^i&lt;/script&gt; predicted
by the encoder.&lt;/p&gt;

&lt;p&gt;However, beyond simply &lt;em&gt;reacting&lt;/em&gt; to the predicted latent strategy, an
intelligent agent should proactively &lt;em&gt;influence&lt;/em&gt; this strategy to
maximize rewards over repeated interactions. Returning to our hockey
example, consider an opponent with three different strategies: it fires
to the left, down the middle, or to the right. Moreover, left-side shots
are easier for the agent to block and so gives a higher reward when
successfully blocked. The agent should influence its opponent to adopt
the left strategy more frequently in order to earn higher long-term
rewards.&lt;/p&gt;

&lt;p&gt;For learning this influential behavior, we train the agent policy
&lt;script type=&quot;math/tex&quot;&gt;\pi_\theta&lt;/script&gt; to maximize rewards across multiple interactions:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\max_\theta~\sum_{i=1}^{\infty} \gamma^i~ \mathbb{E} \left[ \sum_{t=1}^H R(s, z^i) \right]&lt;/script&gt;

&lt;p&gt;With this objective, the agent learns to generate interactions that
influence the other agent, and hence the system, toward outcomes that
are more desirable for the agent or for the team as a whole.&lt;/p&gt;

&lt;h3 id=&quot;experiments&quot;&gt;Experiments&lt;/h3&gt;

&lt;h4 id=&quot;2d-navigation&quot;&gt;2D Navigation&lt;/h4&gt;

&lt;p&gt;We first consider a simple point mass navigation task. Similar to
pursuit-evasion games, the agent needs to reach the other agent (i.e.,
the target) in a 2D plane. This target moves one step clockwise or
counterclockwise around a circle depending on where the agent ended the
previous interaction. Because the agent starts off-center, some target
locations can be reached more efficiently than others. Importantly, the
agent never observes the location of the target.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-11-14-lili/pm.png&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Below, we visualize 25 consecutive interactions from policies learned by
Soft Actor-Critic (&lt;strong&gt;SAC&lt;/strong&gt;) (a standard RL algorithm), &lt;strong&gt;LILI (no influence)&lt;/strong&gt;,
and &lt;strong&gt;LILI&lt;/strong&gt;. &lt;strong&gt;LILI (no influence)&lt;/strong&gt; corresponds to our approach without the
influencing objective; i.e., the agent optimizes rewards accumulated in
a &lt;em&gt;single&lt;/em&gt; interaction. The gray circle represents the target, while the
teal line marks the trajectory taken by the agent and the teal circle
marks the agent’s position at the final timestep of the interaction.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;figure class=&quot;postfigurethird&quot;&gt;
  &lt;img src=&quot;/blog/assets/img/posts/2020-11-14-lili/pm_sac.gif&quot; /&gt;
  &lt;figcaption&gt;
  &lt;b&gt;SAC&lt;/b&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;figure class=&quot;postfigurethird&quot;&gt;
  &lt;img src=&quot;/blog/assets/img/posts/2020-11-14-lili/pm_lili_no_influence.gif&quot; /&gt;
  &lt;figcaption&gt;
  &lt;b&gt;LILI (no influence)&lt;/b&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;figure class=&quot;postfigurethird&quot;&gt;
  &lt;img src=&quot;/blog/assets/img/posts/2020-11-14-lili/pm_lili.gif&quot; /&gt;
  &lt;figcaption&gt;
  &lt;b&gt;LILI&lt;/b&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;The &lt;strong&gt;SAC&lt;/strong&gt; policy, at convergence, moves to the center of the circle in
every interaction. Without knowledge of or any mechanism to infer where
the other agent is, the center of the circle gives the highest stable
rewards. In contrast, &lt;strong&gt;LILI (no influence)&lt;/strong&gt; successfully models the other
agent’s behavior dynamics and correctly navigates to the other agent,
but isn’t trained to influence the other agent. Our full approach &lt;strong&gt;LILI&lt;/strong&gt;
&lt;em&gt;does&lt;/em&gt; learn to influence: it traps the other agent at the top of the
circle, where the other agent is closest to the agent’s starting
position and yields the highest rewards.&lt;/p&gt;

&lt;h4 id=&quot;robotic-air-hockey&quot;&gt;Robotic Air Hockey&lt;/h4&gt;

&lt;p&gt;Next, we evaluate our approach on the air hockey task, played between
two robotic agents. The agent first learns alongside a robot opponent,
then plays against a human opponent. The opponent is a rule-based agent
which always aims away from where the agent last blocked. When blocking,
the robot does not know where the opponent is aiming, and only observes
the vertical position of the puck. We additionally give the robot a
bonus reward if it blocks a shot on the left of the board, which
incentivizes the agent to influence the opponent into aiming left.&lt;/p&gt;

&lt;p&gt;In contrast to the &lt;strong&gt;SAC&lt;/strong&gt; agent, the &lt;strong&gt;LILI&lt;/strong&gt; agent learns to anticipate
the opponent’s future strategies and successfully block the different
incoming shots.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-11-14-lili/hockey_lili.gif&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Because the agent receives a bonus reward for blocking left, it should
lead the opponent into firing left more often. &lt;strong&gt;LILI (no influence)&lt;/strong&gt; fails
to guide the opponent into taking advantage of this bonus: the
distribution over the opponent’s strategies is uniform. In contrast,
&lt;strong&gt;LILI&lt;/strong&gt; leads the opponent to strike left 41% of the time, demonstrating
the agent’s ability to influence the opponent. Specifically, the agent
manipulates the opponent into alternating between the left and middle
strategies.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-11-14-lili/influence.jpg&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Finally, we test the policy learned by &lt;strong&gt;LILI (no influence)&lt;/strong&gt; against a
human player following the same strategy pattern as the robot opponent.
Importantly, the human has imperfect aim and so introduces new noise to
the environment. We originally intended to test our approach &lt;strong&gt;LILI&lt;/strong&gt; with
human opponents, but we found that – although &lt;strong&gt;LILI&lt;/strong&gt; worked well when
playing against another robot – the learned policy was too brittle
and did not generalize to playing alongside human opponents. However,
the policy learned with &lt;strong&gt;LILI (no influence)&lt;/strong&gt; was able to block 73% of
shots from the human.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-11-14-lili/human.gif&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;h3 id=&quot;final-thoughts&quot;&gt;Final Thoughts&lt;/h3&gt;

&lt;p&gt;We proposed a framework for multi-agent interaction that represents the
behavior of other agents with learned high-level strategies, and
incorporates these strategies into an RL algorithm. Robots with our
approach were able to anticipate how their behavior would affect another
agent’s latent strategy, and actively influenced that agent for more
seamless co-adaptation.&lt;/p&gt;

&lt;p&gt;Our work represents a step towards building robots that act alongside
humans and other agents. To this end, we’re excited about these next
steps:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The agents we examined in our experiments had a small number of simple strategies determining their behavior. We’d like to study the scalability of our approach to more complex agent strategies that we’re likely to see in humans and intelligent agents.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Instead of training alongside artificial agents, we hope to study the human-in-the-loop setting in order to adapt to the dynamic needs and preferences of real people.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;This post is based on the following paper:&lt;/p&gt;

&lt;p&gt;Annie Xie, Dylan P. Losey, Ryan Tolsma, Chelsea Finn, Dorsa Sadigh.
&lt;a href=&quot;http://iliad.stanford.edu/pdfs/publications/xie2020learning.pdf&quot;&gt;&lt;strong&gt;Learning Latent Representations for Multi-Agent Interaction.&lt;/strong&gt;&lt;/a&gt;
&lt;a href=&quot;https://sites.google.com/view/latent-strategies/&quot;&gt;Project webpage&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Finally, thanks to Dylan Losey, Chelsea Finn, Dorsa Sadigh, Andrey Kurenkov, and Michelle Lee for valuable feedback on this post.&lt;/p&gt;
</description>
              <pubDate>Sat, 14 Nov 2020 00:00:00 -0800</pubDate>
          </item>
          
        
          
          <item>
              <title>Bootleg: Chasing the Tail with Self-Supervised Named Entity Disambiguation</title>
              <link>/blog/bootleg/</link>
              <guid isPermaLink="true">/blog/bootleg/</guid>
              <description>&lt;figure style=&quot;text-align: center&quot;&gt;
    &lt;img style=&quot;width: 20%;&quot; src=&quot;/blog/assets/img/posts/2020-11-12-bootleg/logo.png&quot; /&gt;
&lt;/figure&gt;

&lt;p&gt;Named entity disambiguation (NED) is the process of mapping “strings” to “things” in a knowledge base. You have likely already used a system that requires NED multiple times today. Every time you ask a question to your personal assistant or issue a search query on your favorite browser, these systems use NED to understand what people, places, and things (entities) are being talked about.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-11-12-bootleg/ned_example_1.svg&quot; /&gt;&lt;/p&gt;
&lt;figcaption style=&quot;text-align: left;&quot;&gt;Named entity disambiguation example. The ambiguous “Lincoln” refers to the car, not the person or location.&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Take the example shown above. You ask your personal assistant “What is the average gas mileage of a Lincoln?”. The assistant would need NED to know that “Lincoln” refers to Lincoln Motors (the car company)—not the former president or city in Nebraska. The ambiguity of mentions in text is what makes NED so challenging as it requires the use of subtle cues.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage_90&quot; src=&quot;/blog/assets/img/posts/2020-11-12-bootleg/ned_distribution_2.svg&quot; /&gt;&lt;/p&gt;
&lt;figcaption style=&quot;text-align: left;&quot;&gt;The spectrum of entities. Popular (head) entities occur frequently in data while rare (tail) entities are infrequent.&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;NED gets more interesting when we examine the full spectrum of entities shown above, specifically the more rare &lt;em&gt;tail&lt;/em&gt; and &lt;em&gt;unseen&lt;/em&gt; entities. These are entities that occur infrequently or not at all in data. &lt;strong&gt;Performance over the tail is critical because the majority of entities are rare.&lt;/strong&gt; In &lt;a href=&quot;https://www.wikidata.org/wiki/Wikidata:Main_Page&quot;&gt;Wikidata&lt;/a&gt;, only 13% of entities even have Wikipedia pages as a source of textual information.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage_60&quot; src=&quot;/blog/assets/img/posts/2020-11-12-bootleg/frequency_plot_3.svg&quot; /&gt;&lt;/p&gt;
&lt;figcaption style=&quot;text-align: left;&quot;&gt;Bootleg compared to a BERT-based baseline model &lt;a href=&quot;https://arxiv.org/pdf/2005.14253.pdf&quot;&gt;Févry et el. 2020&lt;/a&gt; showing average F1 versus number of times an entity occurred in the training data. As there are 15x the number of entities in Wikidata than in Wikipedia (most of them are rare) and the baseline model needs to see an entity on average 100x for it to achieve 60 F1, it follows that the baseline model would need to train on data 1,500x the size of Wikipedia to achieve 60 F1 over all entities.&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Prior approaches to NED use BERT-based systems to memorize textual patterns associated with an entity (e.g., Abraham Lincoln is associated with “president”). As shown above, the SotA BERT-based &lt;strong&gt;baseline&lt;/strong&gt; from &lt;a href=&quot;https://arxiv.org/pdf/2005.14253.pdf&quot;&gt;Févry&lt;/a&gt; does a great job at memorizing patterns over popular entities (it achieves 86 F1 points over all entities). For the rare entities, it does much worse (58 F1 points lower on the tail). One possible solution to better tail performance is to simply train over more data, but this would likely require training over data 1,500x the size of Wikipedia for the model to achieve 60 F1 points over all entities!&lt;/p&gt;

&lt;p&gt;In this blog post, we present &lt;strong&gt;Bootleg&lt;/strong&gt;, a self-supervised approach to NED that is better able to handle rare entities.&lt;/p&gt;

&lt;h1 id=&quot;tail-disambiguation-through-ned-reasoning-patterns&quot;&gt;Tail Disambiguation through NED Reasoning Patterns&lt;/h1&gt;

&lt;p&gt;The question we are left with is how to disambiguate these rare entities? &lt;strong&gt;Our insight is that humans disambiguate entities, including rare entities, by using signals from text as well as from entity relations and types.&lt;/strong&gt; For example, the sentence “What is the gas mileage of a Lincoln?” requires reasoning that cars have a gas mileage, not people or locations. This can be used to reason that the mention of “Bluebird” in “What is the average gas mileage of a Bluebird?” refers to the car, a Nissan Bluebird, not the animal. Our goal in Bootleg is to train a model to reason over entity types and relations and better identify these tail entities.&lt;/p&gt;

&lt;p&gt;Through empirical analysis, we found four reasoning patterns for NED, shown and defined in the figure below.&lt;/p&gt;
&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage_90&quot; src=&quot;/blog/assets/img/posts/2020-11-12-bootleg/reasoning_patterns_4.svg&quot; /&gt;&lt;/p&gt;
&lt;figcaption style=&quot;text-align: left;&quot;&gt;Four reasoning patterns of NED. Each pattern uses some combination of entity, type, and relation information.&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;These patterns rely on signals from entities, types, and relations. Luckily, &lt;strong&gt;tail entities do not have equally rare types and relations&lt;/strong&gt;. This means we should be able to learn type and relation patterns from our data that can apply to tail entities.&lt;/p&gt;

&lt;h1 id=&quot;bootleg-a-model-for-tail-ned&quot;&gt;Bootleg: A Model for Tail NED&lt;/h1&gt;
&lt;p&gt;Bootleg takes as input a sentence, determines the possible entity candidates that could be mentioned in the sentence, and outputs the most likely candidates. The core insight that enables Bootleg to better identify rare entities is in how it internally represents entities.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage_60&quot; src=&quot;/blog/assets/img/posts/2020-11-12-bootleg/candidate_embedding_5.svg&quot; /&gt;&lt;/p&gt;
&lt;figcaption style=&quot;text-align: left;&quot;&gt;The creation of an entity candidate representation. Each candidate is a combination of an entity, type, and relation learned embedding.&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Similar to how words are often represented by continuous word embeddings (e.g., &lt;a href=&quot;https://arxiv.org/pdf/1810.04805.pdf&quot;&gt;BERT&lt;/a&gt; or &lt;a href=&quot;https://arxiv.org/pdf/1802.05365.pdf&quot;&gt;ELMo&lt;/a&gt;), Bootleg represents entity candidates as a combination of a unique entity embedding, a type embedding, and a relation embedding, as shown above. For example, each car entity will get the &lt;em&gt;same&lt;/em&gt; car type embedding (likewise for relations) which will encode patterns learned over all cars in the training data. A rare car can then use this global “car type” knowledge for disambiguation, as it will have the car embedding as part of its representation.&lt;/p&gt;

&lt;p&gt;To output the correct entities, Bootleg uses these representations in a stacked &lt;a href=&quot;https://arxiv.org/pdf/1706.03762.pdf&quot;&gt;Transformer&lt;/a&gt; module to allow the model to naturally learn the useful patterns for disambiguation without hard-coded rules. Bootleg then scores the output candidate representations and returns the most likely candidates.&lt;/p&gt;

&lt;p&gt;There are other exciting techniques we present in our &lt;a href=&quot;https://arxiv.org/pdf/2010.10363.pdf&quot;&gt;paper&lt;/a&gt; regarding regularization and weak labeling to improve tail performance.&lt;/p&gt;

&lt;h1 id=&quot;bootleg-improves-tail-performance-and-allows-for-knowledge-transfer&quot;&gt;Bootleg Improves Tail Performance and Allows for Knowledge Transfer&lt;/h1&gt;

&lt;p&gt;Our simple insight of training a model to reason over types and relations &lt;strong&gt;provides state-of-the-art performance on three standard NED benchmarks&lt;/strong&gt; – matching or exceeding SotA by up to 5.6 F1 points – and &lt;strong&gt;outperforms a BERT-based NED baseline by 5.4 F1 points over all entities and 40 F1 points over tail entities&lt;/strong&gt; (see F1 versus entity occurrence plot above).&lt;/p&gt;

&lt;p&gt;
  &lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;table&gt;
    &lt;thead&gt;
        &lt;tr&gt;
          &lt;th&gt;Benchmark&lt;/th&gt;
          &lt;th&gt;System&lt;/th&gt;
            &lt;th&gt;Precision&lt;/th&gt;
            &lt;th&gt;Recall&lt;/th&gt;
            &lt;th&gt;F1&lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td rowspan=&quot;2&quot;&gt;&lt;a href=&quot;https://www.hoffart.ai/wp-content/papercite-data/pdf/hoffart-2012vx.pdf&quot;&gt;KORE50&lt;/a&gt;&lt;/td&gt;
        &lt;td&gt;&lt;a href=&quot;https://www.mdpi.com/2073-8994/11/4/453&quot;&gt;Hu et al., 2019&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;80.0&lt;/td&gt;
          &lt;td&gt;79.8&lt;/td&gt;
          &lt;td&gt;79.9&lt;/td&gt;
      &lt;/tr&gt;
    &lt;td&gt;Bootleg&lt;/td&gt;
        &lt;td&gt;&lt;b&gt;86.0&lt;/b&gt;&lt;/td&gt;
        &lt;td&gt;&lt;b&gt;85.4&lt;/b&gt;&lt;/td&gt;
        &lt;td&gt;&lt;b&gt;85.7&lt;/b&gt;&lt;/td&gt;
      &lt;tr&gt;
        &lt;td rowspan=&quot;2&quot;&gt;&lt;a href=&quot;https://link.springer.com/chapter/10.1007/978-3-642-41335-3_9&quot;&gt;RSS500&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;&lt;a href=&quot;https://arxiv.org/pdf/1802.01074.pdf&quot;&gt;Phan et al., 2019&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;82.3 &lt;/td&gt;
          &lt;td&gt;82.3&lt;/td&gt;
          &lt;td&gt;82.3&lt;/td&gt;
      &lt;/tr&gt;
      &lt;td&gt;Bootleg&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;82.5&lt;/b&gt; &lt;/td&gt; 
      &lt;td&gt;&lt;b&gt;82.5&lt;/b&gt;&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;82.5&lt;/b&gt;&lt;/td&gt;
      &lt;tr&gt;
        &lt;td rowspan=&quot;2&quot;&gt;&lt;a href=&quot;https://www.aclweb.org/anthology/D11-1072.pdf&quot;&gt;AIDA CoNLL YAGO&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;&lt;a href=&quot;https://arxiv.org/pdf/2005.14253.pdf&quot;&gt;Févry et al., 2020&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
          &lt;td&gt;&lt;b&gt;96.7&lt;/b&gt;&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
    &lt;td&gt;Bootleg&lt;/td&gt;
    &lt;td&gt;96.9&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;96.7&lt;/b&gt;&lt;/td&gt;
    &lt;td&gt;96.8&lt;/td&gt;
    &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;We’ll now show how the entity knowledge encoded in Bootleg’s entity representations can transfer to non-NED tasks. We extract our entity representations and use them in both a production task at a major technology company and relation extraction task. We find that the use of Bootleg embeddings in the production task provides a 8% lift in performance and even improves quality over Spanish, French, and German languages. We repeat this experiment by adding Bootleg representations to a SotA model for the &lt;a href=&quot;https://arxiv.org/pdf/2004.14855.pdf&quot;&gt;TACRED&lt;/a&gt; relation extraction task (see &lt;a href=&quot;https://github.com/HazyResearch/bootleg/tree/master/tutorials/downstream_tutorial&quot;&gt;tutorial&lt;/a&gt;). We find this Bootleg-enhanced model sets a new SotA by 1 F1 point.&lt;/p&gt;

&lt;p&gt;
  &lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;table&gt;
    &lt;thead&gt;
        &lt;tr&gt;
          &lt;th&gt;Model&lt;/th&gt;
          &lt;th&gt;TACRED F1&lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Bootleg-Enhanced&lt;/td&gt;
        &lt;td&gt;&lt;b&gt;80.3&lt;/b&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;&lt;a href=&quot;https://arxiv.org/pdf/1909.04164.pdf&quot;&gt;KnowBERT&lt;/a&gt;&lt;/td&gt;
        &lt;td&gt;79.3&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;&lt;a href=&quot;https://arxiv.org/pdf/1907.10529.pdf&quot;&gt;SpanBERT&lt;/a&gt;&lt;/td&gt;
        &lt;td&gt;78.0&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;These results suggest that Bootleg entity representations can transfer entity knowledge to other language tasks!&lt;/p&gt;

&lt;h1 id=&quot;recap&quot;&gt;Recap&lt;/h1&gt;
&lt;p&gt;To recap, we described the problem of the tail of NED and showed that existing NED systems fall short at disambiguating these rare, yet important entities. We then introduced four reasoning patterns for NED and described how we trained Bootleg to learn these patterns through the use of embeddings and Transformer modules. We finally showed that Bootleg is a SotA NED system that better disambiguates rare entities than prior methods. Further, Bootleg learns representations that can transfer entity knowledge to non-NED tasks.&lt;/p&gt;

&lt;p&gt;We are actively developing Bootleg and would love to hear your thoughts. See our &lt;a href=&quot;http://hazyresearch.stanford.edu/bootleg/&quot;&gt;website&lt;/a&gt;, &lt;a href=&quot;https://github.com/HazyResearch/bootleg&quot;&gt;source code&lt;/a&gt;, and &lt;a href=&quot;https://arxiv.org/pdf/2010.10363.pdf&quot;&gt;paper&lt;/a&gt;.&lt;/p&gt;
</description>
              <pubDate>Thu, 12 Nov 2020 00:00:00 -0800</pubDate>
          </item>
          
        
          
          <item>
              <title>Measuring Bias in NLP (with Confidence!)</title>
              <link>/blog/bias-nlp/</link>
              <guid isPermaLink="true">/blog/bias-nlp/</guid>
              <description>&lt;p&gt;Countless studies have found that “bias” – typically with respect to race and gender – pervades the &lt;a href=&quot;https://arxiv.org/abs/1904.03310&quot;&gt;embeddings&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/1804.09301&quot;&gt;predictions&lt;/a&gt; of the black-box models that dominate natural language processing (NLP). For example, the language model &lt;a href=&quot;https://en.wikipedia.org/wiki/GPT-3&quot;&gt;GPT-3&lt;/a&gt;, of OpenAI fame, can generate &lt;a href=&quot;https://www.technologyreview.com/2020/10/23/1011116/chatbot-gpt3-openai-facebook-google-safety-fix-racist-sexist-language-ai/&quot;&gt;racist rants&lt;/a&gt; when given the right prompt. Attempts to detect hate speech can itself harm minority populations, &lt;a href=&quot;https://www.aclweb.org/anthology/P19-1163.pdf&quot;&gt;whose dialect is more likely to be flagged as hateful&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This, in turn, has led to a wave of work on how to “&lt;a href=&quot;http://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-d&quot;&gt;debias&lt;/a&gt;” models, only for others to find ways in which debiased models &lt;a href=&quot;https://arxiv.org/abs/1903.03862&quot;&gt;are still biased&lt;/a&gt;, and so on.&lt;/p&gt;

&lt;p&gt;But are these claims of NLP models being biased (or unbiased) being made with enough evidence?&lt;/p&gt;

&lt;p&gt;Consider the sentence &lt;em&gt;“The doctor gave instructions to the nurse before she left.”&lt;/em&gt; A &lt;a href=&quot;https://en.wikipedia.org/wiki/Coreference#Coreference_resolution&quot;&gt;co-reference resolution system&lt;/a&gt;, tasked with finding which person the pronoun “she” is referring to&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;, may incorrectly predict that it’s the nurse. Does this incorrect prediction – which conforms to gender stereotypes that doctors are usually male – mean that the system is gender-biased? Possibly – but it may also make mistakes in the other direction with equal frequency (e.g., thinking “he” refers to a nurse when it doesn’t). What if the system makes gender-stereotypical mistakes on not one sentence, but 100, or 1000? Then we could be more confident in claiming that it’s biased.&lt;/p&gt;

&lt;p&gt;In my ACL 2020 paper, “&lt;a href=&quot;https://www.aclweb.org/anthology/2020.acl-main.262/&quot;&gt;Measuring Fairness under Uncertainty with Bernstein Bounds&lt;/a&gt;”, I go over how, in the haste to claim the presence or absence of bias, the inherent uncertainty in measuring bias is often overlooked in the literature:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Bias is not a single number&lt;/strong&gt;. When we test how biased a model is, we are &lt;em&gt;estimating&lt;/em&gt; its bias on a sample of the data; our estimate may suggest that the model is biased or unbiased, but the opposite could still be true.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;This uncertainty can be captured using confidence intervals.&lt;/strong&gt; Instead of reporting a single number for bias, practitioners should report an interval, based on factors such as the desired confidence and the proposed definition of “bias”.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Existing datasets are too small to conclusively identify bias.&lt;/strong&gt; Existing datasets for measuring specific biases can only be used to make 95% confidence claims when the bias estimate is egregiously high; to catch more subtle bias, the NLP community needs bigger datasets.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Although this problem can exist with any kind of model, we focus on a remedy for classification models in particular.&lt;/p&gt;

&lt;h3 id=&quot;bernstein-bounded-unfairness&quot;&gt;Bernstein-Bounded Unfairness&lt;/h3&gt;

&lt;p&gt;A bias estimate, made using a small sample of data, likely differs from the true bias (i.e., at the population-level). How can we express our uncertainty about the estimate? We propose a method called Bernstein-bounded unfairness that translates this uncertainty into a confidence interval&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;Let’s say we want to measure whether some &lt;a href=&quot;https://en.wikipedia.org/wiki/Protected_group&quot;&gt;protected group&lt;/a&gt; &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; – that is legally protected due to an attribute such as race or gender – is being discriminated against by some classifier, relative to some unprotected group &lt;script type=&quot;math/tex&quot;&gt;B&lt;/script&gt;. They occur in the population with frequency &lt;script type=&quot;math/tex&quot;&gt;\gamma_A, \gamma_B&lt;/script&gt; respectively. We need&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;An annotation function &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; that maps each example &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; to &lt;script type=&quot;math/tex&quot;&gt;A, B,&lt;/script&gt; or neither. Note that the annotation function maps inputs to the protected/unprotected groups, not to the output space &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt;. For example, if we wanted to study how a sentiment classifier performed across different racial groups, then the inputs &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; would be sentences, labels &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; would be the sentiment, and the annotation function &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; might map &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; to {white, non-white} depending on the racial group of the sentence author.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A cost function &lt;script type=&quot;math/tex&quot;&gt;c : (y, \hat{y}) \to [0,C]&lt;/script&gt; that describes the cost of incorrectly predicting &lt;script type=&quot;math/tex&quot;&gt;\hat{y}&lt;/script&gt; when the true label is &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;C&lt;/script&gt; is the maximum possible cost. Since a model making an incorrect prediction for &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; is an undesirable outcome for the group that &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; belongs to, we frame this as a cost that must be borne by the group.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We want to choose these functions such that our bias metric of choice – which we call the &lt;em&gt;groupwise disparity&lt;/em&gt; &lt;script type=&quot;math/tex&quot;&gt;\delta(f,c)&lt;/script&gt; – can be expressed as the difference in expected cost borne by the protected and unprotected groups. Given a model that makes predictions &lt;script type=&quot;math/tex&quot;&gt;\hat{y}_a&lt;/script&gt; for protected &lt;script type=&quot;math/tex&quot;&gt;x_a \in A&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\hat{y}_b&lt;/script&gt; for unprotected &lt;script type=&quot;math/tex&quot;&gt;x_b \in B&lt;/script&gt;, we want to express the bias as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\delta(f,c) = \mathbb{E}_a[c(y_a, \hat{y}_a)] - \mathbb{E}_b[c(y_b, \hat{y}_b)]&lt;/script&gt;

&lt;p&gt;If the protected group is incurring higher costs in expectation, it is being biased against. For example, if we want to determine whether a classifier is more accurate on the unprotected group &lt;script type=&quot;math/tex&quot;&gt;B&lt;/script&gt;, then we would set the cost function to be the 1-0 loss (1 for an incorrect prediction, 0 for a correct one). If &lt;script type=&quot;math/tex&quot;&gt;B&lt;/script&gt; has a lower cost on average then &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;, then it would mean that the classifier is more accurate on &lt;script type=&quot;math/tex&quot;&gt;B&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;For a desired confidence level &lt;script type=&quot;math/tex&quot;&gt;\rho \in [0,1)&lt;/script&gt;, a dataset of &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; examples, and the variance &lt;script type=&quot;math/tex&quot;&gt;\sigma^2&lt;/script&gt; of the amortized groupwise disparity across examples, the confidence interval &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; would be&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
t &amp;= \frac{B + \sqrt{B^2 - 8 n \sigma^2 \log \left[\frac{1}{2} (1 - \rho) \right]}}{2n} \\
\text{where } B &amp;= -\frac{2 C}{3 \gamma} \log \left[ \frac{1}{2} (1 - \rho) \right],  \gamma = \min(\gamma_A, \gamma_B)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;If we set &lt;script type=&quot;math/tex&quot;&gt;\rho = 0.95&lt;/script&gt;, we could claim with 95% confidence that the true bias experienced by the protected group lies in the interval &lt;script type=&quot;math/tex&quot;&gt;[ \hat{\delta} - t, \hat{\delta} + t]&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;\hat{\delta}&lt;/script&gt; is our bias estimate.&lt;/p&gt;

&lt;h3 id=&quot;why-we-need-bigger-datasets&quot;&gt;Why We Need Bigger Datasets&lt;/h3&gt;

&lt;p&gt;If we want to say with 95% confidence that a classifier is biased &lt;em&gt;to some extent&lt;/em&gt; – but want to spend as little time annotating data as possible – we need to find the smallest &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; such that &lt;script type=&quot;math/tex&quot;&gt;0 \not\in [ \hat{\delta} - t, \hat{\delta} + t]&lt;/script&gt;. We can do this by working backwards from the formula for &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; given above (see paper for details).&lt;/p&gt;

&lt;p&gt;Let’s go back to our original example. Say we want to figure out whether a co-reference resolution system, tasked with matching pronouns to the nouns they refer to, is gender-biased or not. We have a dataset of 500 examples to test whether the model does better on gender-stereotypical examples (e.g., a female nurse) than non-gender-stereotypical examples (e.g., a male nurse). Since we are measuring the difference in accuracy, we set the cost function to be the 1-0 loss.&lt;/p&gt;

&lt;p&gt;On this dataset, our bias estimate for a model we’re evaluating is &lt;script type=&quot;math/tex&quot;&gt;\bar{\delta} = 0.05&lt;/script&gt;. Is this enough to claim with 95% confidence that the model is gender-biased?&lt;/p&gt;

&lt;p&gt;In this scenario &lt;script type=&quot;math/tex&quot;&gt;C = 1, \bar{\delta} = 0.05, \rho = 0.95&lt;/script&gt;. We assume that there are equally many stereotypical and non-stereotypical examples and that the variance is maximal, so &lt;script type=&quot;math/tex&quot;&gt;\gamma = 0.5, \sigma^2 = 4&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;With these settings, &lt;script type=&quot;math/tex&quot;&gt;n &gt; 11903&lt;/script&gt;; we would need a dataset of more than 11903 examples to claim with 95% confidence that the co-reference resolution system is gender-biased. This is roughly 3.8 times larger than &lt;a href=&quot;https://arxiv.org/abs/1804.06876&quot;&gt;WinoBias&lt;/a&gt;, the largest dataset currently available for this purpose. We could only use WinoBias if &lt;script type=&quot;math/tex&quot;&gt;\bar{\delta} = 0.0975&lt;/script&gt; – that is, if the sample bias were almost twice as high.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
	&lt;img src=&quot;/blog/assets/img/posts/2020-11-11-bias-nlp/bbu_3.png&quot; style=&quot;width: 80%&quot; /&gt;
    &lt;figcaption&gt;As seen above, the WinoBias dataset cannot be used to make claims of bias with 95% confidence unless the sample bias is egregiously high.&lt;/figcaption&gt;
&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;In the haste to claim the presence or absence of bias in models, the uncertainty in estimating bias is often overlooked in the literature. A model’s bias is often thought of as a single number, even though this number is ultimately an estimate and not the final word on whether the model is or is not biased.&lt;/p&gt;

&lt;p&gt;We proposed a method called Bernstein-bounded unfairness for capturing this uncertainty using confidence intervals. To faithfully reflect the range of possible conclusions, we recommend that NLP practitioners measuring bias not only report their bias estimate but also this confidence interval.&lt;/p&gt;

&lt;p&gt;What if we want to catch more subtle bias? Although it may be possible to derive tighter confidence intervals, what we really need are larger bias-specific datasets. The datasets we currently have are undoubtedly helpful, but they need to be much larger in order to diagnose biases with confidence.&lt;/p&gt;

&lt;h5 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h5&gt;

&lt;p class=&quot;small-text&quot;&gt; 
Many thanks to Krishnapriya Vishnubhotla, Michelle Lee, and Kaitlyn Zhou for their feedback on this blog post.
&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;The goal of coreference resolution more broadly is to find all expressions that refer to the same entity in a text. For example, in “I gave my mother Sally a gift for her birthday.”, the terms “my mother”, “Sally”, and “her” all refer to the same entity. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;We use &lt;a href=&quot;https://en.wikipedia.org/wiki/Bernstein_inequalities_(probability_theory)&quot;&gt;Bernstein’s inequality&lt;/a&gt; to derive the confidence intervals, hence the name Bernstein-bounded unfairness. This inequality tells us with what probability the average of &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; independent random variables will be within a constant $t$ of their true mean $\mu$. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
              <pubDate>Wed, 11 Nov 2020 00:00:00 -0800</pubDate>
          </item>
          
        
          
          <item>
              <title>Learning to Fix Programs from Error Messages</title>
              <link>/blog/DrRepair/</link>
              <guid isPermaLink="true">/blog/DrRepair/</guid>
              <description>&lt;h3 id=&quot;machine-learning-for-program-repair&quot;&gt;&lt;strong&gt;Machine Learning for Program Repair&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;When writing programs, a lot of time is spent debugging or fixing source code errors, both for beginners (imagine the intro programming classes you took) as well as for professional developers (for example, &lt;a href=&quot;https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42184.pdf&quot;&gt;this case study from Google&lt;/a&gt; &lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;). Automating program repair could dramatically enhance the productivity of both programming and learning programming. In &lt;a href=&quot;https://arxiv.org/pdf/2005.10636.pdf&quot;&gt;our recent work&lt;/a&gt; published at ICML 2020, we study how to use machine learning to repair programs automatically.&lt;/p&gt;

&lt;h3 id=&quot;problem-setting&quot;&gt;&lt;strong&gt;Problem Setting&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Programmers write programs incrementally: write code, compile or execute it, and if there are any errors, repair the program based on the received feedback. Can we model and solve this problem with machine learning?&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage&quot; src=&quot;/blog/assets/img/posts/2020-11-08-DrRepair/task.png&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Let’s say we have a broken C++ program (figure left), where the &lt;code class=&quot;highlighter-rouge&quot;&gt;char&lt;/code&gt; in line 5 should actually be &lt;code class=&quot;highlighter-rouge&quot;&gt;string&lt;/code&gt;. When we compile it, we get an error (figure top right), which says “line 9 is requesting for size in &lt;code class=&quot;highlighter-rouge&quot;&gt;a&lt;/code&gt; which is of type &lt;code class=&quot;highlighter-rouge&quot;&gt;char&lt;/code&gt;”. From this message, a programmer can notice that the error is related to the type of the variable &lt;code class=&quot;highlighter-rouge&quot;&gt;a&lt;/code&gt;, track how &lt;code class=&quot;highlighter-rouge&quot;&gt;a&lt;/code&gt; has been used or declared in the source code, reaching line 5, and then edit the line to fix the error.  Thus, the concrete task we want our machine learning model to solve is, given broken code (figure left) and an error message (figure top right), &lt;strong&gt;localize&lt;/strong&gt; the error line (line 5) and &lt;strong&gt;generate a repaired version&lt;/strong&gt; of it (“string tmp, a, b;”) (figure bottom right).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Challenges&lt;/strong&gt;:
This task poses two main challenges. First, on the modeling side, we need to connect and jointly reason over two modalities, the program and the error message: for instance, tracking variables that caused the error as we saw in the example above. Second, on the training data side, we need an efficient source of data that provides supervision for correcting broken programs; unfortunately, existing labeled datasets with &amp;lt;broken code, fixed code&amp;gt; pairs are small and hard to come by, and don’t scale up. In this work, we introduce promising solutions to those two challenges by: 1) modeling program repair with program-feedback graph, and 2) introducing a self-supervised training scheme that uses unlabeled programs.&lt;/p&gt;

&lt;h3 id=&quot;modeling-approach-program-feedback-graph&quot;&gt;&lt;strong&gt;Modeling Approach: Program-Feedback Graph&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;How can we effectively connect the two modalities (programs and error messages) and perform the reasoning needed for repair? To achieve this, we introduce a program-feedback graph, a joint graph representation that connects symbols across the program and error message. For instance, the compiler message in the example mentions &lt;code class=&quot;highlighter-rouge&quot;&gt;a&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;size&lt;/code&gt;, and &lt;code class=&quot;highlighter-rouge&quot;&gt;char&lt;/code&gt;, so we connect these symbols to their occurrences in the source code, to capture semantic correspondence. This way, we treat the two modalities in a shared semantic space rather than separately. We then perform reasoning over the symbols in this space using &lt;a href=&quot;https://arxiv.org/abs/1710.10903&quot;&gt;graph attention&lt;/a&gt; &lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-11-08-DrRepair/graph.png&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Specifically, for the model architecture, we build on the encoder-decoder framework commonly used in NLP, which encodes input sequences (in our case, the program and error message; next figure bottom) and then decodes outputs (in our case, the localized line index, and the repaired version of the line; figure top), and we incorporate a graph attention module applied to the program-feedback graph in the intermediate layer of the architecture (figure middle).&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage&quot; src=&quot;/blog/assets/img/posts/2020-11-08-DrRepair/model.png&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;h3 id=&quot;training-approach-self-supervised-learning&quot;&gt;&lt;strong&gt;Training Approach: Self-Supervised Learning&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Our second technique is self-supervised learning. Labeled datasets of program repair are small, but there are vast amounts of unlabeled programs available online. For example, GitHub has more than 30M public repositories. Using this large amount of freely available code to improve learning program repair would significantly enhance the scalability and reliability of our system.
Our idea is as follows: we first collect unlabeled, working programs from online resources such as GitHub and codeforce.com (figure left). We then design randomized program corruption procedures (e.g. delete/insert/replace tokens) and corrupt the unlabeled programs (figure middle). As a result, the corrupted programs give us errors (figure right). This way, we can create a lot of new examples of program repair, &amp;lt;broken code, error message, fixed code&amp;gt;. We can use this extra data to pre-train the program repair model, and then fine-tune on the labeled target dataset.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage&quot; src=&quot;/blog/assets/img/posts/2020-11-08-DrRepair/self-supervised.png&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;h3 id=&quot;lets-use-our-program-repair-model&quot;&gt;&lt;strong&gt;Let’s use our program repair model!&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;We apply and evaluate our repair model (we call DrRepair) on two benchmark tasks:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Correcting C programs written by students (&lt;a href=&quot;https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14603&quot;&gt;DeepFix dataset&lt;/a&gt;&lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;)&lt;/li&gt;
  &lt;li&gt;Correcting the output of C++ program synthesis &lt;a href=&quot;https://arxiv.org/abs/1906.04908&quot;&gt;(SPoC dataset&lt;/a&gt;&lt;sup id=&quot;fnref:4&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Application to DeepFix (Correcting Student Programs)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In DeepFix, the task is to correct C programs written by students in an intro programming class so that they will compile. The input programs may have multiple lines with errors, so we apply the repair model iteratively, addressing one error at a time. For instance, the following figure shows an example program in DeepFix, which has a compiler error saying that “&lt;code class=&quot;highlighter-rouge&quot;&gt;i&lt;/code&gt; is undeclared”. By applying the repair model, DrRepair, it repairs this error by inserting a declaration of &lt;code class=&quot;highlighter-rouge&quot;&gt;i&lt;/code&gt; in line 5. After this fix, we notice that there is another error, which says “expected semicolon before brace”. We can apply the repair model again - this time, the model inserts a semicolon in line 12, and now the repaired program compiles successfully! This approach is conducive to the idea of iterative refinement: we can keep running the repair model and progressively fixing errors.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage&quot; src=&quot;/blog/assets/img/posts/2020-11-08-DrRepair/application_deepfix.png&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;What is the effect of using error messages, program-feedback graphs, and self-supervised pre-training?&lt;/strong&gt; Existing repair systems studied on DeepFix did not use compiler error messages - they aimed to directly translate from broken code to fixed code. To see the effect of using error messages in the first place, we tried removing all our techniques from the system: the use of compiler messages, program-feedback graphs, and pre-training. This version of our model (“ours: no compiler” in the figure below) achieves 34% repair accuracy on DeepFix, which is comparable to the existing systems. Now we add compiler messages to our input. We find that this model achieves much better performance and generalization (62.5% accuracy; “ours: base” in the figure). This suggests that with an access to error messages, the model learns the right inductive bias to repair the code based on the feedback. Next, we add program-feedback graphs and self-supervised pre-training. We find that both provide further improvements (“ours: base+graph” and “ours: base+graph+pretrain”), and our final system can fix 68.2% of the broken programs in DeepFix!&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-11-08-DrRepair/result_deepfix.png&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Application to SPoC (Natural Language to Code)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Program synthesis, in particular systems that can translate natural language descriptions (e.g. English) into code (e.g. Python, C++), are useful because they can help a wider range of people use programming languages. In SPoC (Pseudocode-to-Code), the task is to synthesize C++ implementation from pseudocode, a natural language description of a program. However, one challenge experienced by existing synthesizers (machine translation models applied to SPoC) is that they tend to output inconsistent code that does not compile - for instance, in the figure below, the variable &lt;code class=&quot;highlighter-rouge&quot;&gt;i&lt;/code&gt; is declared twice in the synthesized code. We find that we can apply our program repair model to this invalid code and fix it into a correct one, helping the program synthesis task. In the evaluation on SPoC, the use of our repair model improves the final synthesis success rate from the existing system’s 34% to 37.6%.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage&quot; src=&quot;/blog/assets/img/posts/2020-11-08-DrRepair/application_spoc.png&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;In this work, we studied how to use machine learning to repair programs from error messages, and developed three key insights:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Error messages provide a crucial signal for learning program repair.&lt;/li&gt;
  &lt;li&gt;Program-feedback graphs (joint representations of code &amp;amp; error messages) help model the reasoning of repair (e.g. tracking variables that caused the error).&lt;/li&gt;
  &lt;li&gt;Self-supervised learning allows us to turn freely-available, unlabeled programs (e.g. GitHub code) into useful training examples of program repair.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This work also provides a general framework of “learning from feedback”, which has various applications: editing documents based on comments, learning from users in interactive dialog, etc.&lt;/p&gt;

&lt;p&gt;You can check out our full paper (ICML 2020) &lt;a href=&quot;https://arxiv.org/pdf/2005.10636.pdf&quot;&gt;here&lt;/a&gt; and our source code/data on &lt;a href=&quot;https://github.com/michiyasunaga/DrRepair&quot;&gt;GitHub&lt;/a&gt;. You can also find the presentation slides on this work &lt;a href=&quot;https://cs.stanford.edu/~myasu/files/DrRepair_slides.pdf&quot;&gt;here&lt;/a&gt;. If you have questions, please feel free to email us!&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Michihiro Yasunaga: &lt;a href=&quot;mailto:myasu@cs.stanford.edu&quot;&gt;myasu@cs.stanford.edu&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;acknowledgments&quot;&gt;&lt;strong&gt;Acknowledgments&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Many thanks to Percy Liang, as well as members of the P-Lambda lab and the Stanford NLP group for their valuable feedback, and to Sidd Karamcheti and Andrey Kurenkov for edits on this blog post!&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42184.pdf&quot;&gt;Programmers’ Build Errors: A Case Study (at Google)&lt;/a&gt;. Hyunmin Seo, Caitlin Sadowski, Sebastian Elbaum, Edward Aftandilian, Robert Bowdidge. 2014 &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1710.10903&quot;&gt;Graph Attention Networks&lt;/a&gt;. Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, Yoshua Bengio. 2018. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14603&quot;&gt;DeepFix: Fixing common C language errors by deep learning&lt;/a&gt;. Rahul Gupta, Soham Pal, Aditya Kanade, Shirish Shevade. 2017. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1906.04908&quot;&gt;SPoC: Search-based Pseudocode to Code&lt;/a&gt;. Sumith Kulal, Panupong Pasupat, Kartik Chandra, Mina Lee, Oded Padon, Alex Aiken and Percy Liang. 2019. &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
              <pubDate>Sun, 08 Nov 2020 00:00:00 -0800</pubDate>
          </item>
          
        
          
          <item>
              <title>Adapting on the Fly to Test Time Distribution Shift</title>
              <link>/blog/adaptive-risk-minimization/</link>
              <guid isPermaLink="true">/blog/adaptive-risk-minimization/</guid>
              <description>&lt;p&gt;Imagine that you are building the next generation machine learning model for handwriting transcription. Based on previous iterations of your product, you have identified a key challenge for this rollout: after deployment, new end users often have different and unseen handwriting styles, leading to &lt;em&gt;distribution shift&lt;/em&gt;. One solution for this challenge is to learn an &lt;em&gt;adaptive&lt;/em&gt; model that can specialize and adjust to each user’s handwriting style over time. This solution seems promising, but it must be balanced against concerns about ease of use: requiring users to provide feedback to the model may be cumbersome and hinder adoption. Is it possible instead to learn a model that can adapt to new users &lt;em&gt;without labels&lt;/em&gt;?&lt;/p&gt;

&lt;p&gt;In many scenarios, including this example, the answer is “yes”. Consider the ambiguous example shown enlarged in the figure below. Is this character a “2” with a loop or a &lt;a href=&quot;https://en.wikipedia.org/wiki/A#English&quot;&gt;double-storey “a”&lt;/a&gt;? For a non adaptive model that pays attention to the biases in the training data, the reasonable prediction would be “2”. However, even without labels, we can extract useful information from the user’s other examples: an adaptive model, for example, can observe that this user has written “2”s without loops and conclude that this character is thus more likely to be “a”.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img width=&quot;100%&quot; src=&quot;/blog/assets/img/posts/2020-11-05-adaptive-risk-minimization/intro.gif&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Handling the distribution shift that arises from deploying a model to new users is an important motivating example for unlabeled adaptation. But, this is far from the only example. In an ever-changing world, autonomous cars need to adapt to new weather conditions and locations, image classifiers need to adapt to new cameras with different intrinsics, and recommender systems need to adapt to users’ evolving preferences. Humans have demonstrated the ability to &lt;a href=&quot;http://pages.cs.wisc.edu/~jerryzhu/pub/tie.pdf&quot;&gt;adapt without labels&lt;/a&gt; by inferring information from the distribution of test examples. Can we develop methods that can allow machine learning models to do the same?&lt;/p&gt;

&lt;p&gt;This question has enjoyed growing attention from researchers, with a number of recent works proposing methods for unlabeled test time adaptation. In this post, I will survey these works as well as other prominent frameworks for handling distribution shift. With this broader context in mind, I will then discuss our recent work (see the paper &lt;a href=&quot;https://arxiv.org/abs/2007.02931&quot;&gt;here&lt;/a&gt; and the code &lt;a href=&quot;https://github.com/henrikmarklund/arm&quot;&gt;here&lt;/a&gt;), in which we propose a problem formulation that we term &lt;strong&gt;adaptive risk minimization&lt;/strong&gt;, or ARM.&lt;/p&gt;

&lt;h2 id=&quot;diving-into-distribution-shift&quot;&gt;Diving into Distribution Shift&lt;/h2&gt;

&lt;p&gt;The vast majority of work in machine learning follows the canonical framework of &lt;strong&gt;empirical risk minimization&lt;/strong&gt;, or ERM. ERM methods assume that there is no distribution shift, so the test distribution exactly matches the training distribution. This assumption simplifies the development and analysis of powerful machine learning methods but, as discussed above, is routinely violated in real-world applications. To move beyond ERM and learn models that generalize in the face of distribution shift, we must introduce additional assumptions. However, we must carefully choose these assumptions such that they are still realistic and broadly applicable.&lt;/p&gt;

&lt;p&gt;How do we maintain realism and applicability? One answer is to model the assumptions on the conditions that machine learning systems face in the real world. For example, in the ERM setting, models are evaluated on each test point one at a time, but in the real world, these test points are often available sequentially or in &lt;em&gt;batches&lt;/em&gt;. For handwriting transcription, for example, we can imagine collecting entire sentences and paragraphs from new users. If there is distribution shift, observing multiple test points can be useful either to infer the test distribution or otherwise adapt the model to this new distribution, even in the absence of labels.&lt;/p&gt;

&lt;p&gt;Many recent methods that use this assumption can be classified as &lt;strong&gt;test time adaptation&lt;/strong&gt;, including &lt;a href=&quot;https://arxiv.org/abs/1603.04779&quot;&gt;batch normalization&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1802.03916&quot;&gt;label shift estimation&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1909.13231&quot;&gt;rotation prediction&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2006.10726&quot;&gt;entropy minimization&lt;/a&gt;, and more. Oftentimes, these methods build in strong inductive biases that enable useful adaptation; for example, rotation prediction is well aligned with many image classification tasks. But these methods generally either propose heuristic training procedures or do not consider the training procedure at all, relying instead on pretrained models.&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; This begs the question: can test time adaptation be further enhanced by improved training, such that the model can make better use of the adaptation procedure?&lt;/p&gt;

&lt;p&gt;We can gain insight into this question by investigating other prominent frameworks for handling distribution shift and, in particular, the assumptions these frameworks make. In real-world applications, the training data generally does not consist only of input label pairs; instead, there are additional &lt;em&gt;meta-data&lt;/em&gt; associated with each example, such as time and location, or the particular user in the handwriting example. These meta-data can be used to organize the training data into &lt;em&gt;groups&lt;/em&gt;,&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; and a common assumption in a number of frameworks is that the test time distribution shifts represent either new group distributions or new groups altogether. This assumption still allows for a wide range of realistic distribution shifts and has driven the development of numerous practical methods.&lt;/p&gt;

&lt;p&gt;For example, &lt;strong&gt;domain adaptation&lt;/strong&gt; methods typically assume access to two training groups: source and target data, with the latter being drawn from the test distribution. Thus, these methods augment training to focus on the target distribution, such as through &lt;a href=&quot;https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.370.4921&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;importance&lt;/a&gt; &lt;a href=&quot;http://sifaka.cs.uiuc.edu/czhai/pub/acl07.pdf&quot;&gt;weighting&lt;/a&gt; or learning &lt;a href=&quot;https://arxiv.org/abs/1505.07818&quot;&gt;invariant&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/1702.05464&quot;&gt;representations&lt;/a&gt;. Methods for &lt;a href=&quot;http://papers.neurips.cc/paper/3019-mixture-regression-for-covariate-shift.pdf&quot;&gt;&lt;strong&gt;group&lt;/strong&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/1611.02041&quot;&gt;&lt;strong&gt;distributionally robust&lt;/strong&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/1911.08731&quot;&gt;&lt;strong&gt;optimization&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&quot;https://papers.nips.cc/paper/4312-generalizing-from-several-related-classification-tasks-to-a-new-unlabeled-sample&quot;&gt;&lt;strong&gt;domain&lt;/strong&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2007.01434&quot;&gt;&lt;strong&gt;generalization&lt;/strong&gt;&lt;/a&gt; do not directly assume access to data from the test distribution, but instead use data drawn from multiple training groups in order to learn a model that generalizes at test time to new groups (or new group distributions). So, these prior works have largely focused on the training procedure and generally do not adapt at test time (despite the name “domain adaptation”).&lt;/p&gt;

&lt;h2 id=&quot;combining-training-and-test-assumptions&quot;&gt;Combining Training and Test Assumptions&lt;/h2&gt;

&lt;p&gt;Prior frameworks for distribution shift have assumed either training groups or test batches, but we are not aware of any prior work that uses both assumptions. In our work, we demonstrate that it is precisely this conjunction that allows us to &lt;em&gt;learn to adapt&lt;/em&gt; to test time distribution shift, by simulating both the shift and the adaptation procedure at training time. In this way, our framework can be understood as a &lt;strong&gt;meta-learning&lt;/strong&gt; framework, and we refer interested readers to this &lt;a href=&quot;https://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/&quot;&gt;blog post&lt;/a&gt; for a detailed overview of meta-learning.&lt;/p&gt;

&lt;h3 id=&quot;adaptive-risk-minimization&quot;&gt;Adaptive Risk Minimization&lt;/h3&gt;

&lt;p&gt;Our work proposes &lt;a href=&quot;https://arxiv.org/abs/2007.02931&quot;&gt;adaptive risk minimization&lt;/a&gt;, or ARM, which is a problem setting and objective that makes use of both groups at training time and batches at test time. This synthesis provides a general and principled answer, through the lens of meta-learning, to the question of how to train for test time adaptation. In particular, we &lt;em&gt;meta-train&lt;/em&gt; the model using simulated distribution shifts, which is enabled by the training groups, such that it exhibits strong &lt;em&gt;post-adaptation&lt;/em&gt; performance on each shift. The model therefore directly learns how to best leverage the adaptation procedure, which it then executes in the exact same way at test time. If we can identify which test distribution shifts are likely, such as seeing data from new end users, then we can better construct simulated training shifts, such as sampling data from only one particular training user.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img width=&quot;100%&quot; src=&quot;/blog/assets/img/posts/2020-11-05-adaptive-risk-minimization/arm.gif&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;The training procedure for optimizing the ARM objective is illustrated in the graphic above. From the training data, we sample different batches that simulate different group distribution shifts. An &lt;em&gt;adaptation model&lt;/em&gt; then has the opportunity to adapt the model parameters using the unlabeled examples. This allows us to meta-train the model for post-adaptation performance by directly performing gradient updates on both the model and the adaptation model.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img width=&quot;100%&quot; src=&quot;/blog/assets/img/posts/2020-11-05-adaptive-risk-minimization/methods.png&quot; /&gt;&lt;/p&gt;
&lt;figcaption&gt;
We draw inspiration from contextual meta-learning (left) and gradient based meta-learning (right) in order to devise methods for ARM. For contextual meta-learning, we investigate two different methods that fall under this category. These methods are described in detail in &lt;a href=&quot;https://arxiv.org/abs/2007.02931&quot;&gt;our paper&lt;/a&gt;.
&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;The connection to meta-learning is one key advantage of the ARM framework, as we are not starting from scratch when devising methods for solving ARM. In our work in particular, we draw inspiration from both &lt;a href=&quot;https://arxiv.org/abs/1807.01613&quot;&gt;contextual meta-learning&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/1703.03400&quot;&gt;gradient based meta-learning&lt;/a&gt; to develop three methods for solving ARM, which we name ARM-CML, ARM-BN, and ARM-LL. We omit the details of these methods here, but they are illustrated in the figure above and described in full in &lt;a href=&quot;https://arxiv.org/abs/2007.02931&quot;&gt;our paper&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The diversity of methods that we construct demonstrate the versatility and generality of the ARM problem formulation. But do we actually observe empirical gains using these methods? We investigate this question next.&lt;/p&gt;

&lt;h3 id=&quot;experiments&quot;&gt;Experiments&lt;/h3&gt;

&lt;p&gt;In our experiments, we first conducted a thorough study of the proposed ARM methods compared to various baselines, prior methods, and ablations, on four different image classification benchmarks exhibiting group distribution shift. &lt;a href=&quot;https://arxiv.org/abs/2007.02931&quot;&gt;Our paper&lt;/a&gt; provides full details on the benchmarks and comparisons.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img width=&quot;100%&quot; src=&quot;/blog/assets/img/posts/2020-11-05-adaptive-risk-minimization/results.png&quot; /&gt;&lt;/p&gt;
&lt;figcaption&gt;
We found that ARM methods empirically resulted in both better worst case (WC) and average (Avg) performance across groups compared to prior methods, indicating both better robustness and performance from the final trained models.
&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;In our main study, we found that ARM methods do better across the board both in terms of worst case and average test performance across groups, compared to a number of prior methods along with other baselines and ablations. The simplest method of ARM-BN, which can be implemented in just a few lines of additional code, often performed the best. This empirically shows the benefits of meta-learning, in that the model can be meta-trained to take greater advantage of the adaptation procedure.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img width=&quot;75%&quot; src=&quot;/blog/assets/img/posts/2020-11-05-adaptive-risk-minimization/femnist.gif&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;We also conducted some qualitative analyses, in which we investigated a test situation similar to the motivating example described at the beginning with a user that wrote double-storey a’s. We empirically found that models trained with ARM methods did in fact successfully adapt and predict “a” in this situation, when given enough examples of the user’s handwriting that included other “a”s and “2”s. Thus, this confirms our original hypothesis that training adaptive models is an effective way to deal with distribution shift.&lt;/p&gt;

&lt;p&gt;We believe that the motivating example from the beginning as well as the empirical results in our paper convincingly argue for further study into general techniques for &lt;em&gt;adaptive models&lt;/em&gt;. We have presented a general scheme for meta-training these models to better harness their adaptation capabilities, but a number of open questions remain, such as devising better adaptation procedures themselves. This broad research direction will be crucial for machine learning models to truly realize their potential in complex, real-world environments.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Thanks to Chelsea Finn and Sergey Levine for providing valuable feedback on this post.
This blog post also appeared on the &lt;a href=&quot;https://bair.berkeley.edu/blog/2020/11/05/arm/&quot;&gt;Berkeley AI Research Blog&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Part of this post is based on the following paper:&lt;/p&gt;

&lt;p&gt;Marvin Zhang*, Henrik Marklund*, Nikita Dhawan*, Abhishek Gupta, Sergey Levine, Chelsea Finn.
&lt;a href=&quot;https://arxiv.org/abs/2007.02931&quot;&gt;&lt;strong&gt;Adaptive Risk Minimization: A Meta-Learning Approach for Tackling Group Shift.&lt;/strong&gt;&lt;/a&gt;
&lt;a href=&quot;https://sites.google.com/view/adaptive-risk-minimization&quot;&gt;Project webpage&lt;/a&gt;
&lt;a href=&quot;https://github.com/henrikmarklund/arm&quot;&gt;Open source code&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;On the flip side, applicability to even pretrained models can be seen as a strength of these methods. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;Alternatively referred to as domains, subpopulations, tasks, users, and more. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
              <pubDate>Thu, 05 Nov 2020 00:00:00 -0800</pubDate>
          </item>
          
        
          
          <item>
              <title>The Coming Wave of ML Systems</title>
              <link>/blog/mlsys/</link>
              <guid isPermaLink="true">/blog/mlsys/</guid>
              <description>&lt;p&gt;AI and ML products now permeate every aspect of our digital lives–from recommendations of what to watch, to divining our search intent, to powering increasingly-present virtual assistants in consumer and enterprise settings. While quality improvements are the main focus of traditional ML and AI research, a second and arguably less well understood benefit of machine learning is that it can dramatically reshape the practice of building applications. With an eye toward generations of compiler, database, and operating systems work, they may inspire new foundational questions for how to build the next generation of AI-powered systems.&lt;/p&gt;

&lt;p&gt;Tools are important. They are the scaffolding of the machine learning revolution: the widespread adoption of tools like PyTorch and TensorFlow (building on earlier academic prototypes like Theano and Torch) enabled users to more easily assemble models due to both well-suited domain-specific languages and a rich collection of building blocks. Supported by large companies, these tools have spawned a rich ecosystem to which new building blocks are contributed almost daily and which even contains tools for deployment (eg TFX and TorchScript). Moving from the era of bespoke AI tools to a shared communal foundation has seen stunning productivity gains–on a personal note, it was wild to live through and modestly contribute to.&lt;/p&gt;

&lt;p&gt;With the stunning success of these platforms, these libraries have moved the pain point for engineers who build and maintain these products. To understand what might be next, perhaps we can take a page from computing history? One view is that the current generation of tools are akin to software libraries, but they lack some of the features that distinguish long-lived computing systems, such as:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;monitoring and lifecycle management (most ML systems only deal with training monitoring),&lt;/li&gt;
  &lt;li&gt;support collaboration of all stakeholders around the life of the product (most ML systems lack a model management solution),&lt;/li&gt;
  &lt;li&gt;end-to-end data flow debugging and monitoring (most ML systems don’t manage training data production pipelines)&lt;/li&gt;
  &lt;li&gt;… and many more …
Understanding this thought has been a driving force behind our recent work.
We presented some of our initial ideas in the &lt;a href=&quot;https://www.youtube.com/watch?v=CR1g2-ZqswE&quot;&gt;MLSys keynote&lt;/a&gt; and described some of our thoughts for production and research systems.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;While we contend entirely new ways of building these systems are possible, we are at the start of this journey. There is preliminary evidence that there’s something here: these new breed of systems have found their way into industry products used by billions of people every day like Google
[&lt;a href=&quot;https://ai.googleblog.com/2019/03/harnessing-organizational-knowledge-for.html&quot;&gt;data programming&lt;/a&gt;,
&lt;a href=&quot;https://research.google/pubs/pub48846/&quot;&gt;information extraction&lt;/a&gt;],
YouTube [&lt;a href=&quot;https://arxiv.org/abs/2008.09983&quot;&gt;multi-modal&lt;/a&gt;],
multiple Apple products [&lt;a href=&quot;https://arxiv.org/abs/1909.05372&quot;&gt;Overton&lt;/a&gt;],
Uber [&lt;a href=&quot;https://eng.uber.com/cota/&quot;&gt;customer support&lt;/a&gt;,
&lt;a href=&quot;https://eng.uber.com/uber-eats-graph-learning/&quot;&gt;food recommendation&lt;/a&gt;,
&lt;a href=&quot;https://eng.uber.com/introducing-ludwig/&quot;&gt;Ludwig open-sourced&lt;/a&gt;], and many more.&lt;/p&gt;

&lt;p&gt;The goal of this post is to introduce the Stanford MLSys Seminar Series to hopefully engage more of the community around ideas to build these systems. If you’re interested in this area or you have a topic you’d like to see, let us know!
Please visit the webpage at &lt;a href=&quot;http://mlsys.stanford.edu&quot;&gt;mlsys.stanford.edu&lt;/a&gt; to see our preliminary thoughts and the schedule of our first speakers. We welcome your feedback!&lt;/p&gt;

&lt;p&gt;One outcome of the course is to articulate the challenges that we’ve seen, solicit challenges from the community, and try to make the field more accessible for academic research. If we’re lucky, we may just help to spawn the next major subfield of computer science!&lt;/p&gt;

</description>
              <pubDate>Tue, 13 Oct 2020 00:00:00 -0700</pubDate>
          </item>
          
        
          
          <item>
              <title>GTI: Learning to Generalize Across Long-Horizon Tasks from Human Demonstrations</title>
              <link>/blog/gti/</link>
              <guid isPermaLink="true">/blog/gti/</guid>
              <description>&lt;p&gt;It takes a lot of data for robots to autonomously learn to perform simple manipulation tasks as as grasping and pushing. For example, prior work&lt;sup id=&quot;fnref:qtopt&quot;&gt;&lt;a href=&quot;#fn:qtopt&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&quot;fnref:dm_reward_sketch&quot;&gt;&lt;a href=&quot;#fn:dm_reward_sketch&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; has leveraged Deep Reinforcement Learning to train robots to grasp and stack various objects. These tasks are usually short and relatively simple - for example, picking up a plastic bottle in a tray. However, because reinforcement learning relies on gaining experiences through trial-and-error, hundreds of robot hours were required for the robot to learn to picking up objects reliably.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; class=&quot;postimagehalf&quot;&gt;
  &lt;source src=&quot;/blog/assets/img/posts/2020-10-07-gti/qt_opt.mp4&quot; type=&quot;video/mp4&quot; /&gt;
&lt;/video&gt;
&lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; class=&quot;postimagehalf&quot;&gt;
  &lt;source src=&quot;/blog/assets/img/posts/2020-10-07-gti/dm_reward_sketch.mp4&quot; type=&quot;video/mp4&quot; /&gt;
&lt;/video&gt;

&lt;figcaption&gt;
It takes 100s of hours for robots to autonomously learn to perform manipulation tasks- even for grasping, or stacking, which are short-horizon tasks.
&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;On the other hand, imitation learning can learn robot control policies directly from expert demonstrations without trial-and-error and thus require far less data than reinforcement learning. In prior work, a handful of human demonstrations have been used to train a robot to perform different skills such as pushing an object to a target location from only image input &lt;sup id=&quot;fnref:deep_imitation&quot;&gt;&lt;a href=&quot;#fn:deep_imitation&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/blog/assets/img/posts/2020-10-07-gti/deep_imitation_1.png&quot; class=&quot;postimagehalf&quot; /&gt;
&lt;img src=&quot;/blog/assets/img/posts/2020-10-07-gti/deep_imitation_2.png&quot; class=&quot;postimagehalf&quot; /&gt;&lt;/p&gt;
&lt;figcaption&gt;
Imitation Learning has been used to directly learn short-horizon skills from 100-300 demonstrations.
&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;However, because the control policies are only trained with a fixed set of task demonstrations, it is difficult for the policies to generalize outside of the training data. In this work, we present a method for learning to solve new tasks by piecing together parts of training tasks that the robot has already seen in the demonstration data.&lt;/p&gt;

&lt;h2 id=&quot;a-motivating-example&quot;&gt;A Motivating Example&lt;/h2&gt;

&lt;p&gt;Consider the setup shown below. In the first task, the bread starts in the container, and the robot needs to remove the purple lid, retrieve the bread, put it into this green bowl, and then serve it on a plate.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/blog/assets/img/posts/2020-10-07-gti/setup_a_start.png&quot; class=&quot;postimagehalf&quot; /&gt;
&lt;img src=&quot;/blog/assets/img/posts/2020-10-07-gti/setup_a_goal.png&quot; class=&quot;postimagehalf&quot; /&gt;&lt;/p&gt;
&lt;figcaption&gt;
In the first task, the robot needs to retrieve the bread from the covered container and serve it on a plate.
&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;In the second task, the bread starts on the table, and it needs to be placed in the green bowl and then put into the oven for baking.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/blog/assets/img/posts/2020-10-07-gti/setup_b_start.png&quot; class=&quot;postimagehalf&quot; /&gt;
&lt;img src=&quot;/blog/assets/img/posts/2020-10-07-gti/setup_b_goal.png&quot; class=&quot;postimagehalf&quot; /&gt;&lt;/p&gt;
&lt;figcaption&gt;
In the second task, the robot needs to pick the bread off the table and place it into the oven for baking.
&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;We provide the robot with demonstrations of both tasks. Note that both tasks require the robot to place the bread into this green bowl! In other words, these task trajectories intersect in the state space! The robot should be able to generalize to new start and goal pairs by choosing different paths at the intersection, as shown in the picture. For example, the robot could retrieve the bread from the container and place the bread into the oven, instead of placing it on the plate.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/blog/assets/img/posts/2020-10-07-gti/cross.png&quot; class=&quot;postimagehalf&quot; /&gt;&lt;/p&gt;
&lt;figcaption&gt;
The task demonstrations for both tasks will intersect in the state space since both tasks require the robot to place the bread into the green bowl. By leveraging this task intersection and composing pieces of different demonstrations together, the robot will be able to generalize to new start and goal pairs.
&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;
In summary, our &lt;strong&gt;key insights&lt;/strong&gt; are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Multi-task domains often contain task intersections.
&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;It should be possible for a policy to generate new task trajectories by composing training tasks via the intersections.
&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;generalization-through-imitation&quot;&gt;Generalization Through Imitation&lt;/h2&gt;

&lt;p&gt;In this work, we introduce &lt;strong&gt;Generalization Through Imitation (GTI)&lt;/strong&gt;, a two-stage algorithm for enabling robots to generalize to new start and goal pairs through compositional imitation.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Stage 1:&lt;/strong&gt; Train policies to generate diverse (potentially new) rollouts from human demonstrations. 
&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Stage 2:&lt;/strong&gt; Use these rollouts to train goal-directed policies to achieve targeted new behaviors by self-imitation.
&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;generating-diverse-rollouts-from-human-demonstrations&quot;&gt;Generating Diverse Rollouts from Human Demonstrations&lt;/h3&gt;

&lt;p&gt;In Stage 1, we would like to train policies that are able to both reproduce the task trajectories in the data and also generate new task trajectories consisting of unseen start and goal pairs. This can be challenging - we need to encourage our trained policy to understand how to stop following one trajectory from the dataset and start following a different one in order to end up in a different goal state.&lt;/p&gt;

&lt;p&gt;Here, we list two core technical challenges.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Mode Collapse.&lt;/strong&gt; If we naively train imitation learning policies on the demonstration data of the two tasks,  the policy tends to only go to a particular goal regardless of the initial states, as indicated by the red arrows in the picture below.
&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Spatio-temporal Variation&lt;/strong&gt; There is a large amount of spatio-temporal variation from human demonstrations on a real robot that must be modeled and accounted for.
&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/blog/assets/img/posts/2020-10-07-gti/diverse_rollouts_1.png&quot; class=&quot;postimagehalf&quot; /&gt;
&lt;img src=&quot;/blog/assets/img/posts/2020-10-07-gti/diverse_rollouts_2.png&quot; class=&quot;postimagehalf&quot; /&gt;&lt;/p&gt;
&lt;figcaption&gt;
  Generating diverse rollouts from a fixed set of human demonstrations is difficult due to the potential for mode collapse (left) and because the policy must also model spatio-temporal variations in the data (right).
&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;In order to get a better idea of how to encourage a policy to generate diverse rollouts, let’s take a closer look at the task space. The left image in the figure below shows the set of demonstrations. Consider a state near the beginning of a demonstration, as shown in the middle image. If we start in this state, and try to set a goal for our policy to achieve, according to the demonstration data, the goals can be modeled by a gaussian distribution. However, if we start at the intersection, the goal could spread across two tasks. It would be better for us to model the goal distributions with a multi-modal gaussian.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/blog/assets/img/posts/2020-10-07-gti/diverse_rollouts_3.png&quot; class=&quot;postimagethird&quot; /&gt;
&lt;img src=&quot;/blog/assets/img/posts/2020-10-07-gti/diverse_rollouts_4.png&quot; class=&quot;postimagethird&quot; /&gt;
&lt;img src=&quot;/blog/assets/img/posts/2020-10-07-gti/diverse_rollouts_5.png&quot; class=&quot;postimagethird&quot; /&gt;&lt;/p&gt;
&lt;figcaption&gt;
  Task intersections are better modeled with mixtures of gaussians in order to capture the different possible future states.
&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Based on this observation, we design a hierarchical policy learning algorithm, where the high-level policy captures distribution of future observations in a multimodal latent space. The low-level policy conditions on the latent goal to fully explore the space of demonstrations.&lt;/p&gt;

&lt;h3 id=&quot;gti-algorithm-details&quot;&gt;GTI Algorithm Details&lt;/h3&gt;

&lt;p&gt;Let’s take a closer look at the learning architecture for our Stage 1 policy, shown below. The high-level planner is a conditional variational autoencoder&lt;sup id=&quot;fnref:VAE&quot;&gt;&lt;a href=&quot;#fn:VAE&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;, that attempts to learn the distribution of future image observations conditioned on current image observations. The encoder encodes both a current and future observation into a latent space. The decoder attempts to reconstruct the future observation from the latent. The latent space is regularized with a learned Gaussian mixture model prior. This prior encourages the model to a latent multimodal distribution of future observations. We can think of this latent space as modeling short-horizon subgoals. We train our low-level controller to imitate actions in the dataset that lead to particular subgoals.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/blog/assets/img/posts/2020-10-07-gti/stage1_1.png&quot; class=&quot;postimagesmaller&quot; /&gt;&lt;/p&gt;
&lt;figcaption&gt;
  The diagram above depicts the Stage 1 training architecture.
&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Next, we use the Stage 1 policy to collect a handful of self-generated diverse rollouts, shown below. Every 10 timesteps, we sample a new latent subgoal from the GMM prior, and use it to condition the low-level policy. The diversity captured in the GMM prior ensures that the Stage 1 policy will exhibit different behaviors at trajectory intersections, resulting in novel trajectories with unseen start and goal pairs.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/blog/assets/img/posts/2020-10-07-gti/stage1_2.png&quot; class=&quot;postimagesmaller&quot; /&gt;&lt;/p&gt;
&lt;figcaption&gt;
  The Stage 1 trained policy is used to generate a self-supervised dataset that covers the space of start and goal states by composing seen behaviors together.
&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Finally, the self-generated dataset is used to train a new, goal-directed policy that can perform intentional behaviors from these undirected rollouts.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/blog/assets/img/posts/2020-10-07-gti/stage2.png&quot; class=&quot;postimagesmaller&quot; /&gt;&lt;/p&gt;
&lt;figcaption&gt;
  Stage 2 policy learning is just goal-conditioned behavioral cloning from the Stage 1 dataset, where the goals are final image observations from the trajectories collected in Stage 1.
&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;h2 id=&quot;real-robot-experiments&quot;&gt;Real Robot Experiments&lt;/h2&gt;

&lt;h3 id=&quot;data-collection&quot;&gt;Data Collection&lt;/h3&gt;

&lt;p&gt;This is our hardware setup. We used a Franka robotic arm and two cameras for data collection - a front view camera and a wrist-mounted camera.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/blog/assets/img/posts/2020-10-07-gti/hardware_setup.png&quot; class=&quot;postimagehalf&quot; /&gt;&lt;/p&gt;
&lt;figcaption&gt;
  Hardware setup used in our work.
&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;We used the RoboTurk phone teleoperation interface&lt;sup id=&quot;fnref:RoboTurk_v1&quot;&gt;&lt;a href=&quot;#fn:RoboTurk_v1&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&quot;fnref:RoboTurk_v2&quot;&gt;&lt;a href=&quot;#fn:RoboTurk_v2&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt; to collect human demonstrations. We collect only 50 demonstrations for each of the two tasks. The data collection took less than an hour.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; class=&quot;postimagesmaller&quot;&gt;
  &lt;source src=&quot;/blog/assets/img/posts/2020-10-07-gti/demo_video.mp4&quot; type=&quot;video/mp4&quot; /&gt;
&lt;/video&gt;

&lt;figcaption&gt;
  We collected demonstrations using the RoboTurk phone teleoperation interface.
&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;h3 id=&quot;results&quot;&gt;Results&lt;/h3&gt;

&lt;p&gt;Below, we show the final trained Stage 2 model. We ask the robot to start from the initial state of one task, bread-in-container, and reach the goal of the other task, which is to put the bread in the oven. The goal is specified by providing an image observation that shows the bread in the oven. We emphasize that the policy is performing closed-loop visuomotor control at 20hz purely from image observations. Note that this task requires accurate contact-rich manipulations, and is long-horizon. With only visual information, our method can perform intricate tasks such as grasping, pushing the oven tray into the oven, or manipulating a constrained mechanism like closing door of the oven.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; class=&quot;postimagesmaller&quot;&gt;
  &lt;source src=&quot;/blog/assets/img/posts/2020-10-07-gti/final_result_video.mp4&quot; type=&quot;video/mp4&quot; /&gt;
&lt;/video&gt;

&lt;figcaption&gt;
  GTI is able to produce a goal-conditioned policy that solves both tasks seen in the demonstrations and tasks that were not seen.
&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Our Stage 1 policy can recover all start and goal combinations, including both behavior seen in training and new unseen behaviors.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; class=&quot;postimagehalf&quot;&gt;
  &lt;source src=&quot;/blog/assets/img/posts/2020-10-07-gti/seen_container_plate.mp4&quot; type=&quot;video/mp4&quot; /&gt;
&lt;/video&gt;
&lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; class=&quot;postimagehalf&quot;&gt;
  &lt;source src=&quot;/blog/assets/img/posts/2020-10-07-gti/seen_table_oven.mp4&quot; type=&quot;video/mp4&quot; /&gt;
&lt;/video&gt;

&lt;figcaption&gt;
  The GTI Stage 1 policy can imitate the demonstrations to solve the tasks seen in the demonstrations.
&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; class=&quot;postimagehalf&quot;&gt;
  &lt;source src=&quot;/blog/assets/img/posts/2020-10-07-gti/unseen_table_plate.mp4&quot; type=&quot;video/mp4&quot; /&gt;
&lt;/video&gt;
&lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; class=&quot;postimagehalf&quot;&gt;
  &lt;source src=&quot;/blog/assets/img/posts/2020-10-07-gti/unseen_container_oven.mp4&quot; type=&quot;video/mp4&quot; /&gt;
&lt;/video&gt;

&lt;figcaption&gt;
  The GTI Stage 1 policy can compose different parts of the demonstrations together to produce novel behavior and solve unseen tasks as well.
&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Finally, we show that our method is robust towards unexpected situations. In the case below (left), the policy is stuck because of conflicting supervisions. Sampling latent goals allows the policy to get unstuck and complete the task successfully. Our policy is also very reactive and can quickly recover from errors. In the case below (right), the policy failed to grasp the bread twice, and finally succeeded the third time. It also made two attempts to get a good grasp of the bowl, and complete the task successfully&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; class=&quot;postimagehalf&quot;&gt;
  &lt;source src=&quot;/blog/assets/img/posts/2020-10-07-gti/unstuck.mp4&quot; type=&quot;video/mp4&quot; /&gt;
&lt;/video&gt;
&lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; class=&quot;postimagehalf&quot;&gt;
  &lt;source src=&quot;/blog/assets/img/posts/2020-10-07-gti/reactive.mp4&quot; type=&quot;video/mp4&quot; /&gt;
&lt;/video&gt;

&lt;figcaption&gt;
  Robustness results. The policy is able to deal with conflicting supervision and get unstuck by sampling latent goals (left). The policy is reactive and can quickly recover from errors (right).
&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Imitation learning is an effective and safe technique to train robot policies in the real world because it does not depend on an expensive random exploration process. However, due to the lack of exploration, learning policies that generalize beyond the demonstrated behaviors is still an open challenge.
&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;Our key insight is that multi-task domains often present a latent structure, where demonstrated trajectories for different tasks intersect at common regions of the state space.
&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;We present Generalization Through Imitation (GTI), a two-stage offline imitation learning algorithm that exploits this intersecting structure to train goal-directed policies that generalize to unseen start and goal state combinations.&lt;/li&gt;
  &lt;li&gt;We validate GTI on a real robot kitchen domain and showcase the capacity of trained policies to solve both seen and unseen task configurations.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;This blog post is based on the following paper:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2003.06085&quot;&gt;“Learning to Generalize Across Long-Horizon Tasks from Human Demonstrations”&lt;/a&gt; by Ajay Mandlekar*, Danfei Xu*, Roberto Martin-Martin, Silvio Savarese, and Li Fei-Fei.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:qtopt&quot;&gt;
      &lt;p&gt;Quillen, D., Jang, E., Nachum, O., Finn, C., Ibarz, J., &amp;amp; Levine, S. (2018, May). Deep reinforcement learning for vision-based robotic grasping: A simulated comparative evaluation of off-policy methods. In 2018 IEEE International Conference on Robotics and Automation (ICRA) (pp. 6284-6291). IEEE. &lt;a href=&quot;#fnref:qtopt&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:dm_reward_sketch&quot;&gt;
      &lt;p&gt;Cabi, S., Colmenarejo, S. G., Novikov, A., Konyushkova, K., Reed, S., Jeong, R., … &amp;amp; Sushkov, O. (2019). A Framework for Data-Driven Robotics. arXiv preprint arXiv:1909.12200. &lt;a href=&quot;#fnref:dm_reward_sketch&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:deep_imitation&quot;&gt;
      &lt;p&gt;Zhang, T., McCarthy, Z., Jow, O., Lee, D., Chen, X., Goldberg, K., &amp;amp; Abbeel, P. (2018, May). Deep imitation learning for complex manipulation tasks from virtual reality teleoperation. In 2018 IEEE International Conference on Robotics and Automation (ICRA) (pp. 1-8). IEEE. &lt;a href=&quot;#fnref:deep_imitation&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:VAE&quot;&gt;
      &lt;p&gt;Kingma, D. P., &amp;amp; Welling, M. (2013). Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114. &lt;a href=&quot;#fnref:VAE&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:RoboTurk_v1&quot;&gt;
      &lt;p&gt;Mandlekar, A., Zhu, Y., Garg, A., Booher, J., Spero, M., Tung, A., … &amp;amp; Savarese, S. (2018). Roboturk: A crowdsourcing platform for robotic skill learning through imitation. arXiv preprint arXiv:1811.02790. &lt;a href=&quot;#fnref:RoboTurk_v1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:RoboTurk_v2&quot;&gt;
      &lt;p&gt;Mandlekar, A., Booher, J., Spero, M., Tung, A., Gupta, A., Zhu, Y., … &amp;amp; Fei-Fei, L. (2019). Scaling Robot Supervision to Hundreds of Hours with RoboTurk: Robotic Manipulation Dataset through Human Reasoning and Dexterity. arXiv preprint arXiv:1911.04052. &lt;a href=&quot;#fnref:RoboTurk_v2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
              <pubDate>Wed, 07 Oct 2020 00:00:00 -0700</pubDate>
          </item>
          
        
          
          <item>
              <title>How to Fill in the Blanks with Language Models</title>
              <link>/blog/infilling-by-language-modeling/</link>
              <guid isPermaLink="true">/blog/infilling-by-language-modeling/</guid>
              <description>&lt;p&gt;When editing or revising we often write in a &lt;em&gt;non-linear&lt;/em&gt; manner.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Writing an email&lt;/strong&gt;&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/blog/assets/img/posts/2020-09-10-infilling-by-language-modeling/email.png&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;An existing system might suggest something like “great to me” because it only considers the preceding text but not the subsequent text.&lt;/p&gt;

&lt;p&gt;A better suggestion in this case would be something like “good with one exception” since the writer is not completely satisfied and suggesting a further revision.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Writing a novel&lt;/strong&gt;&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/blog/assets/img/posts/2020-09-10-infilling-by-language-modeling/novel.png&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;When you don’t have a concrete idea on how to connect two scenes, the system can suggest a way to connect the fragmented ideas.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;task&quot;&gt;&lt;strong&gt;Task&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Fill in the blanks?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Consider the following sentence with blanks:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;She ate ____ for ____&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;To fill in the blanks, one needs to consider both preceding and subsequent text (in this case, &lt;em&gt;“She ate”&lt;/em&gt; and &lt;em&gt;“for”&lt;/em&gt;). There can be many reasonable ways to fill in the blanks:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;She ate &lt;strong&gt;leftover pasta&lt;/strong&gt; for &lt;strong&gt;lunch&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;

  &lt;p&gt;&lt;em&gt;She ate &lt;strong&gt;chocolate ice cream&lt;/strong&gt; for &lt;strong&gt;dessert&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;

  &lt;p&gt;&lt;em&gt;She ate &lt;strong&gt;toast for breakfast before leaving&lt;/strong&gt; for &lt;strong&gt;school&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;

  &lt;p&gt;&lt;em&gt;She ate &lt;strong&gt;rather quickly&lt;/strong&gt; for &lt;strong&gt;she was in a hurry that evening&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The task of filling in the blanks is known as &lt;em&gt;text infilling&lt;/em&gt; in the field of Natural Language Processing (NLP). It is the task of predicting blanks (or missing spans) of text at any position in text.&lt;/p&gt;

&lt;p&gt;The general definition of text infilling considers text with an arbitrary number of blanks where each blank can represent one of more missing words.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/blog/assets/img/posts/2020-09-10-infilling-by-language-modeling/task-simple.png&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Language models?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Language modeling is a special case of text infilling where only the preceding text is present and there is only one blank at the end.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;She ate leftover pasta for ____&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In recent few years, a number of large-scale language models are introduced and shown to achieve human-like performance. These models are often pre-trained on massive amount of unlabeled data, requiring huge amount of computation and resource.&lt;/p&gt;

&lt;p&gt;Our goal is to take these existing language models and make them perform the more general task of filling in the blanks.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;approach&quot;&gt;&lt;strong&gt;Approach&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;How can we make a language model fill in the blanks?&lt;/p&gt;

&lt;p&gt;Our approach is infilling by language modeling. With this approach, one can simply (1) download an existing pre-trained language model and (2) enable it to fill in any number and length of blanks in text by fine-tuning it on artificially generated examples.&lt;/p&gt;

&lt;p&gt;Main advantages of our framework are as follows:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Conceptual simplicity&lt;/strong&gt;: Minimal change to standard language model training&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Model-agnostic framework&lt;/strong&gt;: Leverage massively pre-trained language models&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Now, let’s see what happens at training and test time!&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;training-time&quot;&gt;Training time&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;1.  Manufacture infilling examples&lt;/strong&gt;&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/blog/assets/img/posts/2020-09-10-infilling-by-language-modeling/training.png&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Suppose we have plain text as our data:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Data: &lt;em&gt;She ate leftover pasta for lunch.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;To produce an infilling example for given data, first generate input by randomly replacing some tokens in the data with &lt;strong&gt;[blank]&lt;/strong&gt; tokens.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Input: &lt;em&gt;She ate &lt;strong&gt;[blank]&lt;/strong&gt; for &lt;strong&gt;[blank]&lt;/strong&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Then, generate a target by concatenating the replaced tokens, separated by the &lt;strong&gt;[answer]&lt;/strong&gt; token.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Target: &lt;em&gt;leftover pasta &lt;strong&gt;[answer]&lt;/strong&gt; lunch &lt;strong&gt;[answer]&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Finally, construct the complete infilling example by concatenating input, a special separator token &lt;strong&gt;[sep]&lt;/strong&gt;, and target.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Infilling example: &lt;em&gt;She ate &lt;strong&gt;[blank]&lt;/strong&gt; for &lt;strong&gt;[blank]&lt;/strong&gt;. &lt;strong&gt;[sep]&lt;/strong&gt; leftover pasta &lt;strong&gt;[answer]&lt;/strong&gt; lunch &lt;strong&gt;[answer]&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Looking for a script to automate this step? It is available &lt;a href=&quot;https://github.com/chrisdonahue/ilm&quot;&gt;here&lt;/a&gt;!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2. Download your favorite language model&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For instance, OpenAI &lt;a href=&quot;https://huggingface.co/transformers/quickstart.html&quot;&gt;GPT-2&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3. Fine-tune the model on infilling examples&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Now, let’s fine-tune the model on the infilling examples using standard language model training methodology.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;test-time&quot;&gt;Test time&lt;/h3&gt;

&lt;p&gt;Once trained, we can use the language model to infill at test time.&lt;/p&gt;

&lt;p&gt;As input, the model takes incomplete text with &lt;strong&gt;[blank]&lt;/strong&gt; and generates a target.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Input: &lt;em&gt;He drinks &lt;strong&gt;[blank]&lt;/strong&gt; after &lt;strong&gt;[blank]&lt;/strong&gt;.&lt;/em&gt;&lt;/p&gt;

  &lt;p&gt;Target: &lt;em&gt;water &lt;strong&gt;[answer]&lt;/strong&gt; running &lt;strong&gt;[answer]&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;You can then construct the complete text by simply replacing &lt;strong&gt;[blank]&lt;/strong&gt; tokens in the input with predicted answers in the target in a deterministic fashion.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Output: &lt;em&gt;He drinks water after running.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;practical-advantages&quot;&gt;Practical advantages&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Our framework incurs &lt;strong&gt;almost no computational overhead&lt;/strong&gt; compared to language modeling. This is particularly good when considering models like GPT-2 whose memory usage grows quadratically with sequence length.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Our framework requires &lt;strong&gt;minimal change to the vocabulary&lt;/strong&gt; of existing language models. Specifically, you need three additional tokens: &lt;strong&gt;[blank]&lt;/strong&gt;, &lt;strong&gt;[answer]&lt;/strong&gt;, and &lt;strong&gt;[sep]&lt;/strong&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Our framework offers the &lt;strong&gt;ability to attend to the entire context&lt;/strong&gt; on both sides of a blank with the simplicity of decoding from language models.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;evaluation&quot;&gt;&lt;strong&gt;Evaluation&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&quot;turing-test&quot;&gt;Turing test&lt;/h3&gt;

&lt;p&gt;The following is a short story consisting of five sentences. One of the sentences is swapped with a sentence generated by our model. Can you find it?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q. Identify one of the five sentences generated by machine.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;[1] Patty was excited about having her friends over.
[2] She had been working hard preparing the food.
[3] Patty knew her friends wanted pizza.
[4] All of her friends arrived and were seated at the table.
[5] Patty had a great time with her friends.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q. Identify one of the five sentences generated by machine.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;[1] Yesterday was Kelly’s first concert.
[2] She was nervous to get on stage.
[3] When she got on stage the band was amazing.
[4] Kelly was then happy.
[5] She couldn’t wait to do it again.&lt;/p&gt;

&lt;p&gt;(Answers are at the end of the post.)&lt;/p&gt;

&lt;p&gt;In our experiments, we sampled a short story from ROCstories (Mostafazadeh et al., 2016), randomly replaced one of the sentences with a &lt;strong&gt;[blank]&lt;/strong&gt; token, and infilled with a sentence generated by a model. Then, we asked 100 people to identify which of the sentences in a story was machine-generated.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;strong&gt;System&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;How many people were fooled?&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;Generated sentence&lt;/strong&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;BERT (Devlin et al., 2019)&lt;/td&gt;
      &lt;td&gt;20%&lt;/td&gt;
      &lt;td&gt;favoritea “, Mary brightly said.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Self-Attention Model (Zhu et al., 2019)&lt;/td&gt;
      &lt;td&gt;29%&lt;/td&gt;
      &lt;td&gt;She wasn’t sure she had to go to the store.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Standard Language Model (Radford et al., 2019)&lt;/td&gt;
      &lt;td&gt;41%&lt;/td&gt;
      &lt;td&gt;She went to check the tv.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Infilling by Language Model (Ours)&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;45%&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;Patty knew her friends wanted pizza.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Human&lt;/td&gt;
      &lt;td&gt;78%&lt;/td&gt;
      &lt;td&gt;She also had the place looking spotless.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The results show that people have difficulty identifying sentences infilled by our model as machine-generated 45% of the time. Generated sentences in the table are the system outputs for sentence [3] in the first story of the Turing test.&lt;/p&gt;

&lt;p&gt;More experiments and analysis can be found in our &lt;a href=&quot;https://arxiv.org/abs/2005.05339&quot;&gt;paper&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;try-it-out&quot;&gt;&lt;strong&gt;Try it out!&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;We have a &lt;a href=&quot;https://chrisdonahue.com/ilm&quot;&gt;demo&lt;/a&gt; where you can explore the infilling functionality for
multiple variable-length spans and different granularities (e.g. words, phrases, and sentences)
on the domains of short stories, scientific abstracts, and song lyrics!&lt;/p&gt;

&lt;p&gt;You can check out our &lt;a href=&quot;https://arxiv.org/abs/2005.05339&quot;&gt;paper on arXiv&lt;/a&gt; and our &lt;a href=&quot;https://github.com/chrisdonahue/ilm&quot;&gt;source code on GitHub&lt;/a&gt;.
You can also find a short talk on this work &lt;a href=&quot;https://slideslive.com/38929175/enabling-language-models-to-fill-in-the-blanks&quot;&gt;here&lt;/a&gt;. If you have questions, please feel free to email us!&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Chris Donahue &lt;a href=&quot;mailto:cdonahue@cs.stanford.edu&quot;&gt;cdonahue@cs.stanford.edu&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Mina Lee &lt;a href=&quot;mailto:minalee@cs.stanford.edu&quot;&gt;minalee@cs.stanford.edu&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Percy Liang &lt;a href=&quot;mailto:pliang@cs.stanford.edu&quot;&gt;pliang@cs.stanford.edu&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Answers&lt;/strong&gt;: [3] and [3]&lt;/p&gt;
</description>
              <pubDate>Thu, 10 Sep 2020 00:00:00 -0700</pubDate>
          </item>
          
        
          
          <item>
              <title>Safety Validation of Black-Box Autonomous Systems</title>
              <link>/blog/black-box-safety-validation/</link>
              <guid isPermaLink="true">/blog/black-box-safety-validation/</guid>
              <description>&lt;p&gt;With autonomous systems becoming more capable, they are entering into safety-critical domains such as autonomous driving, aircraft collision avoidance, and healthcare. Ensuring the safe operations of these systems is a crucial step before they can be deployed and accepted by our society. Failure to perform the proper degree of safety validation can risk the loss of property or even  human life.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage&quot; src=&quot;/blog/assets/img/posts/2020-08-25-black-box-safety-validation/design.png&quot; /&gt;&lt;/p&gt;
&lt;figcaption&gt;
The autonomous system design cycle.
&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Safety can be incorporated at various stages of the development of an autonomous system.  Consider the above model for the design cycle of such a system. A necessary component of safety is the &lt;strong&gt;definition&lt;/strong&gt; of a complete set of realistic and safe requirements such as the Responsibility-Sensitive Safety model&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; which encodes commonsense driving rules—such as &lt;em&gt;don’t rear end anyone&lt;/em&gt; and &lt;em&gt;right of way is given, not taken&lt;/em&gt;—into formal mathematical statements about what a vehicle is and is not allowed to do in a given driving scenario. Safety can also be incorporated directly into the &lt;strong&gt;design&lt;/strong&gt; of the system through techniques such as safety-masked reinforcement learning (RL)&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; where a driving agent learns how to drive under the constraint that it only takes actions that have a minimal likelihood of causing a collision. Compared to traditional reinforcement learning techniques which have no constraint on their exploratory actions, safety-masked RL results in a safer driving policy.&lt;/p&gt;

&lt;p&gt;Once a prototype of a system is available, safety validation can be performed through &lt;strong&gt;testing&lt;/strong&gt;, performance &lt;strong&gt;evaluation&lt;/strong&gt;, and &lt;strong&gt;interpretation&lt;/strong&gt; of the failure modes of the system. Testing can discover failures due to implementation bugs, missing requirements, and emergent behavior due to the complex interaction of subcomponents. For complex autonomous systems operating in physical environments, we can not guarantee safety in all situations, so performance evaluation techniques can determine if the system is acceptably safe. The failure examples generated from testing can then be used to understand flaws in the systems and help engineers to fix them in the next iteration. Even with safety embedded in the process of defining requirements and system design, safety validation is a critical part of ensuring safe autonomy.&lt;/p&gt;

&lt;p&gt;There are multiple ways to go about safety validation. White-box approaches use knowledge of the design of the system to construct challenging scenarios and evaluate the behavior of the system. They are often interpretable and can give a high degree of confidence in a system, but can suffer from problems of scalability. Modern autonomous systems employ complex components such as deep neural networks for perception and decision making. Despite improvements to white-box approaches for small neural networks&lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;, they don’t scale to the large networks used in practice. We can, however, trade formal guarantees for scalability by employing algorithms that treat the autonomous system as a black-box.&lt;/p&gt;

&lt;p&gt;Safety validation algorithms for black-box autonomous systems have become the preferred tool for validation since they scale to complex systems and can rely on the latest advancements in machine learning to become more effective. In this blog post we cover the latest research in algorithms for the safety validation of black box autonomous systems. For a more in-depth description of the following algorithms (including pseudocode) see our recent survey paper &lt;em&gt;&lt;a href=&quot;https://arxiv.org/abs/2005.02979&quot;&gt;A Survey of Algorithms for Black-Box Safety Validation&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-08-25-black-box-safety-validation/formulation.jpg&quot; /&gt;&lt;/p&gt;
&lt;figcaption&gt;
The problem formulation for the safety validation of black-box autonomous systems.
&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;The setup for safety validation algorithms for  black-box systems is shown above. We have a black-box system that is going to be tested, such as an autonomous vehicle driving policy or an aircraft collision avoidance system. We assume we have a simulated environment in which the system takes actions after making observations with its sensors, while an adversary perturbs the environment through disturbances &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; in an effort to make the system fail. Disturbances could include sensor noise, the behavior of other agents in the environment, or environmental conditions such as weather. The adversary may have access to the state of the environment which, for example, may describe the positions and velocity of all the vehicles and pedestrians in a driving scenario. The systems we care about usually operate over time in a physical environment, in which case the adversary seeks to find the &lt;em&gt;sequence&lt;/em&gt; of disturbances that leads to failure. Finding a disturbance trajectory &lt;script type=&quot;math/tex&quot;&gt;X = [x_1, \ldots, x_N]&lt;/script&gt; that leads to failure, rather than just a single disturbance, makes the problem much more challenging. We may also have a model of the disturbances in the environment &lt;script type=&quot;math/tex&quot;&gt;p(X)&lt;/script&gt; that describes which sequences of disturbances are most likely. The disturbance model can be constructed through expert knowledge or learned from real-world data.  The exact goal of the adversary may be&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Falsification&lt;/strong&gt;: Find any disturbance trajectory &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; that leads to a failure.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Most likely failure analysis&lt;/strong&gt;: Find the most likely disturbance trajectory that leads to a failure (i.e. maximize &lt;script type=&quot;math/tex&quot;&gt;p(X)&lt;/script&gt;).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Estimation of the probability of failure&lt;/strong&gt;: Determine how likely it is that &lt;em&gt;any&lt;/em&gt; failure will occur based on knowledge of &lt;script type=&quot;math/tex&quot;&gt;p(X)&lt;/script&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The adversary can use a variety of algorithms to generate disturbances. We will cover 4 categories: optimization, path-planning, reinforcement learning, and importance sampling.&lt;/p&gt;

&lt;h2 id=&quot;optimization&quot;&gt;Optimization&lt;/h2&gt;

&lt;p&gt;Optimization approaches search over the space of possible disturbance trajectories to find those that lead to a system failure. Optimization techniques can involve adaptive sampling or a coordinated search, both of which are guided by a cost function &lt;script type=&quot;math/tex&quot;&gt;c(X)&lt;/script&gt; which measures the level of safety for a particular disturbance trajectory. The lower the cost, the closer we are to a failure. Some common cost functions include&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Miss distance&lt;/strong&gt;: Often a physically-motivated measure of safety such as the point of closest approach between two aircraft or two vehicles.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Temporal logic robustness&lt;/strong&gt;: When the safety requirements of a system are expressed formally using temporal logic, a language used to reason about events over time, the &lt;em&gt;robustness&lt;/em&gt;&lt;sup id=&quot;fnref:6&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; measures how close a trajectory is to violating the specification&lt;sup id=&quot;fnref:7&quot;&gt;&lt;a href=&quot;#fn:7&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When performing most likely failure analysis, the probability of the disturbance trajectory is incorporated into the regular cost function to produce a new cost &lt;script type=&quot;math/tex&quot;&gt;c'(X)&lt;/script&gt;. Ideally, probability can be incorporated as a piecewise objective where &lt;script type=&quot;math/tex&quot;&gt;c'(X) = c(X)&lt;/script&gt; when &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; does not lead to failure and &lt;script type=&quot;math/tex&quot;&gt;c'(X) = -p(X)&lt;/script&gt; when &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; does lead to a failure. In practice, however, using a penalty term &lt;script type=&quot;math/tex&quot;&gt;c'(X) = c(X) - \lambda p(X)&lt;/script&gt; may be easier to optimize.&lt;/p&gt;

&lt;p&gt;The upside of formulating safety validation as an optimization problem is the ability to use off-the-shelf optimizers and rely on the significant amount of optimization literature (see Kochenderfer and Wheeler&lt;sup id=&quot;fnref:8&quot;&gt;&lt;a href=&quot;#fn:8&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt; for an overview). Approaches that have been successfully used for safety validation include simulated annealing&lt;sup id=&quot;fnref:9&quot;&gt;&lt;a href=&quot;#fn:9&quot; class=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt;, genetic algorithms&lt;sup id=&quot;fnref:10&quot;&gt;&lt;a href=&quot;#fn:10&quot; class=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt;, Bayesian optimization&lt;sup id=&quot;fnref:11&quot;&gt;&lt;a href=&quot;#fn:11&quot; class=&quot;footnote&quot;&gt;9&lt;/a&gt;&lt;/sup&gt;, extended ant-colony optimization&lt;sup id=&quot;fnref:12&quot;&gt;&lt;a href=&quot;#fn:12&quot; class=&quot;footnote&quot;&gt;10&lt;/a&gt;&lt;/sup&gt;, and genetic programming&lt;sup id=&quot;fnref:13&quot;&gt;&lt;a href=&quot;#fn:13&quot; class=&quot;footnote&quot;&gt;11&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;The downsides of optimization-based approaches are twofold. First, we are directly searching over the space of all possible disturbance &lt;em&gt;trajectories&lt;/em&gt; which is exponential in the length of the trajectory. This can quickly get out of hand. Second, the state of the environment is not typically used when choosing the disturbance trajectory. The state of the environment may not be available for logistical or privacy reasons, but if it is, then the state can provide additional information to the adversary. The next two sections describe techniques to address these limitations by building the disturbance trajectories sequentially and using the state information to help guide the search.&lt;/p&gt;

&lt;h2 id=&quot;path-planning&quot;&gt;Path Planning&lt;/h2&gt;

&lt;p&gt;When the safety validation problem is cast as a path-planning problem, we search for failures by sequentially building disturbance trajectories that explore the state space of the environment.  There are several metrics of state-space coverage that can be used to guide the search and decide when the state space has been sufficiently explored&lt;sup id=&quot;fnref:14&quot;&gt;&lt;a href=&quot;#fn:14&quot; class=&quot;footnote&quot;&gt;12&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimagesmaller&quot; src=&quot;/blog/assets/img/posts/2020-08-25-black-box-safety-validation/rrt.png&quot; /&gt;&lt;/p&gt;
&lt;figcaption&gt;
Two sample trees generated by the RRT Algorithm.
&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;One of the most common path-planning algorithms that has been used for safety validation is the rapidly-exploring random tree (RRT) algorithm, depicted above&lt;sup id=&quot;fnref:15&quot;&gt;&lt;a href=&quot;#fn:15&quot; class=&quot;footnote&quot;&gt;13&lt;/a&gt;&lt;/sup&gt;. In RRT, a space-filling tree is iteratively constructed by choosing disturbances that bring the environment into unexplored regions of the state space. The RRT algorithm has been used to find failures of an adaptive cruise control system&lt;sup id=&quot;fnref:16&quot;&gt;&lt;a href=&quot;#fn:16&quot; class=&quot;footnote&quot;&gt;14&lt;/a&gt;&lt;/sup&gt; where failures involved complex motion of the lead vehicle (shown below) that would be rarely discovered by traditional sampling techniques.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage&quot; src=&quot;/blog/assets/img/posts/2020-08-25-black-box-safety-validation/acc_failure.png&quot; /&gt;&lt;/p&gt;
&lt;figcaption&gt;
Sample failure of an adaptive cruise control system.
&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Many path planning approaches were designed to be used with white-box systems and environments where dynamics and gradient information is available. When applied to black-box safety validation, these algorithms need to be adapted to forego the use of such information. For example, in multiple shooting methods, a trajectory is constructed through disjoint segments, which are then joined using gradient descent. In the absence of gradient information, a black-box multiple shooting method was developed that connected segments by successively refining the segment inputs and outputs through full trajectory rollouts&lt;sup id=&quot;fnref:17&quot;&gt;&lt;a href=&quot;#fn:17&quot; class=&quot;footnote&quot;&gt;15&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;h2 id=&quot;reinforcement-learning&quot;&gt;Reinforcement Learning&lt;/h2&gt;

&lt;p&gt;The safety validation problem can be further simplified if we describe it as a &lt;a href=&quot;https://en.wikipedia.org/wiki/Markov_decision_process&quot;&gt;Markov decision process&lt;/a&gt; where the next state of the environment is only a function of the current state and disturbance. The Markov assumption allows us to select disturbances based only on the current state and apply reinforcement learning (RL) algorithms such as Monte Carlo tree search (MCTS), and deep RL algorithms such as Deep Q-Networks or Proximal Policy Optimization.&lt;/p&gt;

&lt;p&gt;Monte Carlo tree search is similar to RRT in that a search tree is iteratively created to find disturbance trajectories that end in failure. Unlike RRT, however, MCTS is designed for use with black-box systems. The trajectories are always rolled out from the initial state of the simulator and the search is guided by a reward function rather than a coverage of the state space. These modifications allow MCTS to be applied in the most information-poor environments.  Lee et. al&lt;sup id=&quot;fnref:4&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;16&lt;/a&gt;&lt;/sup&gt; used MCTS to find failures of an aircraft collision avoidance system (an example failure is depicted below) where they had no access to the simulator state and could only control actions through a pseudorandom seed. This approach may be preferred when organizations don’t want to expose any aspect of the functioning of their system.&lt;/p&gt;

&lt;p&gt;Deep RL has seen a lot of success in recent years due to its ability to solve problems with large state spaces, complex dynamics, and large action spaces. The success of deep RL is due to the large representational capacity of neural networks and advanced optimization techniques, which make it a natural choice as a safety validation algorithm. For example, it has been used to find failures of autonomous driving policies&lt;sup id=&quot;fnref:5&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot;&gt;17&lt;/a&gt;&lt;/sup&gt; where the state and action spaces are large and continuous—attributes that are difficult for other algorithms to handle well. A sample failure of an autonomous driving policy is demonstrated below&lt;sup id=&quot;fnref:19&quot;&gt;&lt;a href=&quot;#fn:19&quot; class=&quot;footnote&quot;&gt;18&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimagethird&quot; src=&quot;/blog/assets/img/posts/2020-08-25-black-box-safety-validation/vcas_failure.jpg&quot; /&gt;
&lt;img class=&quot;postimagehalf&quot; src=&quot;/blog/assets/img/posts/2020-08-25-black-box-safety-validation/rss_failure.gif&quot; /&gt;&lt;/p&gt;
&lt;figcaption&gt;
(Left) Sample failure of an aircraft collision avoidance system, (right) sample failure of a driving policy.
&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Optimization, path-planning and RL approaches all lend themselves to solving the problems of falsification and most likely failure analysis. However, when we need to evaluate the failure probability of a system, importance sampling approaches should be used.&lt;/p&gt;

&lt;h2 id=&quot;importance-sampling&quot;&gt;Importance Sampling&lt;/h2&gt;

&lt;p&gt;The final set of approaches are well-suited for the task of estimating the probability of failure of the system from many failure examples. Importance sampling approaches seek to learn a sampling distribution &lt;script type=&quot;math/tex&quot;&gt;q(X)&lt;/script&gt; that reliably produces failures and can be used to estimate the probability of failure with the minimal number of samples. Some common approaches are the cross-entropy method&lt;sup id=&quot;fnref:20&quot;&gt;&lt;a href=&quot;#fn:20&quot; class=&quot;footnote&quot;&gt;19&lt;/a&gt;&lt;/sup&gt;, multilevel splitting&lt;sup id=&quot;fnref:21&quot;&gt;&lt;a href=&quot;#fn:21&quot; class=&quot;footnote&quot;&gt;20&lt;/a&gt;&lt;/sup&gt;, supervised learning&lt;sup id=&quot;fnref:22&quot;&gt;&lt;a href=&quot;#fn:22&quot; class=&quot;footnote&quot;&gt;21&lt;/a&gt;&lt;/sup&gt;, and approximate dynamic programming&lt;sup id=&quot;fnref:23&quot;&gt;&lt;a href=&quot;#fn:23&quot; class=&quot;footnote&quot;&gt;22&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;Most importance sampling approaches suffer the same drawback as optimization-based approaches: they are constructing a distribution across the entire disturbance trajectory &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt;. If we can invoke the Markov assumption, however, then we can construct a good sampling distribution based only on the current state using dynamic programming. However, the downside to dynamic programming is its inability to scale to large state spaces and thus complex scenarios. Our recent work&lt;sup id=&quot;fnref:25&quot;&gt;&lt;a href=&quot;#fn:25&quot; class=&quot;footnote&quot;&gt;23&lt;/a&gt;&lt;/sup&gt; shows that we can overcome this scalability problem by decomposing the system into subproblems and combining the subproblem solutions. For example, in an autonomous driving scenario, each adversarial agent on the road is paired with the ego vehicle to create a smaller safety validation problem with just two agents. Each of these problems are solved and then recombined using a neural network based on the Attend, Adapt and Transfer (A2T) architecture&lt;sup id=&quot;fnref:24&quot;&gt;&lt;a href=&quot;#fn:24&quot; class=&quot;footnote&quot;&gt;24&lt;/a&gt;&lt;/sup&gt;. The combined solution is then refined using simulations of the full scenario. The decomposition strategy, network architecture and a sample failure for a 5-agent driving scenario is shown below. These types of hybrid approaches will be required to solve the most challenging safety validation problems.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimagehalf&quot; src=&quot;/blog/assets/img/posts/2020-08-25-black-box-safety-validation/decomp.png&quot; /&gt;
&lt;img class=&quot;postimagehalf&quot; src=&quot;/blog/assets/img/posts/2020-08-25-black-box-safety-validation/A2T.png&quot; /&gt;&lt;/p&gt;
&lt;figcaption&gt;
(Left) Decomposition into pairwise subproblems, each involving the blue ego vehicle. (Right) The network used to fuse the subproblem solutions based on A2T.
&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/blog/assets/img/posts/2020-08-25-black-box-safety-validation/A2T_failure.gif&quot; width=&quot;91%&quot; /&gt;&lt;/p&gt;
&lt;figcaption&gt;
Sample failure for an autonomous driving policy in a complex environment.
&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;h2 id=&quot;the-future&quot;&gt;The Future&lt;/h2&gt;

&lt;p&gt;The validation of complex and safety-critical autonomous systems will likely involve many different techniques throughout the system design cycle, and black-box safety validation algorithms will play a crucial role. In particular, black-box algorithms are useful to the engineers who design safety-critical systems as well as third-party organizations that wish to validate the safety of such systems for regulatory or risk-assessment purposes. Although this post reviews many algorithms that will be of practical use for the validation of safety-critical autonomous systems, there are still areas that require more investigation. For example, we would like to be able to answer the question: if no failure has been found, how sure are we that the system is safe? This will require the development of algorithms that have formal or probabilistic guarantees of convergence. Scalability also remains a significant challenge. Autonomous systems can encounter a wide range of complex interactions, so safety validation algorithms must be able to efficiently discover failures in the most complex scenarios. The algorithms presented in this survey are a promising step toward safe and beneficial autonomy.&lt;/p&gt;

&lt;h5 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h5&gt;

&lt;p class=&quot;small-text&quot;&gt;
Many thanks to Michelle Lee, Andrey Kurenkov, Robert Moss, Mark Koren, Ritchie Lee, and Mykel Kochenderfer for comments and edits on this blog post.
&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;Shalev-Shwartz, Shai, et al. “On a formal model of safe and scalable self-driving cars.” &lt;em&gt;arXiv preprint arXiv:1708.06374&lt;/em&gt; (2017). &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;Bouton, Maxime, et al. “Reinforcement learning with probabilistic guarantees for autonomous driving.” &lt;em&gt;arXiv preprint arXiv:1904.07189&lt;/em&gt; (2019). &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;Katz, Guy, et al. “Reluplex: An efficient SMT solver for verifying deep neural networks.” &lt;em&gt;International Conference on Computer Aided Verification&lt;/em&gt;. Springer, 2017. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot;&gt;
      &lt;p&gt;Fainekos, Georgios E., et al. “Robustness of temporal logic specifications for continuous-time signals.” &lt;em&gt;Theoretical Computer Science&lt;/em&gt; 410.42 (2009): 4262-4291. &lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:7&quot;&gt;
      &lt;p&gt;Mathesen, Logan, et al. “Falsification of cyber-physical systems with robustness uncertainty quantification through stochastic optimization with adaptive restart.” &lt;em&gt;International Conference on Automation Science and Engineering (CASE)&lt;/em&gt;. IEEE, 2019. &lt;a href=&quot;#fnref:7&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:8&quot;&gt;
      &lt;p&gt;M. J. Kochenderfer and T. A. Wheeler, &lt;em&gt;Algorithms for optimization&lt;/em&gt;. MIT Press, 2019. &lt;a href=&quot;#fnref:8&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:9&quot;&gt;
      &lt;p&gt;Abbas, Houssam, et al. “Probabilistic temporal logic falsification of cyber-physical systems.” &lt;em&gt;ACM Transactions on Embedded Computing Systems (TECS)&lt;/em&gt; 12.2s (2013): 1-30. &lt;a href=&quot;#fnref:9&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:10&quot;&gt;
      &lt;p&gt;Zou, Xueyi, et al. “Safety validation of sense and avoid algorithms using simulation and evolutionary search.” &lt;em&gt;International Conference on Computer Safety, Reliability, and Security&lt;/em&gt;. Springer, 2014. &lt;a href=&quot;#fnref:10&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:11&quot;&gt;
      &lt;p&gt;Mullins, Galen E., et al. “Adaptive generation of challenging scenarios for testing and evaluation of autonomous vehicles.” &lt;em&gt;Journal of Systems and Software&lt;/em&gt; 137 (2018): 197-215. &lt;a href=&quot;#fnref:11&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:12&quot;&gt;
      &lt;p&gt;Annapureddy, Yashwanth Singh Rahul, et al. “Ant colonies for temporal logic falsification of hybrid systems.” &lt;em&gt;Annual Conference on IEEE Industrial Electronics Society (IECON)&lt;/em&gt;. IEEE, 2010. &lt;a href=&quot;#fnref:12&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:13&quot;&gt;
      &lt;p&gt;Corso, Anthony, et al. “Interpretable safety validation for autonomous vehicles.” To appear in &lt;em&gt;International Conference on Intelligent Transportation Systems (ITSC)&lt;/em&gt;. IEEE, 2020. &lt;a href=&quot;#fnref:13&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:14&quot;&gt;
      &lt;p&gt;Nahhal, Tarik, et al. “Test coverage for continuous and hybrid systems.” &lt;em&gt;International Conference on Computer Aided Verification&lt;/em&gt;. Springer, Berlin, Heidelberg, 2007. &lt;a href=&quot;#fnref:14&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:15&quot;&gt;
      &lt;p&gt;LaValle, Steven M. &lt;em&gt;Planning algorithms&lt;/em&gt;. Cambridge University Press, 2006. &lt;a href=&quot;#fnref:15&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:16&quot;&gt;
      &lt;p&gt;Koschi, Markus, et al. “Computationally efficient safety falsification of adaptive cruise control systems.”_ Intelligent Transportation Systems Conference (ITSC)_. IEEE, 2019. &lt;a href=&quot;#fnref:16&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:17&quot;&gt;
      &lt;p&gt;Zutshi, Aditya, et al. “Multiple shooting, cegar-based falsification for hybrid systems.” &lt;em&gt;International Conference on Embedded Software&lt;/em&gt;. 2014. &lt;a href=&quot;#fnref:17&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot;&gt;
      &lt;p&gt;Lee, Ritchie, et al. “Adaptive stress testing of airborne collision avoidance systems.” &lt;em&gt;Digital Avionics Systems Conference (DASC)&lt;/em&gt;. IEEE, 2015. &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot;&gt;
      &lt;p&gt;Koren, Mark, et al. “Adaptive stress testing for autonomous vehicles.” &lt;em&gt;Intelligent Vehicles Symposium (IV)&lt;/em&gt;. IEEE, 2018. &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:19&quot;&gt;
      &lt;p&gt;Corso, Anthony, et al. “Adaptive stress testing with reward augmentation for autonomous vehicle validation.” &lt;em&gt;Intelligent Transportation Systems Conference (ITSC)&lt;/em&gt;. IEEE, 2019. &lt;a href=&quot;#fnref:19&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:20&quot;&gt;
      &lt;p&gt;O’Kelly, Matthew, et al. “Scalable end-to-end autonomous vehicle testing via rare-event simulation.” &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt;. 2018. &lt;a href=&quot;#fnref:20&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:21&quot;&gt;
      &lt;p&gt;Norden, Justin, et al. “Efficient black-box assessment of autonomous vehicle safety.” &lt;em&gt;arXiv preprint arXiv:1912.03618&lt;/em&gt; (2019). &lt;a href=&quot;#fnref:21&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:22&quot;&gt;
      &lt;p&gt;Uesato, Jonathan, et al. “Rigorous agent evaluation: An adversarial approach to uncover catastrophic failures.” &lt;em&gt;arXiv preprint arXiv:1812.01647&lt;/em&gt; (2018). &lt;a href=&quot;#fnref:22&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:23&quot;&gt;
      &lt;p&gt;Corso, Anthony, et al. “Scalable autonomous vehicle safety validation through dynamic programming and scene decomposition.” To appear in &lt;em&gt;International Conference on Intelligent Transportation Systems (ITSC)&lt;/em&gt;. IEEE, 2020. &lt;a href=&quot;#fnref:23&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:25&quot;&gt;
      &lt;p&gt;Corso, Anthony, et al. “Scalable autonomous vehicle safety validation through dynamic programming and scene decomposition.” To appear in &lt;em&gt;International Conference on Intelligent Transportation Systems (ITSC)&lt;/em&gt;. IEEE, 2020. &lt;a href=&quot;#fnref:25&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:24&quot;&gt;
      &lt;p&gt;Rajendran, Janarthanan, et al. “Attend, adapt and transfer: Attentive deep architecture for adaptive transfer from multiple sources in the same domain.” &lt;em&gt;arXiv preprint arXiv:1510.02879&lt;/em&gt; (2015). &lt;a href=&quot;#fnref:24&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
              <pubDate>Mon, 31 Aug 2020 00:00:00 -0700</pubDate>
          </item>
          
        
          
          <item>
              <title>Explore then Execute: Adapting without Rewards via Factorized Meta-Reinforcement Learning</title>
              <link>/blog/meta-exploration/</link>
              <guid isPermaLink="true">/blog/meta-exploration/</guid>
              <description>&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-08-23-meta-exploration/fulfilling_activities.svg&quot; /&gt;&lt;/p&gt;
&lt;figcaption&gt;
Activities more fulfilling than chores.
&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Nobody likes chores — can we build robots to do these chores, such as
cooking, for us? A common paradigm for training agents to perform
various tasks is to train a separate agent on each task, completely from
scratch, with reinforcement learning. However, training a robot to cook
with reinforcement learning from scratch in each person’s home would
completely fail, as it would result in many disasters (e.g., kitchen
fires), would require a lot of supervision from each person to reward
the robot for successfully cooking meals, and would take a long time
(learning even simple tasks from scratch can take reinforcement learning
agents millions of attempts).&lt;/p&gt;

&lt;p&gt;Instead, it would be ideal if we could train a robot to be able to
quickly adapt to various home kitchens, after first training in many
kitchens in a robot chef factory. Intuitively, this should be possible
since different tasks and environments share considerable structure
(e.g., cooking pizza in one kitchen is similar to cooking a hamburger in
another kitchen), which can make learning each task easier and more
efficient.&lt;/p&gt;

&lt;p&gt;Fortunately, meta-reinforcement learning seeks this exact goal of
training agents to adapt to new tasks from very few interactions on the
new task, after first training on many similar tasks. So, why aren’t
robots cooking in our kitchens today? To answer this question, we’ll
turn our attention to the problem of &lt;em&gt;meta-exploration&lt;/em&gt;: how to best
spend these few interactions &lt;em&gt;exploring&lt;/em&gt; the new task. For example, in
order to adapt to a new kitchen, a robot chef should ideally spend its
few interactions exploring the new kitchen to find the ingredients,
which will allow it to later cook a meal (solve the task). In this blog
post, we’ll cover and solve two key challenges about
meta-exploration that keep humans in the kitchen.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;First, we’ll show that existing meta-reinforcement learning
approaches suffer from a chicken-and-egg coupling problem:
learning to explore and find the ingredients only helps a robot
prepare a meal if it already knows how to cook, but the robot can
only learn to cook if it already knows where the ingredients are.
We’ll avoid this cyclic dependence of learning to explore and learning to
execute (solve the task) by proposing an objective to learn them
independently of each other.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Second, we’ll observe that the standard meta-reinforcement learning
problem setting expects robots to cook the correct meal by
trial-and-error, without even being told what meal to cook, which
unnecessarily complicates the meta-exploration problem. To avoid
this, we propose instruction-based meta-reinforcement learning,
where the robot receives instructions specifying what meal to
cook.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;standard-meta-reinforcement-learning&quot;&gt;Standard Meta-Reinforcement Learning&lt;/h2&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/blog/assets/img/posts/2020-08-23-meta-exploration/standard_setting.svg&quot; /&gt;&lt;/p&gt;
&lt;figcaption&gt;
Standard meta-RL setting.
&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Before we dive in, let’s review the standard meta-reinforcement learning
(meta-RL) problem statement. In meta-reinforcement learning, an agent
(e.g., a robot chef) trains on many tasks (different recipes) and
environments (different kitchens), and then must accomplish a new task
in a new environment during meta-testing. When presented with a new task
and environment, the agent is allowed to first spend an episode
exploring, gathering any necessary information (e.g., locating the
ingredients), before execution episodes, where the agent must accomplish
the task (e.g., cook a meal).&lt;/p&gt;

&lt;p&gt;In more formal language, standard meta-RL considers a family of
problems, where each problem &lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt; identifies a reward function
&lt;script type=&quot;math/tex&quot;&gt;\mathcal{R}_\mu&lt;/script&gt; (e.g., cook a pizza) and transition dynamics
(e.g., a kitchen).
Using the terminology from Duan et al., 2016, we define a trial to consist of
several episodes in the same problem. The first episode is the exploration
episode, where the agent is allowed to gather information, without needing to
maximize returns. All subsequent episodes are execution episodes, where the
agent must accomplish the task. The goal is to maximize the returns achieved
during the execution episodes of meta-testing trials, after first training on
many trials during meta-training.&lt;/p&gt;

&lt;h2 id=&quot;decoupled-reward-free-exploration-and-execution-in-meta-reinforcement-learning-dream&quot;&gt;&lt;strong&gt;D&lt;/strong&gt;ecoupled &lt;strong&gt;R&lt;/strong&gt;eward-free &lt;strong&gt;E&lt;/strong&gt;xplor&lt;strong&gt;a&lt;/strong&gt;tion and Execution in &lt;strong&gt;M&lt;/strong&gt;eta-Reinforcement Learning (DREAM)&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;A chicken-and-egg coupling problem.&lt;/strong&gt; A common approach (Wang et al.,
2016, Duan et al., 2016) for the meta-exploration problem is to optimize
a recurrent policy that performs both exploration and execution episodes
end-to-end based on the execution episode rewards. The hope is to
capture the information learned during the exploration episode in the
recurrent policy’s hidden state, which will then be useful for
execution episodes. However, this leads to a chicken-and-egg coupling
problem, where learning good exploration behaviors requires already
having learned good execution behaviors and vice-versa, which prevents
such an approach from learning.&lt;/p&gt;

&lt;p&gt;For example, if a robot chef fails to discover the locations of
ingredients in a kitchen (bad exploration), then it cannot possibly
learn how to cook (bad execution). On the other hand, if the robot does
not know how to cook (bad execution), then no matter what it does during
the exploration episode, it will still not successfully cook a meal,
making learning exploration challenging. Since robots can’t cook or
explore at the beginning of training, they get stuck in this local
optimum and have a hard time learning either.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimagehalf&quot; style=&quot;width:40%&quot; src=&quot;/blog/assets/img/posts/2020-08-23-meta-exploration/coupling.svg&quot; /&gt;
&lt;img class=&quot;postimagehalf&quot; style=&quot;width:40%&quot; src=&quot;/blog/assets/img/posts/2020-08-23-meta-exploration/coupling_example.svg&quot; /&gt;&lt;/p&gt;
&lt;figcaption&gt;
The coupling problem. What came first: the chicken (good exploration) or
the egg (good execution)?
&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Avoiding the coupling problem with DREAM.&lt;/strong&gt; To avoid the
chicken-and-egg coupling problem, we propose a method to break the
cyclic dependency between learning exploration and learning execution
behaviors, which we call DREAM. Intuitively, good exploration can be
learned by trying to recover the information necessary for executing
instructions. Therefore, from a high-level, DREAM consists of two main
steps: 1) simultaneously learn an execution policy independently from
exploration and learn what information is necessary for execution and 2)
learn an exploration policy to recover that information.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/blog/assets/img/posts/2020-08-23-meta-exploration/dream_meta_training.svg&quot; /&gt;&lt;/p&gt;
&lt;figcaption&gt;
To answer the chicken-and-egg problem, DREAM manufactures its own egg,
and out comes the chicken.
&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;More concretely, in the first step, we train an execution policy
&lt;script type=&quot;math/tex&quot;&gt;\pi^\text{exec}&lt;/script&gt;
conditioned on the problem identifier &lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt;, which in the cooking example,
may either directly identify attributes of the kitchen (e.g., wall color
or ingredient locations), or simply be a unique identifier (e.g., a
one-hot) for each kitchen. This problem identifier (directly or
indirectly) encodes all the information necessary to solve tasks in the
kitchen, allowing the execution policy to learn independently from
exploration, which avoids the coupling problem. At the same time, our
goal in the first step is to identify only the information necessary for
executing instructions, and the problem identifier may also encode
extraneous information, such as the wall color. To remove this, we apply
an information bottleneck to obtain a bottlenecked representation &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;,
which we use for training an exploration policy &lt;script type=&quot;math/tex&quot;&gt;\pi^\text{exp}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;In the second step, once we’ve obtained a bottleneck representation &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;
that ideally contains only the information necessary for executing
instructions, we can train an exploration policy &lt;script type=&quot;math/tex&quot;&gt;\pi^\text{exp}&lt;/script&gt; to recover this
information in the exploration episode. To do this, we roll-out the
exploration policy to obtain an episode &lt;script type=&quot;math/tex&quot;&gt;\tau&lt;/script&gt; and then reward the policy
based on how well this episode encodes the information contained in &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;.
Roughly, this reward is the mutual information &lt;script type=&quot;math/tex&quot;&gt;I(z; \tau)&lt;/script&gt; between the
bottlenecked representation &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt; and the episode &lt;script type=&quot;math/tex&quot;&gt;\tau&lt;/script&gt;.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-08-23-meta-exploration/dream_meta_testing.svg&quot; /&gt;&lt;/p&gt;
&lt;figcaption&gt;
DREAM meta-testing.
&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;The problem identifier &lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt; is easy to provide during meta-training by
simply assigning each problem a unique one-hot, but is typically
unavailable or unhelpful during meta-testing (e.g., if &lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt; is a completely
new one-hot). This might seem concerning, since, during meta-training,
the execution policy conditions on &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;, which requires knowing &lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt;. However,
since the exploration policy is trained to produce exploration
trajectories &lt;script type=&quot;math/tex&quot;&gt;\tau&lt;/script&gt; that contain the same information as &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;, we can directly
swap &lt;script type=&quot;math/tex&quot;&gt;\tau&lt;/script&gt; for &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt; at meta-test time by rolling out the exploration policy.
See our paper for the details!&lt;/p&gt;

&lt;h2 id=&quot;instruction-based-meta-reinforcement-learning-imrl&quot;&gt;Instruction-based Meta-Reinforcement Learning (IMRL)&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Improving the standard meta-RL setting.&lt;/strong&gt; A second meta-exploration
challenge concerns the meta-reinforcement learning setting itself.
While the above standard
meta-RL setting is a useful problem formulation, we observe two areas
that can be made more realistic. First, the standard setting requires
the agent to infer the task (e.g., the meal to cook) from reward
observations, which can be needlessly inefficient. In more realistic
situations, the user would just tell the agent what they want, instead.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/blog/assets/img/posts/2020-08-23-meta-exploration/instructions.svg&quot; /&gt;&lt;/p&gt;
&lt;figcaption&gt;
Open and honest communication is important for your robots too.
&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Second, while the standard meta-RL setting leverages shared structure
between different problems (environment and task pairs), it does not
capture shared structure between different tasks in the same
environment. More concretely, the task is fixed across all episodes in a
trial, and in order to perform a new task (e.g., cook a new meal), the
agent requires another exploration episode, even when the underlying
environment (e.g., the kitchen) stays the same. Instead, an agent would
ideally be able to perform many tasks after a single exploration
episode. For example, after exploring the kitchen to find any
ingredients, an ideal robot chef would be able to then cook &lt;em&gt;any&lt;/em&gt; meal
involving those ingredients, whereas an agent trained in the standard
meta-reinforcement learning setting would only be able to cook a single meal.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage_50&quot; style=&quot;width:66%&quot; src=&quot;/blog/assets/img/posts/2020-08-23-meta-exploration/multitask.svg&quot; /&gt;&lt;/p&gt;
&lt;figcaption&gt;
Dinner schedule according to a robot chef trained in the standard
meta-reinforcement learning setting.
&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;These two areas can obscure the meta-exploration problem of how to
optimally spend the exploration episode, as the former requires
unnecessary exploration to infer the task, while the latter only
requires the agent to explore to discover information relevant to a
single task. While intuitively, the agent should spend the exploration
episode gathering useful information for later execution episodes, in
many cases, optimal exploration collapses to simply solving the task.
For example, the agent can only discover that the task is to cook pizza
by successfully cooking pizza and receiving positive rewards, only to do
the same thing again and again on future execution episodes. This can
make the exploration episode nearly useless.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Instruction-based meta-RL (IMRL).&lt;/strong&gt; To make the meta-RL setting more
realistic, we propose a new setting called instruction-based meta-RL
(IMRL), which addresses the two above areas by (i) providing the agent
with instructions (e.g., &quot;cook pizza&quot; or a one-hot representation)
that specify the task during execution episodes and (ii) varying the
task by providing a different instruction on each execution episode.
Then, for example, after meta-training in different kitchens at a
factory, a robot chef could begin cooking many different meals specified
by a human in a new home kitchen, after a single setup period
(exploration episode).&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/blog/assets/img/posts/2020-08-23-meta-exploration/imrl.svg&quot; /&gt;&lt;/p&gt;
&lt;figcaption&gt;
Instruction-based meta-RL: The task, which changes each execution
episode, is conveyed to the agent via instructions. The environment
still stays the same within a trial.
&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Reward-free adaptation.&lt;/strong&gt; In the standard meta-RL setting, the agent
requires reward observations during exploration episodes in order to
infer the task. However, by receiving instructions that specify the task
in IMRL, a further benefit is that the agent no longer requires observing
rewards to adapt to new tasks and environments. Concretely, IMRL enables
reward-free adaptation, where during meta-training, the agent uses reward
observations during execution episodes to learn to solve the task, but does
not observe rewards during exploration episodes. During meta-testing, the
agent never observes any rewards. This enables modeling real-world deployment
situations where gathering reward supervision is really expensive. For
example, a robot chef would ideally be able to adapt to a home kitchen without
any supervision from a human.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Is IMRL general?&lt;/strong&gt; Importantly, setting the instruction to always be
some &quot;empty&quot; instruction recovers the standard meta-RL setting. In
other words, standard meta-RL is just IMRL, where the user's desires
are fixed within a trial and the user says nothing for the instructions.
Therefore, algorithms developed for IMRL can also be directly applied to
the standard setting and vice-versa.&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;figure class=&quot;postfigurehalf&quot;&gt;
  &lt;img src=&quot;/blog/assets/img/posts/2020-08-23-meta-exploration/blue.png&quot; /&gt;
  &lt;figcaption&gt;
  Sign reads &lt;b style=&quot;color:blue&quot;&gt;blue&lt;/b&gt;.
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;figure class=&quot;postfigurehalf&quot;&gt;
  &lt;img src=&quot;/blog/assets/img/posts/2020-08-23-meta-exploration/red.png&quot; /&gt;
  &lt;figcaption&gt;
  Sign reads &lt;b style=&quot;color:red&quot;&gt;red&lt;/b&gt;.
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Sparse-reward 3D visual navigation.&lt;/strong&gt; In one experiment from our
paper, we evaluate DREAM on the sparse-reward 3D visual navigation
problem family proposed by Kamienny et al., 2020 (pictured above), which
we've made harder by including a visual sign and more objects. We use
the IMRL setting with reward-free adaptation. During execution episodes,
the agent receives an instruction to go to an object: a ball, block or
key. The agent starts episodes on the far side of the barrier, and must
walk around the barrier to read the sign
(highlighted in &lt;b style=&quot;color:#e6e600&quot;&gt;yellow&lt;/b&gt;),
which in the two versions of the problem, either specify going to the
&lt;b style=&quot;color:blue&quot;&gt;blue&lt;/b&gt; or &lt;b style=&quot;color:red&quot;&gt;red&lt;/b&gt; version of the
object. The agent receives 80x60 RGB images as observations
and can turn left or right, or move forward. Going to the correct object gives
reward &lt;b&gt;+1&lt;/b&gt; and going to the wrong object gives reward &lt;b&gt;-1&lt;/b&gt;.&lt;/p&gt;

&lt;p&gt;DREAM learns near-optimal exploration and execution behaviors on this
task, which are pictured below. On the left, DREAM spends the
exploration episode walking around the barrier to read the sign, which
says &lt;b style=&quot;color:blue&quot;&gt;blue&lt;/b&gt;. On the right, during an execution
episode, DREAM receives an instruction to go to the key. Since DREAM already
read that the sign said &lt;b style=&quot;color:blue&quot;&gt;blue&lt;/b&gt; during the exploration
episode, it goes to the &lt;b style=&quot;color:blue&quot;&gt;blue&lt;/b&gt; key.&lt;/p&gt;

&lt;h3 id=&quot;behaviors-learned-by-dream&quot;&gt;Behaviors learned by DREAM&lt;/h3&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;figure class=&quot;postfigurehalf&quot;&gt;
  &lt;img src=&quot;/blog/assets/img/posts/2020-08-23-meta-exploration/resize_explore.gif&quot; /&gt;
  &lt;figcaption&gt;
  Exploration.
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;figure class=&quot;postfigurehalf&quot;&gt;
  &lt;img src=&quot;/blog/assets/img/posts/2020-08-23-meta-exploration/resize_execute.gif&quot; /&gt;
  &lt;figcaption&gt;
  Execution: go to the key.
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Comparisons.&lt;/strong&gt; Broadly, prior meta-RL approaches fall into two main
groups: (i) end-to-end approaches, where exploration and execution are
optimized end-to-end based on execution rewards, and (ii) decoupled
approaches, where exploration and execution are optimized with separate
objectives. We compare DREAM with state-of-the-art approaches from both
categories. In the end-to-end category, we compare with:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;RL&lt;script type=&quot;math/tex&quot;&gt;^2&lt;/script&gt;&lt;sup id=&quot;fnref:duan2016fast&quot;&gt;&lt;a href=&quot;#fn:duan2016fast&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&quot;fnref:wang2016learning&quot;&gt;&lt;a href=&quot;#fn:wang2016learning&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;, the canonical
end-to-end approach, which learns a recurrent policy conditioned
on the entire sequence of past state and reward observations.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;VariBAD&lt;sup id=&quot;fnref:zintgraf2019varibad&quot;&gt;&lt;a href=&quot;#fn:zintgraf2019varibad&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;, which additionally adds auxiliary
loss functions to the hidden state of the recurrent policy to
predict the rewards and dynamics of the current problem. This can
be viewed as learning the belief state&lt;sup id=&quot;fnref:kaelbling1998planning&quot;&gt;&lt;a href=&quot;#fn:kaelbling1998planning&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;, a
sufficient summary of all of its past observations.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;IMPORT&lt;sup id=&quot;fnref:kamienny2020learning&quot;&gt;&lt;a href=&quot;#fn:kamienny2020learning&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;, which additionally leverages the
problem identifier to help learn execution behaviors.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Additionally, in the decoupled category, we compare with:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;PEARL-UB, an upperbound on PEARL&lt;sup id=&quot;fnref:rakelly2019efficient&quot;&gt;&lt;a href=&quot;#fn:rakelly2019efficient&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;. We
analytically compute the expected rewards achieved by the optimal
problem-specific policy that explores with
Thompson sampling&lt;sup id=&quot;fnref:thompson1933likelihood&quot;&gt;&lt;a href=&quot;#fn:thompson1933likelihood&quot; class=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt; using the true posterior
distribution over problems.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Quantitative results.&lt;/strong&gt; Below, we plot the returns achieved by all
approaches. In contrast to DREAM, which achieves near-optimal returns,
we find that the end-to-end approaches never read the sign, and
consequently avoid all objects, in fear of receiving negative reward for
going to the wrong object. This happens even when they are allowed to
observe rewards in the exploration episode (dotted lines). Therefore,
they achieve no rewards, which is indicative of the coupling problem.&lt;/p&gt;

&lt;p&gt;On the other hand, while existing approaches in the decoupled category
avoid the coupling problem, optimizing their objectives does not lead to
the optimal exploration policy. For example, Thompson sampling
approaches (PEARL-UB) do not achieve optimal reward, even with the
optimal problem-specific execution policy and access to the true
posterior distribution over problems. To see this, recall that Thompson
sampling explores by sampling a problem from the posterior distribution
and following the execution policy for that problem. Since the optimal
execution policy directly goes to the correct object, and never reads
the sign, Thompson sampling never reads the sign during exploration. In
contrast, a nice property of DREAM is that with enough data and
expressive-enough policy classes, it theoretically learns optimal
exploration and execution.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/blog/assets/img/posts/2020-08-23-meta-exploration/results.png&quot; /&gt;&lt;/p&gt;
&lt;figcaption&gt;
Training curves with (dotted lines) and without (solid lines) rewards
during exploration. Only DREAM reads the sign and solves the task. And
it does it without needing rewards during exploration!
&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Additional results.&lt;/strong&gt; In our paper, we also evaluate DREAM on
additional didactic problems, designed to to answer the following
questions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Can DREAM efficiently explore to discover only the information
required to execute instructions?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Can DREAM generalize to unseen instructions and environments?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Does DREAM also show improved results in the standard meta-RL
setting, as well as instruction-based meta-RL?&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Broadly, the answer is yes to all of these questions. Check out our
paper for detailed results!&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Summary.&lt;/strong&gt; In this blog post, we tackled the problem of
meta-exploration: how to best gather information in a new environment in
order to perform a task. To do this, we examined and addressed two key
challenges.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;First, we saw how existing meta-RL approaches that optimize both
exploration and execution &lt;em&gt;end-to-end&lt;/em&gt; to maximize reward fall
prey to a chicken-and-egg problem. If the agent hasn’t learned to
explore yet, then it can’t gather key information (e.g., the
location of ingredients) required for learning to solve tasks
(e.g., cook a meal). On the other hand, if the agent hasn’t
learned to solve tasks yet, then there’s no signal for learning to
explore, as it’ll fail to solve the task no matter what. We
avoided this problematic cycle by proposing a &lt;em&gt;decoupled&lt;/em&gt;
objective (DREAM), which learns to explore and learns to solve
tasks independently from each other.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Second, we saw how the standard meta-RL setting captures the notion
of adapting to a new environment and task, but requires the agent
to unnecessarily explore to infer the task (e.g., what meal to
cook) and doesn’t leverage the shared structure between different
tasks in the &lt;em&gt;same&lt;/em&gt; environment (e.g., cooking different meals in
the same kitchen). We addressed this by proposing
instruction-based meta-RL (IMRL), which provides the agent with an
instruction that specifies the task and requires the agent to
explore and gather information useful for &lt;em&gt;many&lt;/em&gt; tasks.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;DREAM and IMRL combine quite nicely: IMRL enables reward-free adaptation
in principle, and DREAM achieves it in practice. Other state-of-the-art
approaches we tested weren’t able to achieve reward-free adaptation, due
to the chicken-and-egg coupling problem.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What’s next?&lt;/strong&gt; There’s a lot of room for future work — here are a
few directions to explore.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;More sophisticated instruction and problem ID representations.&lt;/em&gt;
This work examines the case where the instructions and problem IDs
are represented as unique one-hots, as a proof of concept. Of
course, in the real world, instructions and problem IDs might be
better represented with natural language, or images (e.g., a
picture of the meal to cook).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Applying DREAM to other meta-RL settings.&lt;/em&gt; DREAM applies generally
to any meta-RL setting where some information is conveyed to the
agent and the rest must be discovered via exploration. In this
work, we studied two such instances — in IMRL, the instruction
conveys the task and in the standard meta-RL setting, everything
must be discovered via exploration — but there are other settings
worth examining, too. For example, we might want to convey
information about the environment to the agent, such as the
locations of some ingredients, or that the left burner is broken,
so the robot chef should use the right one.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Seamlessly integrating exploration and execution.&lt;/em&gt; In the most
commonly studied meta-RL setting, the agent is allowed to first
gather information via exploration (exploration episode) before
then solving tasks (execution episodes). This is also the setting
we study, and it can be pretty realistic. For example, a robot
chef might require a setup phase, where it first explores a home
kitchen, before it can start cooking meals. On the other hand, a
few works, such as Zintgraf et al., 2019, require the agent to
start solving tasks from the get-go: there are no exploration
episodes and all episodes are execution episodes. DREAM can
already operate in this setting, by just ignoring the rewards and
exploring in the first execution episode, and trying to make up
for the first execution episode with better performance in later
execution episodes. This works surprisingly well, but it’d be nice
to more elegantly integrate exploration and execution.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;This is work done with my fantastic collaborators
&lt;a href=&quot;https://stanford.edu/~aditir/&quot;&gt;Aditi Raghunathan&lt;/a&gt;,
&lt;a href=&quot;https://cs.stanford.edu/~pliang/&quot;&gt;Percy Liang&lt;/a&gt;,
and &lt;a href=&quot;https://ai.stanford.edu/~cbfinn/&quot;&gt;Chelsea Finn&lt;/a&gt;.
You can check out our &lt;a href=&quot;https://arxiv.org/abs/2008.02790&quot;&gt;full paper on ArXiv&lt;/a&gt;
and our &lt;a href=&quot;https://github.com/ezliu/dream&quot;&gt;source code on GitHub&lt;/a&gt;.
You can also find a short talk on this work &lt;a href=&quot;https://youtu.be/EiIC0Rkz8-s&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Many thanks to &lt;a href=&quot;https://www.andreykurenkov.com/&quot;&gt;Andrey Kurenkov&lt;/a&gt;
for comments and edits on this blog post!&lt;/p&gt;

&lt;p&gt;The icons used in the above figures were made by Freepik, ThoseIcons,
dDara, mynamepong, Icongeek26, photo3idea_studio and Vitaly Gorbachev
from &lt;a href=&quot;https://www.flaticon.com/&quot;&gt;flaticon.com&lt;/a&gt;.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/blog/assets/img/posts/2020-08-23-meta-exploration/collaborators.svg&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:duan2016fast&quot;&gt;
      &lt;p&gt;Y. Duan, J. Schulman, X. Chen, P. L. Bartlett, I. Sutskever, and P. Abbeel. RL2: Fast reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779, 2016. &lt;a href=&quot;#fnref:duan2016fast&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:wang2016learning&quot;&gt;
      &lt;p&gt;J. X. Wang, Z. Kurth-Nelson, D. Tirumala, H. Soyer, J. Z. Leibo, R. Munos, C. Blundell, D. Kumaran, and M. Botvinick. Learning to reinforcement learn. arXiv preprint arXiv:1611.05763, 2016. &lt;a href=&quot;#fnref:wang2016learning&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:zintgraf2019varibad&quot;&gt;
      &lt;p&gt;L. Zintgraf, K. Shiarlis, M. Igl, S. Schulze, Y. Gal, K. Hofmann, and S. Whiteson.  VariBAD A very good method for bayes-adaptive deep RL via meta-learning. arXiv preprint arXiv:1910.08348, 2019. &lt;a href=&quot;#fnref:zintgraf2019varibad&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:kaelbling1998planning&quot;&gt;
      &lt;p&gt;L. P. Kaelbling, M. L. Littman, and A. R. Cassandra. Planning and acting in partially observable stochastic domains. Artificial intelligence, 101(1):99–134, 1998. &lt;a href=&quot;#fnref:kaelbling1998planning&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:kamienny2020learning&quot;&gt;
      &lt;p&gt;P. Kamienny, M. Pirotta, A. Lazaric, T. Lavril, N. Usunier, and L. Denoyer. Learning adaptive exploration strategies in dynamic environments through informed policy regularization. arXiv preprint arXiv:2005.02934, 2020. &lt;a href=&quot;#fnref:kamienny2020learning&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:rakelly2019efficient&quot;&gt;
      &lt;p&gt;K. Rakelly, A. Zhou, D. Quillen, C. Finn, and S. Levine. Efficient off-policy meta-reinforcement learning via probabilistic context variables. arXiv preprint arXiv:1903.08254, 2019. &lt;a href=&quot;#fnref:rakelly2019efficient&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:thompson1933likelihood&quot;&gt;
      &lt;p&gt;W. R. Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3):285–294, 1933. &lt;a href=&quot;#fnref:thompson1933likelihood&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
              <pubDate>Wed, 26 Aug 2020 00:00:00 -0700</pubDate>
          </item>
          
        
          
          <item>
              <title>Stanford AI Lab Papers and Talks at ECCV 2020</title>
              <link>/blog/eccv-2020/</link>
              <guid isPermaLink="true">/blog/eccv-2020/</guid>
              <description>&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-08-23-eccv-2020/logo.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://eccv2020.eu/&quot;&gt;European Conference on Computer Vision&lt;/a&gt; (ECCV) 2020 is being hosted virtually from August 23rd - 28th. We’re excited to share all the work from SAIL that’s being presented, and you’ll find links to papers, videos and blogs below. Feel free to reach out to the contact authors directly to learn more about the work that’s happening at Stanford!&lt;/p&gt;

&lt;h2 id=&quot;list-of-accepted-papers&quot;&gt;List of Accepted Papers&lt;/h2&gt;
&lt;h4 id=&quot;contact-and-human-dynamics-from-monocular-video&quot;&gt;&lt;a href=&quot;https://geometry.stanford.edu/projects/human-dynamics-eccv-2020/content/contact-and-dynamics-2020.pdf&quot;&gt;Contact and Human Dynamics from Monocular Video&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-08-23-eccv-2020/img6.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Davis Rempe, Leonidas J. Guibas, Aaron Hertzmann, Bryan Russell, Ruben Villegas, Jimei Yang
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: drempe@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://geometry.stanford.edu/projects/human-dynamics-eccv-2020/content/contact-and-dynamics-2020.pdf&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://www.youtube.com/watch?v=qR9KW6JzXX4&quot;&gt;Video&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: 3d human pose, 3d human motion, pose estimation, dynamics, physics-based, contact, trajectory optimization, character animation, deep learning&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;curriculum-deepsdf&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2003.08593.pdf&quot;&gt;Curriculum DeepSDF&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-08-23-eccv-2020/img4.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Yueqi Duan, Haidong Zhu, He Wang, Li Yi, Ram Nevatia, Leonidas J. Guibas
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: duanyq19@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/pdf/2003.08593.pdf&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: shape representation, implicit function, deepsdf, curriculum learning&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;deformation-aware-3d-model-embedding-and-retrieval&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2004.01228.pdf&quot;&gt;Deformation-Aware 3D Model Embedding and Retrieval&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-08-23-eccv-2020/img2.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Mikaela Angelina Uy, Jingwei Huang, Minhyuk Sung, Tolga Birdal, Leonidas Guibas
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: mikacuy@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/pdf/2004.01228.pdf&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://www.youtube.com/watch?v=u_8DJ06SQdw&amp;amp;t&quot;&gt;Video&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: 3d model retrieval, deformation-aware embedding, non- metric embedding&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;generative-sparse-detection-networks-for-3d-single-shot-object-detection&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2006.12356&quot;&gt;Generative Sparse Detection Networks for 3D Single-shot Object Detection&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-08-23-eccv-2020/img11.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: JunYoung Gwak, Christopher Choy, Silvio Savarese
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: jgwak@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2006.12356&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://www.youtube.com/watch?v=g8UqlJZVnFo&quot;&gt;Video&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: single shot detection, 3d object detection, generative sparsenetwork, point cloud&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;learning-3d-part-assembly-from-a-single-image&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2003.09754&quot;&gt;Learning 3D Part Assembly from A Single Image&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-08-23-eccv-2020/img5.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Yichen Li, Kaichun Mo, Lin Shao, Minhyuk Sung, Leonidas Guibas
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: liyichen@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2003.09754&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://youtu.be/gtaBaEAs22s&quot;&gt;Video&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: 3d vision, vision for robotics, 3d representation&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;learning-predictive-models-from-observation-and-interaction&quot;&gt;&lt;a href=&quot;https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123650698.pdf&quot;&gt;Learning Predictive Models From Observation and Interaction&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-08-23-eccv-2020/img3.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Karl Schmeckpeper, Annie Xie, Oleh Rybkin, Stephen Tian, Kostas Daniilidis, Sergey Levine, Chelsea Finn
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: cbfinn@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123650698.pdf&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://www.youtube.com/watch?v=jWbwh4uZFgU&amp;amp;feature=emb_title&quot;&gt;Video&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: video prediction, visual planning, action representations, robotic manipulation&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;pt2pc-learning-to-generate-3d-point-cloud-shapes-from-part-tree-conditions&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2003.08624&quot;&gt;PT2PC: Learning to Generate 3D Point Cloud Shapes from Part Tree Conditions&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-08-23-eccv-2020/img7.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Kaichun Mo, He Wang, Xinchen Yan, Leonidas J. Guibas
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: kaichunm@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2003.08624&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://www.youtube.com/watch?v=GZGxaFx-kgw&quot;&gt;Video&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: 3d vision and graphics, generative adversarial network, 3d point cloud&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;pix2surf-learning-parametric-3d-surface-models-of-objects-from-images&quot;&gt;&lt;a href=&quot;https://geometry.stanford.edu/projects/pix2surf/pub/pix2surf.pdf&quot;&gt;Pix2Surf: Learning Parametric 3D Surface Models of Objects from Images&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-08-23-eccv-2020/img1.jpg&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Jiahui Lei, Srinath Sridhar, Paul Guerrero, Minhyuk Sung, Niloy Mitra, Leonidas J. Guibas
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: lei_jiahui@zju.edu.cn, ssrinath@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://geometry.stanford.edu/projects/pix2surf/pub/pix2surf.pdf&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://www.youtube.com/watch?v=jaxB0VSuvms&quot;&gt;Video&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: 3d reconstruction, multi-view, single-view, parametrization&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;quaternion-equivariant-capsule-networks-for-3d-point-clouds&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/1912.12098.pdf&quot;&gt;Quaternion Equivariant Capsule Networks for 3D Point Clouds&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-08-23-eccv-2020/img0.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Yongheng Zhao, Tolga Birdal, Jan Eric Lenssen, Emanuele Menegatti, Leonidas Guibas, Federico Tombari
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: tbirdal@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/pdf/1912.12098.pdf&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: equivariance, 3d point clouds, quaternion, weiszfeld algorithm, capsule networks, dynamic routing, riemannian&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;referit3d-neural-listeners-for-fine-grained-3d-object-identification-in-real-world-scenes&quot;&gt;&lt;a href=&quot;https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460409.pdf&quot;&gt;ReferIt3D: Neural Listeners for Fine-Grained 3D Object Identification in Real-World Scenes&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-08-23-eccv-2020/img12.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Panos Achlioptas, Ahmed Abdelreheem, Fei Xia, Mohamed Elhoseiny, Leonidas J. Guibas
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: panos@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460409.pdf&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://www.youtube.com/watch?v=yEdf24hF_sY&amp;amp;feature=emb_logo&quot;&gt;Video&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: 3d neural-listeners, spatial relations, object identification, referential language&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;robust-and-on-the-fly-dataset-denoising-for-image-classification&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2003.10647&quot;&gt;Robust and On-the-fly Dataset Denoising for Image Classification&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-08-23-eccv-2020/img9.jpg&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Jiaming Song, Yann Dauphin, Michael Auli, Tengyu Ma
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: tsong@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2003.10647&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: web supervision, noisy labels, robust data denoising&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;rubiksnet-learnable-3d-shift-for-efficient-video-action-recognition&quot;&gt;&lt;a href=&quot;https://stanfordvl.github.io/rubiksnet-site/assets/eccv20.pdf&quot;&gt;RubiksNet: Learnable 3D-Shift for Efficient Video Action Recognition&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-08-23-eccv-2020/img10.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Linxi Fan*, Shyamal Buch*, Guanzhi Wang, Ryan Cao, Yuke Zhu, Juan Carlos Niebles, Li Fei-Fei
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: {jimfan,shyamal}@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://stanfordvl.github.io/rubiksnet-site/assets/eccv20.pdf&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://youtu.be/3alaXltwEWw&quot;&gt;Video&lt;/a&gt; | &lt;a href=&quot;https://rubiksnet.stanford.edu&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: efficient action recognition, spatiotemporal, learnable shift, budget-constrained, video understanding&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;trajectron-dynamically-feasible-trajectory-forecasting-with-heterogeneous-data&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2001.03093&quot;&gt;Trajectron++: Dynamically-Feasible Trajectory Forecasting With Heterogeneous Data&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-08-23-eccv-2020/img8.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Tim Salzmann*, Boris Ivanovic*, Punarjay Chakravarty, Marco Pavone 
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: borisi@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2001.03093&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://ai.stanford.edu/blog/trajectory-forecasting/&quot;&gt;Blog Post&lt;/a&gt; 
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: trajectory forecasting, spatiotemporal graph modeling, human-robot interaction, autonomous driving&lt;/p&gt;
&lt;h4 id=&quot;trajectron-dynamically-feasible-trajectory-forecasting-with-heterogeneous-data-1&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2001.03093&quot;&gt;Trajectron++: Dynamically-Feasible Trajectory Forecasting With Heterogeneous Data&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-08-23-eccv-2020/img8.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Tim Salzmann*, Boris Ivanovic*, Punarjay Chakravarty, Marco Pavone 
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: borisi@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2001.03093&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://ai.stanford.edu/blog/trajectory-forecasting/&quot;&gt;Blog Post&lt;/a&gt; 
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: trajectory forecasting, spatiotemporal graph modeling, human-robot interaction, autonomous driving&lt;/p&gt;
&lt;h4 id=&quot;procedure-planning-in-instructional-videos&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/1907.01172&quot;&gt;Procedure Planning in Instructional Videos&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-08-23-eccv-2020/img13.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Chein-Yi Chang, De-An Huang, Danfei Xu, Ehsan Adeli, Li Fei-Fei, Juan Carlos Niebles
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: cy3@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/1907.01172&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;http://ai.stanford.edu/~cy3/publication/ppiv&quot;&gt;Website&lt;/a&gt; 
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: latent space planning, task planning, video understanding, representation for action and skill&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;We look forward to seeing you at ECCV 2020!&lt;/p&gt;
</description>
              <pubDate>Sun, 23 Aug 2020 00:00:00 -0700</pubDate>
          </item>
          
        
          
          <item>
              <title>Stanford AI Lab Papers and Talks at ICML 2020</title>
              <link>/blog/icml-2020/</link>
              <guid isPermaLink="true">/blog/icml-2020/</guid>
              <description>&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-07-11-icml-2020/logo.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://icml.cc/&quot;&gt;International Conference on Machine Learning&lt;/a&gt; (ICML) 2020 is being hosted virtually from July 13th - July 18th. We’re excited to share all the work from SAIL that’s being presented, and you’ll find links to papers, videos and blogs below. Feel free to reach out to the contact authors directly to learn more about the work that’s happening at Stanford!&lt;/p&gt;

&lt;h2 id=&quot;list-of-accepted-papers&quot;&gt;List of Accepted Papers&lt;/h2&gt;
&lt;h4 id=&quot;active-world-model-learning-in-agent-rich-environments-with-progress-curiosity&quot;&gt;Active World Model Learning in Agent-rich Environments with Progress Curiosity&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-07-11-icml-2020/img0&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Kuno Kim, Megumi Sano, Julian De Freitas, Nick Haber, Daniel Yamins
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: khkim@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://vimeo.com/389619940&quot;&gt;Video&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: curiosity, active learning, world models, animacy, attention&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;graph-structure-of-neural-networks&quot;&gt;Graph Structure of Neural Networks&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-07-11-icml-2020/img22&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Jiaxuan You, Jure Leskovec, Kaiming He, Saining Xie
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: jiaxuan@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/pdf/2007.06559.pdf&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: neural network design, network science, deep learning&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;a-distributional-framework-for-data-valuation&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2002.12334&quot;&gt;A Distributional Framework For Data Valuation&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-07-11-icml-2020/img11&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Amirata Ghorbani, Michael P. Kim, James Zou
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: jamesz@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2002.12334&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: shapley value, data valuation, machine learning, data markets&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;a-general-recurrent-state-space-framework-for-modeling-neural-dynamics-during-decision-making&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2001.04571&quot;&gt;A General Recurrent State Space Framework for Modeling Neural Dynamics During Decision-Making&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-07-11-icml-2020/img4&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: David Zoltowski, Jonathan Pillow, Scott Linderman
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: scott.linderman@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2001.04571&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: computational neuroscience, dynamical systems, variational inference&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;an-imitation-learning-approach-for-cache-replacement&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2006.16239&quot;&gt;An Imitation Learning Approach for Cache Replacement&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-07-11-icml-2020/img21&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Evan Zheran Liu, Milad Hashemi, Kevin Swersky, Parthasarathy Ranganathan, Junwhan Ahn
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: evanliu@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2006.16239&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: imitation learning, cache replacement, benchmark&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;an-investigation-of-why-overparameterization-exacerbates-spurious-correlations-&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2005.04345&quot;&gt;An Investigation of Why Overparameterization Exacerbates Spurious Correlations &lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-07-11-icml-2020/img13&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Shiori Sagawa*, Aditi Raghunathan*, Pang Wei Koh*, Percy Liang 
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: ssagawa@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2005.04345&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: robustness, spurious correlations, overparameterization&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;better-depth-width-trade-offs-for-neural-networks-through-the-lens-of-dynamical-systems&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2003.00777&quot;&gt;Better Depth-Width Trade-offs for Neural Networks through the Lens of Dynamical Systems.&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-07-11-icml-2020/img8&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Vaggos Chatziafratis, Sai Ganesh Nagarajan, Ioannis Panageas
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: vaggos@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2003.00777&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: expressivity, depth, width, dynamical systems&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;bridging-the-gap-between-f-gans-and-wasserstein-gans&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/1910.09779&quot;&gt;Bridging the Gap Between f-GANs and Wasserstein GANs&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-07-11-icml-2020/img18&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Jiaming Song, Stefano Ermon
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: jiaming.tsong@gmail.com
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/1910.09779&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: gans, generative models, f-divergence, wasserstein distance&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;concept-bottleneck-models&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2007.04612&quot;&gt;Concept Bottleneck Models&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-07-11-icml-2020/img30&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Pang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, Percy Liang
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: pangwei@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2007.04612&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: concepts, intervention, interpretability&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;domain-adaptive-imitation-learning&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/1910.00105&quot;&gt;Domain Adaptive Imitation Learning&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-07-11-icml-2020/img1&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Kuno Kim, Yihong Gu, Jiaming Song, Shengjia Zhao, Stefano Ermon
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: khkim@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/1910.00105&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: imitation learning, domain adaptation, reinforcement learning, generative adversarial networks, cycle consistency&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;encoding-musical-style-with-transformer-autoencoders&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/1912.05537&quot;&gt;Encoding Musical Style with Transformer Autoencoders&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-07-11-icml-2020/img15&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Kristy Choi, Curtis Hawthorne, Ian Simon, Monica Dinculescu, Jesse Engel
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: kechoi@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/1912.05537&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://magenta.tensorflow.org/transformer-autoencoder&quot;&gt;Blog Post&lt;/a&gt; | &lt;a href=&quot;https://icml.cc/virtual/2020/paper/5978&quot;&gt;Video&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: sequential, network, and time-series modeling; applications - music&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;fair-generative-modeling-via-weak-supervision&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/1910.12008&quot;&gt;Fair Generative Modeling via Weak Supervision&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-07-11-icml-2020/img16&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Kristy Choi, Aditya Grover, Trisha Singh, Rui Shu, Stefano Ermon
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: kechoi@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/1910.12008&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://icml.cc/virtual/2020/paper/6697&quot;&gt;Video&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: deep learning - generative models and autoencoders; fairness, equity, justice, and safety&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;fast-and-three-rious-speeding-up-weak-supervision-with-triplet-methods&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2002.11955&quot;&gt;Fast and Three-rious: Speeding Up Weak Supervision with Triplet Methods&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-07-11-icml-2020/img3&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Daniel Y. Fu, Mayee F. Chen, Frederic Sala, Sarah M. Hooper, Kayvon Fatahalian, Christopher Ré
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: danfu@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2002.11955&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;http://hazyresearch.stanford.edu/flyingsquid&quot;&gt;Blog Post&lt;/a&gt; | &lt;a href=&quot;https://www.youtube.com/watch?v=pHadwUKCoNE&quot;&gt;Video&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: weak supervision, latent variable models&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;flexible-and-efficient-long-range-planning-through-curious-exploration&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2004.10876&quot;&gt;Flexible and Efficient Long-Range Planning Through Curious Exploration&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-07-11-icml-2020/img2&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Aidan Curtis, Minjian Xin, Dilip Arumugam, Kevin Feigelis, Daniel Yamins
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: yamins@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2004.10876&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://neuroailab.github.io/CuriousSamplePlanner/&quot;&gt;Blog Post&lt;/a&gt; | &lt;a href=&quot;https://youtu.be/7DSW8Dy9ADQ&quot;&gt;Video&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: planning, deep learning, sparse reinforcement learning, curiosity&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;formulazero-distributionally-robust-online-adaptation-via-offline-population-synthesis&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2003.03900&quot;&gt;FormulaZero: Distributionally Robust Online Adaptation via Offline Population Synthesis&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-07-11-icml-2020/img17&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Aman Sinha, Matthew O’Kelly, Hongrui Zheng, Rahul Mangharam, John Duchi, Russ Tedrake
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: amans@stanford.edu, mokelly@seas.upenn.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2003.03900&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://youtu.be/7Yat9FZzE4g&quot;&gt;Video&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: distributional robustness, online learning, autonomous driving, reinforcement learning, simulation, mcmc&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;goal-aware-prediction-learning-to-model-what-matters&quot;&gt;&lt;a href=&quot;https://proceedings.icml.cc/static/paper_files/icml/2020/2981-Paper.pdf&quot;&gt;Goal-Aware Prediction: Learning to Model what Matters&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-07-11-icml-2020/img28&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Suraj Nair, Silvio Savarese, Chelsea Finn
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: surajn@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://proceedings.icml.cc/static/paper_files/icml/2020/2981-Paper.pdf&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: reinforcement learning, visual planning, robotics&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;graph-based-self-supervised-program-repair-from-diagnostic-feedback&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2005.10636&quot;&gt;Graph-based, Self-Supervised Program Repair from Diagnostic Feedback&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-07-11-icml-2020/img14&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Michihiro Yasunaga, Percy Liang
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: myasu@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2005.10636&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://cs.stanford.edu/~myasu/files/DrRepair_slides.pdf&quot;&gt;Blog Post&lt;/a&gt; | &lt;a href=&quot;https://icml.cc/virtual/2020/paper/6722#&quot;&gt;Video&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: program repair, program synthesis, self-supervision, pre-training, graph&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;interpretable-off-policy-evaluation-in-reinforcement-learning-by-highlighting-influential-transitions&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2002.03478&quot;&gt;Interpretable Off-Policy Evaluation in Reinforcement Learning by Highlighting Influential Transitions&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-07-11-icml-2020/img24&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Omer Gottesman, Joseph Futoma, Yao Liu, Sonali Parbhoo, Leo Anthony Celi, Emma Brunskill, Finale Doshi-Velez
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: gottesman@fas.harvard.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2002.03478&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: reinforcement learning, off-policy evaluation, interpretability&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;learning-near-optimal-policies-with-low-inherent-bellman-error&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2003.00153&quot;&gt;Learning Near Optimal Policies with Low Inherent Bellman Error&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-07-11-icml-2020/img20&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Andrea Zanette, Alessandro Lazaric, Mykel Kochenderfer, Emma Brunskill
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: zanette@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2003.00153&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: reinforcement learning, exploration, function approximation&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;maximum-likelihood-with-bias-corrected-calibration-is-hard-to-beat-at-label-shift-domain-adaptation&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/1901.06852&quot;&gt;Maximum Likelihood With Bias-Corrected Calibration is Hard-To-Beat at Label Shift Domain Adaptation&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-07-11-icml-2020/img34&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Amr Alexandari*, Anshul Kundaje†, Avanti Shrikumar*† (*co-first †co-corresponding) 
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: avanti@cs.stanford.edu, amr.alexandari@gmail.com, akundaje@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/1901.06852&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://kundajelab.github.io/publication/2020/07/08/max-likelihood-with-bias-corrected-calibration-label-shift-domain-adaptation.html&quot;&gt;Blog Post&lt;/a&gt; | &lt;a href=&quot;https://www.youtube.com/watch?v=ZBXjE9QTruE&quot;&gt;Video&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: domain adaptation, label shift, calibration, maximum likelihood&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;ngboost-natural-gradient-boosting-for-probabilistic-prediction&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/1910.03225&quot;&gt;NGBoost: Natural Gradient Boosting for Probabilistic Prediction&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-07-11-icml-2020/img7&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Tony Duan*, Anand Avati*, Daisy Yi Ding, Sanjay Basu, Andrew Ng, Alejandro Schuler
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: avati@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/1910.03225&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: gradient boosting, uncertainty estimation, natural gradient&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;on-the-expressivity-of-neural-networks-for-deep-reinforcement-learning&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/1910.05927&quot;&gt;On the Expressivity of Neural Networks for Deep Reinforcement Learning&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-07-11-icml-2020/img33&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Kefan Dong, Yuping Luo, Tianhe Yu, Chelsea Finn, Tengyu Ma
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: kefandong@gmail.com
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/1910.05927&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: reinforcement learning&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;on-the-generalization-effects-of-linear-transformations-in-data-augmentation&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2005.00695&quot;&gt;On the Generalization Effects of Linear Transformations in Data Augmentation&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-07-11-icml-2020/img5&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Sen Wu, Hongyang Zhang, Gregory Valiant, Christopher Ré
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: senwu@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2005.00695&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;http://hazyresearch.stanford.edu/data-aug-part-3&quot;&gt;Blog Post&lt;/a&gt; | &lt;a href=&quot;Waiting for ICML to release the video link&quot;&gt;Video&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: data augmentation, generalization&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;predictive-coding-for-locally-linear-control&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2003.01086&quot;&gt;Predictive Coding for Locally-Linear Control&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-07-11-icml-2020/img32&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Rui Shu*, Tung Nguyen*, Yinlam Chow, Tuan Pham, Khoat Than, Mohammad Ghavamzadeh, Stefano Ermon, Hung Bui
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: ruishu@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2003.01086&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://youtu.be/BTU3rWLXTjY&quot;&gt;Video&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: representation learning, information theory, generative models, planning, control&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;robustness-to-spurious-correlations-via-human-annotations&quot;&gt;&lt;a href=&quot;https://cs.stanford.edu/~megha/papers/icml2020.pdf&quot;&gt;Robustness to Spurious Correlations via Human Annotations&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-07-11-icml-2020/img31&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Megha Srivastava, Tatsunori Hashimoto, Percy Liang
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: megha@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://cs.stanford.edu/~megha/papers/icml2020.pdf&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: robustness, distribution shift, crowdsourcing, human-in-the-loop&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;sample-amplification-increasing-dataset-size-even-when-learning-is-impossible&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/1904.12053&quot;&gt;Sample Amplification: Increasing Dataset Size even when Learning is Impossible&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-07-11-icml-2020/img19&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Brian Axelrod, Shivam Garg, Vatsal Sharan, Gregory Valiant
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: shivamgarg@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/1904.12053&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://slideslive.com/38923099/contributed-talk-sample-ampification-increasing-dataset-size-even-when-learning-is-impossible&quot;&gt;Video&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: learning theory, sample amplification, generative models&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;scalable-identification-of-partially-observed-systems-with-certainty-equivalent-em&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2006.11615&quot;&gt;Scalable Identification of Partially Observed Systems with Certainty-Equivalent EM&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-07-11-icml-2020/img27&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Kunal Menda, Jean de Becdelièvre, Jayesh K. Gupta, Ilan Kroo, Mykel J. Kochenderfer, Zachary Manchester
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: kmenda@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2006.11615&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://youtu.be/WzXO--ohv3g&quot;&gt;Video&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: system identification; time series and sequence models&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;the-implicit-and-explicit-regularization-effects-of-dropout&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2002.12915&quot;&gt;The Implicit and Explicit Regularization Effects of Dropout&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-07-11-icml-2020/img29&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Colin Wei, Sham Kakade, Tengyu Ma
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: colinwei@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2002.12915&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: dropout, deep learning theory, implicit regularization&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;training-deep-energy-based-models-with-f-divergence-minimization&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2003.03463&quot;&gt;Training Deep Energy-Based Models with f-Divergence Minimization&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-07-11-icml-2020/img6&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Lantao Yu, Yang Song, Jiaming Song, Stefano Ermon
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: lantaoyu@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2003.03463&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: energy-based models; f-divergences; deep generative models&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;two-routes-to-scalable-credit-assignment-without-weight-symmetry&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2003.01513&quot;&gt;Two Routes to Scalable Credit Assignment without Weight Symmetry&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-07-11-icml-2020/img12&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Daniel Kunin*, Aran Nayebi*, Javier Sagastuy-Brena*, Surya Ganguli, Jonathan M. Bloom, Daniel L. K. Yamins
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: jvrsgsty@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2003.01513&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://www.youtube.com/watch?v=fC_E0dO3Rfo&quot;&gt;Video&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: learning rules, computational neuroscience, machine learning&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;understanding-self-training-for-gradual-domain-adaptation&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2002.11361&quot;&gt;Understanding Self-Training for Gradual Domain Adaptation&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-07-11-icml-2020/img25&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Ananya Kumar, Tengyu Ma, Percy Liang
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: ananya@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2002.11361&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://icml.cc/virtual/2020/paper/6815&quot;&gt;Video&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: domain adaptation, self-training, semi-supervised learning&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;understanding-and-mitigating-the-tradeoff-between-robustness-and-accuracy&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2002.10716&quot;&gt;Understanding and Mitigating the Tradeoff between Robustness and Accuracy&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-07-11-icml-2020/img26&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Aditi Raghunathan*, Sang Michael Xie*, Fanny Yang, John C. Duchi, Percy Liang
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: aditir@stanford.edu, xie@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2002.10716&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://youtu.be/SkKFAY0GiZk&quot;&gt;Video&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: adversarial examples, adversarial training, robustness, accuracy, tradeoff, robust self-training&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;understanding-the-curse-of-horizon-in-off-policy-evaluation-via-conditional-importance-sampling&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/1910.06508&quot;&gt;Understanding the Curse of Horizon in Off-Policy Evaluation via Conditional Importance Sampling&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-07-11-icml-2020/img23&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Yao Liu, Pierre-Luc Bacon, Emma Brunskill
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: yaoliu@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/1910.06508&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: reinforcement learning, off-policy evaluation, importance sampling&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;visual-grounding-of-learned-physical-models&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2004.13664&quot;&gt;Visual Grounding of Learned Physical Models&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-07-11-icml-2020/img35&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Yunzhu Li, Toru Lin*, Kexin Yi*, Daniel M. Bear, Daniel L. K. Yamins, Jiajun Wu, Joshua B. Tenenbaum, Antonio Torralba
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: liyunzhu@mit.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2004.13664&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://youtu.be/P_LrG0lzc-0&quot;&gt;Video&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: intuitive physics, visual grounding, physical reasoning&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;learning-to-simulate-complex-physics-with-graph-networks&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2002.09405&quot;&gt;Learning to Simulate Complex Physics with Graph Networks&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-07-11-icml-2020/img9&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Alvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure Leskovec, Peter W. Battaglia
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: rexying@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2002.09405&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: simulation, graph neural networks&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;coresets-for-data-efficient-training-of-machine-learning-models&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/1906.01827&quot;&gt;Coresets for Data-Efficient Training of Machine Learning Models&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-07-11-icml-2020/img36&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Baharan Mirzasoleiman, Jeff Bilmes, Jure Leskovec
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: baharanm@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/1906.01827&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://icml.cc/virtual/2020/paper/6325&quot;&gt;Video&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: Coresets, Data-efficient training, Submodular optimization, Incremental gradient methods&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;which-tasks-should-be-learned-together-in-multi-task-learning&quot;&gt;&lt;a href=&quot;http://taskgrouping.stanford.edu/&quot;&gt;Which Tasks Should be Learned Together in Multi-Task Learning&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-07-11-icml-2020/img37&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Trevor Standley, Amir Zamir, Dawn Chen, Leonidas Guibas, Jitendra Malik, Silvio Savarese
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: tstand@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/1905.07553&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://www.youtube.com/watch?v=qCRdrczbqUo&quot;&gt;Video&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: machine learning, multi-task learning, computer vision&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;accelerated-message-passing-for-entropy-regularized-map-inference&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2007.00699&quot;&gt;Accelerated Message Passing for Entropy-Regularized MAP Inference&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-07-11-icml-2020/img38&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Jonathan N. Lee, Aldo Pacchiano, Peter Bartlett, Michael I. Jordan
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: jnl@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2007.00699&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: graphical models, map inference, message passing, optimization&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;on-learning-sets-of-symmetric-elements&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2002.08599&quot;&gt;On Learning Sets of Symmetric Elements&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-07-11-icml-2020/img39&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Haggai Maron, Or Litany, Gal Chechik, Ethan Fetaya
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: or.litany@gmail.com
&lt;br /&gt;&lt;strong&gt;Links&lt;/strong&gt;: &lt;a href=&quot;https://arxiv.org/abs/2002.08599&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: equivariance, sets, pointclouds, graphs
&lt;br /&gt;&lt;b&gt;Outstanding Paper Award&lt;/b&gt;&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;individual-calibration-with-randomized-forecasting&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2006.10288&quot;&gt;Individual Calibration with Randomized Forecasting&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2020-07-11-icml-2020/img40&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Shengjia Zhao, Tengyu Ma, Stefano Ermon
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: sjzhao@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links&lt;/strong&gt;: &lt;a href=&quot;https://arxiv.org/abs/2006.10288&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: calibration, uncertainty estimation&lt;/p&gt;
&lt;hr /&gt;

&lt;p&gt;We look forward to seeing you at ICML 2020!&lt;/p&gt;
</description>
              <pubDate>Sat, 11 Jul 2020 00:00:00 -0700</pubDate>
          </item>
          
        
    </channel>
</rss>
