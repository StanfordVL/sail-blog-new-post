<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <!-- Main JS (navbar.js, katex_init.js and masonry_init.js)-->
    <script defer=true src="/blog/assets/js/main.min.js"></script>
    
    <!-- CSS -->
    <link rel="stylesheet" href="/blog/assets/css/main.css">

    <!--Favicon-->
    <link rel="icon" type="image/png" href="/blog/assets/img/favicon-32x32.png" sizes="32x32" />
    <link rel="icon" type="image/png" href="/blog/assets/img/favicon-16x16.png" sizes="16x16" />

    <!-- Canonical -->
    <link rel="canonical" href="http://0.0.0.0:4000/blog/minimax-optimal-pac/">

    <!-- RSS -->
    <link rel="alternate" type="application/atom+xml" title="The Stanford AI Lab Blog" href="http://0.0.0.0:4000/blog/feed.xml"/>

    <!-- Font Awesome -->
    <!-- <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet"> -->
    <link rel="stylesheet" type="text/css" href="/blog/assets/css/vendor/font-awesome.min.css">
    
    <!-- Bootstrap-3.3.7 isolation CSS -->
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" integrity="sha384-JcKb8q3iqJ61gNV9KGb8thSsNjpSL0n8PARn9HuZOnIxN0hoP+VmmDGMN5t9UJ0Z" crossorigin="anonymous">


    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js" integrity="sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha384-B4gt1jrGC7Jh4AgTPSdUtOBvfO8shuf57BaghqFfPlYxofvL8/KUEfYiJOMMV+rV" crossorigin="anonymous"></script>
    
    <!-- JQuery 2.2.4 -->
    <script src="https://code.jquery.com/jquery-2.2.4.min.js" integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin="anonymous"></script>
    
    <link rel="stylesheet" type="text/css" href="/blog/assets/css/vendor/bigfoot-number.css">
    <script type="text/javascript" src="/blog/assets/js/vendor/bigfoot.min.js"></script>
    <script type="text/javascript">
    $(document).ready(function() { 
      $.bigfoot();
      
    window.onload = function() {

        var videos = document.getElementsByTagName("video"),
            fraction = 0.8;

        function checkScroll() {

            for (var i = 0; i < videos.length; i++) {

                var video = videos[i];

                var x = video.offsetLeft,
                    y = video.offsetTop,
                    w = video.offsetWidth,
                    h = video.offsetHeight,
                    r = x + w, //right
                    b = y + h, //bottom
                    visibleX, visibleY, visible;

                visibleX = Math.max(0, Math.min(w, window.pageXOffset + window.innerWidth - x, r - window.pageXOffset));
                visibleY = Math.max(0, Math.min(h, window.pageYOffset + window.innerHeight - y, b - window.pageYOffset));

                visible = visibleX * visibleY / (w * h);

                if (visible > fraction) {
                    video.play();
                } else {
                    video.pause();
                }

            }

        }

        window.addEventListener('scroll', checkScroll, false);
        window.addEventListener('resize', checkScroll, false);
    }
    }); 
    </script>
    


    <!-- Google Fonts -->
    
    <link href="//fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,700italic,400italic" rel="stylesheet" type="text/css"> 
    

    <!-- KaTeX 0.8.3 -->
    
    <link rel="stylesheet" type="text/css" href="/blog/assets/css/vendor/katex.min.css">
    <script src="/blog/assets/js/vendor/katex.min.js">
    </script>
    
    
    
    <!-- Google Analytics -->
    
    <script>
        (function(i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r;
            i[r] = i[r] || function() {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date();
            a = s.createElement(o),
                m = s.getElementsByTagName(o)[0];
            a.async = 1;
            a.src = g;
            m.parentNode.insertBefore(a, m)
        })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'UA-129018108-1', 'auto');
        ga('send', 'pageview');

    </script>
    
    
    <!-- seo tags -->
    <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Policy Certificates and Minimax-Optimal PAC Bounds for Episodic Reinforcement Learning | SAIL Blog</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Policy Certificates and Minimax-Optimal PAC Bounds for Episodic Reinforcement Learning" />
<meta name="author" content="<a href='https://cdann.net/'>Christoph Dann</a>" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Figure 1: Comparison of existing algorithms without policy certificates (top) and with our proposed policy certificates (bottom). While in existing reinforcement learning the user has no information about how well the algorithm will perform in the next episode, we propose that algorithms output policy certificates before playing an episode to allow users to intervene if necessary." />
<meta property="og:description" content="Figure 1: Comparison of existing algorithms without policy certificates (top) and with our proposed policy certificates (bottom). While in existing reinforcement learning the user has no information about how well the algorithm will perform in the next episode, we propose that algorithms output policy certificates before playing an episode to allow users to intervene if necessary." />
<link rel="canonical" href="http://0.0.0.0:4000/blog/minimax-optimal-pac/" />
<meta property="og:url" content="http://0.0.0.0:4000/blog/minimax-optimal-pac/" />
<meta property="og:site_name" content="SAIL Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-08-28T00:00:00-07:00" />
<script type="application/ld+json">
{"description":"Figure 1: Comparison of existing algorithms without policy certificates (top) and with our proposed policy certificates (bottom). While in existing reinforcement learning the user has no information about how well the algorithm will perform in the next episode, we propose that algorithms output policy certificates before playing an episode to allow users to intervene if necessary.","author":{"@type":"Person","name":"<a href='https://cdann.net/'>Christoph Dann</a>"},"@type":"BlogPosting","url":"http://0.0.0.0:4000/blog/minimax-optimal-pac/","headline":"Policy Certificates and Minimax-Optimal PAC Bounds for Episodic Reinforcement Learning","dateModified":"2019-08-28T00:00:00-07:00","datePublished":"2019-08-28T00:00:00-07:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://0.0.0.0:4000/blog/minimax-optimal-pac/"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    
    <!-- Manual seo tags -->
    <title>Policy Certificates and Minimax-Optimal PAC Bounds for Episodic Reinforcement Learning | The Stanford AI Lab Blog</title>
    <meta name="description" content="Introducing a new method that achieves minimax-optimal probably approximately correct (and regret) bounds which match the statistical worst-case lower bounds...">
    
    <!-- Twitter Cards -->
    <meta name="twitter:title" content="Policy Certificates and Minimax-Optimal PAC Bounds for Episodic Reinforcement Learning">
    
    <meta name="twitter:description" content="Introducing a new method that achieves minimax-optimal probably approximately correct (and regret) bounds which match the statistical worst-case lower bounds in the dominating terms for reinforcement learning.">
    
    <meta name="twitter:creator" content="@StanfordAILab">
    
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:image" content="http://0.0.0.0:4000/blog/assets/img/posts/2019-08-28-minimax-optimal-pac/feature.png">
    <meta name="og:image" content="http://0.0.0.0:4000/blog/assets/img/posts/2019-08-28-minimax-optimal-pac/feature.png">
    
</head>

  <body class="post-body">
      <!-- Toggle menu -->
<header class="site-header">

<nav class="clear navbar navbar-expand-lg navbar-light bg-white flex-column flex-md-row bd-navbar fixed-top" id="main_nav">
  
  <div class="container">

    <a class="navbar-brand mr-0 mr-md-2 text-black d-flex align-items-center" href="/blog/" aria-label="Bootstrap">
    
	  <div class="branding">
	    <a href="http://ai.stanford.edu/">
	      <img class="avatar" src="/blog/assets/img/sail-logo.png" alt=""/>
		  </a>

      <a href="/blog/">
	      <h1 class="site-title">
			    The Stanford AI Lab Blog
		    </h1> 
		  </a>
	  </div>
    
    
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
    </button>
    
    <div class="collapse navbar-collapse" id="navbarResponsive">

      <ul class="navbar-nav ml-auto">
      
      
      <li class="nav-item">
      <a class="nav-link" href="/blog/about">About</a>
      </li>
      
      <li class="nav-item dropdown">
        <a class="nav-link dropdown-toggle" data-toggle="dropdown" href="#" role="button" aria-haspopup="true" aria-expanded="false">Posts</a>
        <div class="dropdown-menu">
        <a class="dropdown-item" href="/blog/">All</a>
        <a class="dropdown-item" href="/blog/conferences">Conferences</a>
        <a class="dropdown-item" href="/blog/vision">Computer Vision</a>
        <a class="dropdown-item" href="/blog/robotics">Robotics</a>
        <a class="dropdown-item" href="/blog/nlp">NLP</a>
        <a class="dropdown-item" href="/blog/ml">Machine Learning</a>
        <a class="dropdown-item" href="/blog/rl">Reinforcement Learning</a>

        </div>
      </li>
        
      
      <li class="nav-item">
      <a class="nav-link" href="/blog/subscribe">Subscribe</a>
      </li>
      
      
      <li class="nav-item">
      <a class="nav-link" href="http://ai.stanford.edu/">SAIL</a>
      </li>
      
      </ul> 

    </div>

  </div>
</nav>

</header>

  
    <div class="content">
      

<article>

  <header id="main">
    
    <h1 id="post_title">Policy Certificates and Minimax-Optimal PAC Bounds for Episodic Reinforcement Learning</h1>
    <p class="meta">
    <a href='https://cdann.net/'>Christoph Dann</a>
    <div class="post-date">August 28, 2019</div>
    </p>
  <hr>
  </header>


  <section class="post-content">
  
    <figure class="figure"><div class="figure__main">
<p><img src="/blog/assets/img/posts/2019-08-28-minimax-optimal-pac/fig1.png" /></p>
<figcaption>
Figure 1: Comparison of existing algorithms without policy certificates (top) and with our proposed policy certificates (bottom). While in existing reinforcement learning the user has no information about how well the algorithm will perform in the next episode, we propose that algorithms output policy certificates before playing an episode to allow users to intervene if necessary.
</figcaption>
</div></figure>

<p>Designing reinforcement learning methods which find a good policy with as few samples as possible is a key goal of both empirical and theoretical research. On the theoretical side there are two main ways, regret- or PAC (probably approximately correct) bounds, to measure and guarantee sample-efficiency of a method. Ideally, we would like to have algorithms that have good performance according to both criteria, as they measure different aspects of sample efficiency and we have <a href="https://medium.com/stanford-aiforhi/policy-certificates-and-minimax-optimal-pac-bounds-for-episodic-reinforcement-learning-90668b489c40#a6fd">shown previously [1]</a> that one cannot simply go from one to the other. In a specific setting called tabular episodic MDPs, a recent algorithm achieved close to <a href="https://medium.com/stanford-aiforhi/policy-certificates-and-minimax-optimal-pac-bounds-for-episodic-reinforcement-learning-90668b489c40#1c5e">optimal regret bounds [2]</a> but there was no methods known to be close to optimal according to the PAC criterion despite a long line of research. In <a href="https://medium.com/stanford-aiforhi/policy-certificates-and-minimax-optimal-pac-bounds-for-episodic-reinforcement-learning-90668b489c40#f514">our work</a> presented at ICML 2019, we close this gap with a new method that achieves <strong>minimax-optimal PAC (and regret) bounds which match the statistical worst-case lower bounds</strong> in the dominating terms.</p>

<p>Interestingly, we achieve this by addressing a general issue of PAC and regret bounds which is that they do not reveal <em>when</em> an algorithm will potentially take bad actions (only e.g. how often). This issue leads to a lack of accountability that could be particularly problematic in high-stakes applications (see a motivational scenario in Figure 2).</p>

<p>Besides being sample-efficient, our algorithm also does not suffer from this lack of accountability because it outputs what we call <em>policy certificates</em>. Policy certificates are confidence intervals around the current expected return of the algorithm and optimal return given to us by the algorithm before each episode (see Figure 1). This information allows users of our algorithms to intervene if the certified performance is not deemed adequate. We accompany this algorithm with a new type of learning guarantee called IPOC that is stronger than PAC, regret and the <a href="https://medium.com/stanford-aiforhi/policy-certificates-and-minimax-optimal-pac-bounds-for-episodic-reinforcement-learning-90668b489c40#a6fd">recent Uniform-PAC [1]</a> as it ensures not only sample-efficiency but also the tightness of policy certificates. We primarily consider the simple tabular episodic setting where there is only a small number of possible states and actions. While this is often not the case in practical applications, we believe that the insights developed in this work can potentially be used to design more sample-efficient and accountable reinforcement learning methods for challenging real-world problems with rich observations like images or text.</p>

<figure class="figure"><div class="figure__main">
<p><img src="/blog/assets/img/posts/2019-08-28-minimax-optimal-pac/fig2.png" /></p>
<figcaption>
Figure 2: Existing learning guarantees in reinforcement learning like PAC and regret bounds guarantee the performance of an algorithm over many episodes. They do not tell us when — in which episodes — an algorithm performs badly and only allow us to make statements like the one on the left. In contrast, policy certificates certify the performance for an individual episode in combination with our new IPOC guarantees, we can make statements on the right. (Image adapted from <a href="https://commons.wikimedia.org/wiki/File:Doctor_with_Patient_Cartoon.svg##">Doctor with Patient Cartoon.svg</a> from <a href="https://commons.wikimedia.org/wiki/Main_Page">Wikimedia Commons</a> by <a href="https://videoplasty.com/">Videoplasty.com</a>, <a href="https://creativecommons.org/licenses/by-sa/4.0/deed.en">CC-BY-SA 4.0</a>)
</figcaption>
</div></figure>

<p>We propose to make methods for episodic reinforcement learning more accountable by having them output a <em>policy certificate</em> before each episode. A policy certificate is a confidence interval [<em>l, u</em>]. This interval contains both the expected sum of rewards of the algorithm’s policy in the next episode and the optimal expected sum of rewards in the next episode (see Figure 1 for an illustration). As such, a policy certificate helps answer two questions which are of interest in many applications:</p>

<ul>
  <li><em>What return is the algorithm’s policy expected to achieve in the next episode?</em> — At least the lower end of the interval <em>l</em>.</li>
  <li><em>How far from optimal is the algorithm’s policy in the next episode?</em> — At most the length of the interval <em>u-l</em></li>
</ul>

<p>Policy certificates are only useful if these confidence intervals are not too loose. To ensure this, we introduce a type of guarantee for algorithms with policy certificates <em>IPOC (Individual POlicy Certificates) bounds</em>. These bounds guarantee that all certificates are <em>valid</em> confidence intervals and bound the number of times their length can exceed any given threshold. IPOC bounds guarantee both the sample-efficiency of policy learning and the accuracy of policy certificates. That means the algorithm has to play better and better policies but also needs to tell us more accurately how good these policies are. IPOC bounds are stronger than existing learning bounds such as PAC or regret (see Figure 3) and imply that the algorithm is anytime interruptible (see paper for details).</p>

<figure class="figure"><div class="figure__main">
<p><img src="/blog/assets/img/posts/2019-08-28-minimax-optimal-pac/fig3.png" /></p>
<figcaption>
Figure 3: Our (Mistake) IPOC bounds imply bounds in existing frameworks for finite-sample learning guarantees that are similarly tight (see paper for details).
</figcaption>
</div></figure>

<p>Policy certificates are not limited to specific types of algorithms but optimistic algorithms are particularly natural to extend to output policy certificates. These methods give us the upper end of certificates “for free” as they maintain an upper confidence bound <em>U</em>(s,a) on the optimal value function Q*(s,a) and follow the greedy policy π with respect to this upper confidence bound. In similar fashion, we can compute a lower confidence bound L(s,a) of the Q-function Q(s,a) of this greedy policy. The certificate for this policy is then just these confidence bounds evaluated at the initial state s₁ of the episode [<em>l, u</em>] = [<em>L</em>(s₁, π(s₁)), <em>U</em>(s₁, π(s₁)]</p>

<p>We demonstrate this principle with a new algorithm called ORLC (Optimistic RL with Certificates) for tabular MDPs. Similar to existing optimistic algorithms like <a href="https://medium.com/stanford-aiforhi/policy-certificates-and-minimax-optimal-pac-bounds-for-episodic-reinforcement-learning-90668b489c40#1c5e">UCBVI [2]</a> and <a href="https://medium.com/stanford-aiforhi/policy-certificates-and-minimax-optimal-pac-bounds-for-episodic-reinforcement-learning-90668b489c40#a6fd">UBEV [1]</a>, it computes the confidence bounds <em>U</em>(s,a) by optimistic value iteration on an estimated model but also computes lower confidence bounds <em>L</em>(s,a) with a pessimistic version of value iteration. These procedures are similar to vanilla value iteration but add optimism bonuses or subtract pessimism bonuses in each time step respectively to ensure high confidence bounds.</p>

<p>Interestingly, we found that computing lower confidence bounds for policy certificates can also improve sample-efficiency of policy learning. More concretely, we could tighten the optimism bonuses in our tabular method ORLC using the lower bounds <em>L</em>(s,a). This makes the algorithm less conservative and able to adjust more quickly to observed data. As a result, we were able to prove <strong>the first PAC bounds for tabular MDPs that are minimax-optimal in the dominating term</strong>:</p>

<h2 id="theorem-minimax-ipoc-mistake-pac-and-regret-bound-of-orlc">Theorem: Minimax IPOC Mistake, PAC and regret bound of ORLC</h2>

<p><em>In any episodic MDP with S states, A actions and an episode length H, the algorithm ORLC satisfies the IPOC Mistake bound below. That is, with probability at least 1-δ, all certificates are valid confidence intervals and for all ε &gt; 0 ORLC outputs certificates larger than ε in at most</em></p>

<figure class="figure"><div class="figure__main">
<p><img src="/blog/assets/img/posts/2019-08-28-minimax-optimal-pac/fig4.png" /></p>
</div></figure>

<p><em>episodes. This immediately implies that the bound above is a (Uniform-)PAC bound and that ORLC satisfies a high-probability regret bound for all number of episodes T of</em></p>

<figure class="figure"><div class="figure__main">
<p><img src="/blog/assets/img/posts/2019-08-28-minimax-optimal-pac/fig5.png" /></p>
</div></figure>

<p>Comparing the order of our PAC bounds against the statistical lower bounds and prior state of the art PAC and regret bounds in the table below, this is the first time the optimal polynomial dependency of SAH² has been achieved in the dominating 1/<em>ε</em>² term. Our bounds also improve the prior regret bounds of UCBVI by avoiding their √(H³T) terms, making our bounds minimax-optimal even when the episode length H is large.</p>

<figure class="figure"><div class="figure__main">
<p><img src="/blog/assets/img/posts/2019-08-28-minimax-optimal-pac/fig6.png" /></p>
<figcaption>
Comparison of prior PAC and regret bounds for this setting. Constants and log-terms are omitted for readability.
</figcaption>
</div></figure>

<p>As mentioned above, our algorithm achieves this new IPOC guarantee and improved PAC bounds by maintaining a lower confidence bound <em>L</em>(s,a) of the Q-function <em>Q</em>(s,a) of its current policy at all times in addition to the usual upper confidence bound <em>U</em>(s,a) on the optimal value function <em>Q</em><em>(s,a). Deriving tight lower confidence bounds _L_(s,a) requires new techniques compared to those for upper confidence bounds . All recent optimistic algorithms for tabular MDPs leverage for their upper confidence bounds that _U_ is a confidence bound on _Q_</em> which does not depend on the samples. The optimal Q-function is always the same, no matter what samples the algorithm saw. We cannot leverage the same insight for our lower confidence bounds because the Q-function of the current policy Q does depend on the samples the algorithm saw. After all, the policy π is computed as a function of these samples. We develop a technique that allows us to deal with this challenge by explicitly incorporating both upper and lower confidence bounds in our bonus terms. It turns out that this technique not only helps achieving tighter lower confidence bounds but also tighter upper-confidence bounds. This is the key for our improved PAC and regret bounds.</p>

<p>Our work provided the final ingredient for PAC bounds for episodic tabular MDPs that are minimax-optimal up to lower-order terms and also established the foundation for policy certificates. In the full paper, we also considered more general MDPs and designed a policy certificate algorithm for so-called finite MDPs with linear side information. This is a generalization of the popular linear contextual bandit setting and requires function approximation. In the future, we plan to investigate policy certificates as a useful empirical tool for deep reinforcement learning techniques and examine whether the specific form of optimism bonuses derived in this work can inspire more sample-efficient exploration bonuses in deep RL methods.</p>

<hr />

<p>This post is also featured on the <a href="https://blog.ml.cmu.edu/">ML@CMU</a> blog and is based on the work in the following paper:</p>

<p>Christoph Dann, Lihong Li, Wei Wei, Emma Brunskill
<a href="https://arxiv.org/abs/1811.03056"><strong>Policy Certificates: Towards Accountable Reinforcement Learning</strong></a>
<em>International Conference on Machine Learning (ICML) 2019</em></p>

<p>Other works mentioned in this post:</p>

<p>[1] Dann, Lattimore, Brunskill — <a href="https://arxiv.org/abs/1703.07710">Unifying PAC and Regret: Uniform PAC Bounds for Episodic Reinforcement Learning</a> (NeurIPS 2017)</p>

<p>[2] Azar, Osband, Munos — <a href="https://arxiv.org/abs/1703.05449">Minimax Regret Bounds for Reinforcement Learning</a> (ICML 2017)</p>

<p>[3] Dann, Brunskill — <a href="https://arxiv.org/abs/1510.08906">Sample Complexity of Episodic Fixed-Horizon Reinforcement Learning</a> (NeurIPS 2015)</p>


  
  </section>
  <hr>
  Keep on top of the latest SAIL Blog posts via <a class="social-icon" href="http://0.0.0.0:4000/blog/feed.xml">RSS <i class="fa fa-rss-square fa fa" title="Twitter"></i></a>, <a class="social-icon" href="https://twitter.com/StanfordAILab">Twitter <i class="fa fa-twitter-square fa fa" title="Twitter"></i></a>, or email:

<!-- Begin Mailchimp Signup Form -->
<link href="//cdn-images.mailchimp.com/embedcode/horizontal-slim-10_7.css" rel="stylesheet" type="text/css">
<style type="text/css">
	#mc_embed_signup{background:#fff; clear:left; font:14px Helvetica,Arial,sans-serif; width:100%;}
	/* Add your own Mailchimp form style overrides in your site stylesheet or in this style block.
	   We recommend moving this block and the preceding CSS link to the HEAD of your HTML file. */
</style>
<div id="mc_embed_signup">
<form action="https://stanford.us19.list-manage.com/subscribe/post?u=3a6484754abf2fc18724ec835&amp;id=028823e92b" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <div id="mc_embed_signup_scroll">
	
	<input type="email" value="" name="EMAIL" class="email" id="mce-EMAIL" placeholder="email address" required>
    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_3a6484754abf2fc18724ec835_028823e92b" tabindex="-1" value=""></div>
    <div class="clear"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button"></div>
    </div>
</form>
</div>

<!--End mc_embed_signup-->



  <!-- Social media shares -->
  <div class="share-buttons">
    <ul class="share-buttons">
        <div class="meta">Share</div>
        
        <li>
            <a href="https://www.facebook.com/sharer/sharer.php?u=http://0.0.0.0:4000/blog/minimax-optimal-pac/" target="_blank" title="Share on Facebook">
			<i class="fa fa-facebook-square fa-2x" aria-hidden="true"></i>
			<span class="sr-only">Share on Facebook</span>
		</a>
        </li>
         
        <li>
            <a href="https://twitter.com/intent/tweet?source=http://0.0.0.0:4000/blog/minimax-optimal-pac/&text=Policy+Certificates+and+Minimax-Optimal+PAC+Bounds+for+Episodic+Reinforcement+Learning%20%7C%20SAIL+Blog:%20http://0.0.0.0:4000/blog/minimax-optimal-pac/" target="_blank" title="Tweet">
			<i class="fa fa-twitter-square fa-2x" aria-hidden="true"></i>
			<span class="sr-only">Tweet</span>
		</a>
        </li>
            
        <li>
            <a href="https://getpocket.com/save?url=http://0.0.0.0:4000/blog/minimax-optimal-pac/&title=Policy+Certificates+and+Minimax-Optimal+PAC+Bounds+for+Episodic+Reinforcement+Learning%20%7C%20SAIL+Blog" target="_blank" title="Add to Pocket">
			<i class="fa fa fa-get-pocket fa-2x" aria-hidden="true"></i>
			<span class="sr-only">Add to Pocket</span>
		</a>
        </li>
         
        <li>
            <a href="http://www.reddit.com/submit?url=http://0.0.0.0:4000/blog/minimax-optimal-pac/&title=Policy+Certificates+and+Minimax-Optimal+PAC+Bounds+for+Episodic+Reinforcement+Learning%20%7C%20SAIL+Blog" target="_blank" title="Share on Reddit">
			<i class="fa fa-reddit-square fa-2x" aria-hidden="true"></i>
			<span class="sr-only">Share on Reddit</span>
		</a>
        </li>
           
        <li>
            <a href="mailto:?subject=Policy+Certificates+and+Minimax-Optimal+PAC+Bounds+for+Episodic+Reinforcement+Learning%20%7C%20SAIL+Blog&body=:%20http://0.0.0.0:4000/blog/minimax-optimal-pac/" target="_blank" title="Email">
			<i class="fa fa-envelope-square fa-2x" aria-hidden="true"></i>
			<span class="sr-only">Email</span>
		</a>
        </li>
        
    </ul>
</div>

  <!-- Tag list -->
  
  


<footer>
  <div class="tag-list">
    
      <div class="meta">Tags</div>
    

    
    <a class="button" href="/blog/tags#rl">
      <p><i class="fa fa-tag fa-fw"></i> rl</p>
    </a>
    
    <a class="button" href="/blog/tags#safety">
      <p><i class="fa fa-tag fa-fw"></i> safety</p>
    </a>
    
  </div>
</footer>

</article>

<!-- Disqus -->


<!-- Post navigation -->

  <div id="post-nav">
  
  <div id="previous-post" class="post-nav-post">
      <a href="/blog/topologylayer/">
      <p>Previous post</p>
        A Topology Layer for Machine Learning
      </a>
  </div>
  
  
  <div id="next-post" class="post-nav-post">
      <a href="/blog/acteach/">
      <p>Next post</p>
        AC-Teach: A Bayesian Actor-Critic Method for Policy Learning with an Ensemble of Suboptimal Teachers
      </a>
  </div>
  
</div>



    </div>
    <hr>
<footer class="site-footer">
    
   <div class="footer-icons">
        <ul>
        <!-- Social icons from Font Awesome, if enabled -->
        
<li>
	<a href="http://0.0.0.0:4000/blog/feed.xml" title="Follow RSS feed">
		<span class="fa-stack fa-lg">
            <i class="fa fa-circle fa-stack-2x"></i>
            <i class="fa fa-rss fa-stack-1x fa-inverse"></i>
        </span>
	</a>
</li>







































<li>
	<a href="https://twitter.com/StanfordAILab" title="Follow on Twitter" class="type">
        <span class="fa-stack fa-lg">
            <i class="fa fa-circle fa-stack-2x"></i>
            <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
        </span>
	</a>
</li>








        </ul>
    </div>
    <p class="text">&copy; 2019 Stanford AI Lab
</p>
 
</footer>



  </body>
</html>
