<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <!-- Main JS (navbar.js, katex_init.js and masonry_init.js)-->
    <script defer=true src="/blog/assets/js/main.min.js"></script>
    
    <!-- CSS -->
    <link rel="stylesheet" href="/blog/assets/css/main.css">

    <!--Favicon-->
    <link rel="icon" type="image/png" href="/blog/assets/img/favicon-32x32.png" sizes="32x32" />
    <link rel="icon" type="image/png" href="/blog/assets/img/favicon-16x16.png" sizes="16x16" />

    <!-- Canonical -->
    <link rel="canonical" href="http://0.0.0.0:4000/blog/rl/">

    <!-- RSS -->
    <link rel="alternate" type="application/atom+xml" title="The Stanford AI Lab Blog" href="http://0.0.0.0:4000/blog/feed.xml"/>

    <!-- Font Awesome -->
    <!-- <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet"> -->
    <link rel="stylesheet" type="text/css" href="/blog/assets/css/vendor/font-awesome.min.css">
    
    <!-- Bootstrap-3.3.7 isolation CSS -->
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" integrity="sha384-JcKb8q3iqJ61gNV9KGb8thSsNjpSL0n8PARn9HuZOnIxN0hoP+VmmDGMN5t9UJ0Z" crossorigin="anonymous">


    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js" integrity="sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha384-B4gt1jrGC7Jh4AgTPSdUtOBvfO8shuf57BaghqFfPlYxofvL8/KUEfYiJOMMV+rV" crossorigin="anonymous"></script>
    
    <!-- JQuery 2.2.4 -->
    <script src="https://code.jquery.com/jquery-2.2.4.min.js" integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin="anonymous"></script>
    
    <link rel="stylesheet" type="text/css" href="/blog/assets/css/vendor/bigfoot-number.css">
    <script type="text/javascript" src="/blog/assets/js/vendor/bigfoot.min.js"></script>
    <script type="text/javascript">
    $(document).ready(function() { 
      $.bigfoot();
      
    window.onload = function() {

        var videos = document.getElementsByTagName("video"),
            fraction = 0.8;

        function checkScroll() {

            for (var i = 0; i < videos.length; i++) {

                var video = videos[i];

                var x = video.offsetLeft,
                    y = video.offsetTop,
                    w = video.offsetWidth,
                    h = video.offsetHeight,
                    r = x + w, //right
                    b = y + h, //bottom
                    visibleX, visibleY, visible;

                visibleX = Math.max(0, Math.min(w, window.pageXOffset + window.innerWidth - x, r - window.pageXOffset));
                visibleY = Math.max(0, Math.min(h, window.pageYOffset + window.innerHeight - y, b - window.pageYOffset));

                visible = visibleX * visibleY / (w * h);

                if (visible > fraction) {
                    video.play();
                } else {
                    video.pause();
                }

            }

        }

        window.addEventListener('scroll', checkScroll, false);
        window.addEventListener('resize', checkScroll, false);
    }
    }); 
    </script>
    


    <!-- Google Fonts -->
    
    <link href="//fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,700italic,400italic" rel="stylesheet" type="text/css"> 
    

    <!-- KaTeX 0.8.3 -->
    
    <link rel="stylesheet" type="text/css" href="/blog/assets/css/vendor/katex.min.css">
    <script src="/blog/assets/js/vendor/katex.min.js">
    </script>
    
    
    
    <!-- Google Analytics -->
    
    <script>
        (function(i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r;
            i[r] = i[r] || function() {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date();
            a = s.createElement(o),
                m = s.getElementsByTagName(o)[0];
            a.async = 1;
            a.src = g;
            m.parentNode.insertBefore(a, m)
        })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'UA-129018108-1', 'auto');
        ga('send', 'pageview');

    </script>
    
    
    <!-- seo tags -->
    <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Reinforcement Learning Posts | SAIL Blog</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Reinforcement Learning Posts" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="The official Stanford AI Lab blog" />
<meta property="og:description" content="The official Stanford AI Lab blog" />
<link rel="canonical" href="http://0.0.0.0:4000/blog/rl/" />
<meta property="og:url" content="http://0.0.0.0:4000/blog/rl/" />
<meta property="og:site_name" content="SAIL Blog" />
<link rel="next" href="http://0.0.0.0:4000/blog/rl/page/2/index.html" />
<script type="application/ld+json">
{"description":"The official Stanford AI Lab blog","@type":"WebPage","url":"http://0.0.0.0:4000/blog/rl/","headline":"Reinforcement Learning Posts","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    
    <!-- Manual seo tags -->
    <title>Reinforcement Learning Posts | The Stanford AI Lab Blog</title>
    <meta name="description" content="The Stanford AI Lab (SAIL) Blog is a place for SAIL students, faculty, and researchers to share our work with the general public.">
    
    <!-- Twitter Cards -->
    <meta name="twitter:title" content="Reinforcement Learning Posts">
    
    <meta name="twitter:description" content="The Stanford AI Lab (SAIL) Blog is a place for SAIL students, faculty, and researchers to share our work with the general public.">
    
    <meta name="twitter:creator" content="@StanfordAILab">
     
    <meta name="twitter:card" content="summary">
    <meta name="twitter:image" content="http://0.0.0.0:4000/blog/assets/img/sail-logo.png">
    <meta name="og:image" content="http://0.0.0.0:4000/blog/assets/img/sail-logo.png">
    
</head>

  <body class="category_posts-body">
      <!-- Toggle menu -->
<header class="site-header">

<nav class="clear navbar navbar-expand-lg navbar-light bg-white flex-column flex-md-row bd-navbar fixed-top" id="main_nav">
  
  <div class="container">

    <a class="navbar-brand mr-0 mr-md-2 text-black d-flex align-items-center" href="/blog/" aria-label="Bootstrap">
    
	  <div class="branding">
	    <a href="http://ai.stanford.edu/">
	      <img class="avatar" src="/blog/assets/img/sail-logo.png" alt=""/>
		  </a>

      <a href="/blog/">
	      <h1 class="site-title">
			    The Stanford AI Lab Blog
		    </h1> 
		  </a>
	  </div>
    
    
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
    </button>
    
    <div class="collapse navbar-collapse" id="navbarResponsive">

      <ul class="navbar-nav ml-auto">
      
      
      <li class="nav-item">
      <a class="nav-link" href="/blog/about">About</a>
      </li>
      
      <li class="nav-item dropdown">
        <a class="nav-link dropdown-toggle" data-toggle="dropdown" href="#" role="button" aria-haspopup="true" aria-expanded="false">Posts</a>
        <div class="dropdown-menu">
        <a class="dropdown-item" href="/blog/">All</a>
        <a class="dropdown-item" href="/blog/conferences">Conferences</a>
        <a class="dropdown-item" href="/blog/vision">Computer Vision</a>
        <a class="dropdown-item" href="/blog/robotics">Robotics</a>
        <a class="dropdown-item" href="/blog/nlp">NLP</a>
        <a class="dropdown-item" href="/blog/ml">Machine Learning</a>
        <a class="dropdown-item" href="/blog/rl">Reinforcement Learning</a>

        </div>
      </li>
        
      
      <li class="nav-item">
      <a class="nav-link" href="/blog/subscribe">Subscribe</a>
      </li>
      
      
      <li class="nav-item">
      <a class="nav-link" href="http://ai.stanford.edu/">SAIL</a>
      </li>
      
      </ul> 

    </div>

  </div>
</nav>

</header>

  
    <div class="content">
      <div class="home">
  <div class="posts">
  <br>
    <header id="main">
  <h1 id="Reinforcement+Learning+Posts" class="title">
      Reinforcement Learning Posts
  </h1>
  
  
  <hr>
</header>

    
      
      <div class="post-teaser">
        
        <div class="post-img">

        
          <img src="https://ai.stanford.edu/blog//assets/img/posts/2020-11-14-lili/front.png">
        

        </div>
        
        <div class="info">
          <header>
            <a class="post-link" href="/blog/lili/">
              <h2> Learning to Influence Multi-Agent Interaction </h2>
            </a>

             <p class="meta">
              <a href="https://anxie.github.io/">Annie Xie</a>
              <!-- November 14, 2020 -->
            </p>
          </header>
          <div class="excerpt">
            <!--  -->
            <div class="excerpt-text">
                
                  We introduce a framework for multi-agent interaction that represents the low-level policies of non-stationary agents with high-level latent strategies.
                
            </div>
            <div class="excerpt-continue">
              <a class="button" href="/blog/lili/">
                Continue reading
              </a>
            </div>
          </div>
        </div>
      </div>
      
    
      
      <div class="post-teaser">
        
        <div class="post-img">

        
          <img src="https://ai.stanford.edu/blog/assets/img/posts/2020-10-07-gti/thumbnail.png">
        

        </div>
        
        <div class="info">
          <header>
            <a class="post-link" href="/blog/gti/">
              <h2> GTI: Learning to Generalize Across Long-Horizon Tasks from Human Demonstrations </h2>
            </a>

             <p class="meta">
              <a href="http://web.stanford.edu/~amandlek/">Ajay Mandlekar</a>, <a href="https://cs.stanford.edu/~danfei/">Danfei Xu</a>
              <!-- October 7, 2020 -->
            </p>
          </header>
          <div class="excerpt">
            <!--  -->
            <div class="excerpt-text">
                
                  We developed Generalization Through Imitation (GTI) - an algorithm for learning visuomotor control from human demonstrations and generalizing to new long-horizon tasks by leveraging latent compositional structures.
                
            </div>
            <div class="excerpt-continue">
              <a class="button" href="/blog/gti/">
                Continue reading
              </a>
            </div>
          </div>
        </div>
      </div>
      
    
      
      <div class="post-teaser">
        
        <div class="post-img">

        
          <img src="https://ai.stanford.edu/blog//assets/img/posts/2020-08-25-black-box-safety-validation/formulation.jpg">
        

        </div>
        
        <div class="info">
          <header>
            <a class="post-link" href="/blog/black-box-safety-validation/">
              <h2> Safety Validation of Black-Box Autonomous Systems </h2>
            </a>

             <p class="meta">
              <a href="http://anthonylcorso.com/">Anthony L. Corso</a>
              <!-- August 31, 2020 -->
            </p>
          </header>
          <div class="excerpt">
            <!--  -->
            <div class="excerpt-text">
                
                  Validation of safety-critical autonomous systems is necessary before deployment and is made possible by black-box validation strategies. In this blog post, we provide a model for black-box safety validation and survey the literature to uncover the most effective validation techniques.
                
            </div>
            <div class="excerpt-continue">
              <a class="button" href="/blog/black-box-safety-validation/">
                Continue reading
              </a>
            </div>
          </div>
        </div>
      </div>
      
    
      
      <div class="post-teaser">
        
        <div class="post-img">

        
          <img src="https://ai.stanford.edu/blog/assets/img/posts/2020-08-23-meta-exploration/coupling.svg">
        

        </div>
        
        <div class="info">
          <header>
            <a class="post-link" href="/blog/meta-exploration/">
              <h2> Explore then Execute: Adapting without Rewards via Factorized Meta-Reinforcement Learning </h2>
            </a>

             <p class="meta">
              <a href='https://cs.stanford.edu/~evanliu'>Evan Zheran Liu</a>
              <!-- August 26, 2020 -->
            </p>
          </header>
          <div class="excerpt">
            <!--  -->
            <div class="excerpt-text">
                
                  Adapting to new environments and tasks (e.g., cooking in a new kitchen) requires first gathering information via exploration (e.g., finding the ingredients). We identify a chicken-and-egg coupling problem that prevents existing approaches from effectively exploring, and we solve this with DREAM.
                
            </div>
            <div class="excerpt-continue">
              <a class="button" href="/blog/meta-exploration/">
                Continue reading
              </a>
            </div>
          </div>
        </div>
      </div>
      
    
      
      <div class="post-teaser">
        
        <div class="post-img">

        
          <img src="https://ai.stanford.edu/blog//assets/img/posts/2020-05-18-selfsupervised-multimodal/intro.png">
        

        </div>
        
        <div class="info">
          <header>
            <a class="post-link" href="/blog/selfsupervised-multimodal/">
              <h2> Making Sense of Vision and Touch: Multimodal Representations for Contact-Rich Tasks  </h2>
            </a>

             <p class="meta">
              <a href="http://stanford.edu/~mishlee/">Michelle A. Lee</a>
              <!-- May 18, 2020 -->
            </p>
          </header>
          <div class="excerpt">
            <!--  -->
            <div class="excerpt-text">
                
                  Contact-rich manipulation tasks in unstructured environments often require both tactile and visual feedback. In this blog post, we introduce how to use self-supervision to learn a compact and multimodal representation of vision and touch.
                
            </div>
            <div class="excerpt-continue">
              <a class="button" href="/blog/selfsupervised-multimodal/">
                Continue reading
              </a>
            </div>
          </div>
        </div>
      </div>
      
    
      
      <div class="post-teaser">
        
        <div class="post-img">

        
          <img src="https://ai.stanford.edu/blog//assets/img/posts/2020-04-08-cavin/pull_figure.png">
        

        </div>
        
        <div class="info">
          <header>
            <a class="post-link" href="/blog/cavin/">
              <h2> Sequential Problem Solving by Hierarchical Planning in Latent Spaces </h2>
            </a>

             <p class="meta">
              <a href='http://ai.stanford.edu/~kuanfang/'>Kuan Fang</a>
              <!-- April 10, 2020 -->
            </p>
          </header>
          <div class="excerpt">
            <!--  -->
            <div class="excerpt-text">
                
                  We propose a hierarchical planning algorithm in learned latent spaces. Our method uses deep generative models to prioritize promising actions for sampling-based planning.
                
            </div>
            <div class="excerpt-continue">
              <a class="button" href="/blog/cavin/">
                Continue reading
              </a>
            </div>
          </div>
        </div>
      </div>
      
    
      
      <div class="post-teaser">
        
        <div class="post-img">

        
          <img src="https://ai.stanford.edu/blog/assets/img/posts/2019-11-26-robonet/bg-masthead.jpg">
        

        </div>
        
        <div class="info">
          <header>
            <a class="post-link" href="/blog/robonet/">
              <h2> RoboNet: A Dataset for Large-Scale Multi-Robot Learning </h2>
            </a>

             <p class="meta">
              <a href="https://sudeepdasari.github.io/"> Sudeep Dasari </a>
              <!-- November 26, 2019 -->
            </p>
          </header>
          <div class="excerpt">
            <!--  -->
            <div class="excerpt-text">
                
                  We introduce RoboNet, an open database for sharing robotic experience, and study how this data can be used to learn generalizable models for vision-based robotic manipulation. We find that pre-training on RoboNet enables faster learning in new environments compared to learning from scratch.
                
            </div>
            <div class="excerpt-continue">
              <a class="button" href="/blog/robonet/">
                Continue reading
              </a>
            </div>
          </div>
        </div>
      </div>
      
    
      
      <div class="post-teaser">
        
        <div class="post-img">

        
          <img src="https://ai.stanford.edu/blog/assets/img/posts/2019-11-08-roboturk/mandlekar_iros19.png">
        

        </div>
        
        <div class="info">
          <header>
            <a class="post-link" href="/blog/roboturk/">
              <h2> RoboTurk: Human Reasoning and Dexterity for Large-Scale Dataset Creation </h2>
            </a>

             <p class="meta">
              <a href="http://web.stanford.edu/~amandlek/">Ajay Mandlekar</a>
              <!-- November 8, 2019 -->
            </p>
          </header>
          <div class="excerpt">
            <!--  -->
            <div class="excerpt-text">
                
                  We built a system that enables collecting large-scale robot manipulation datasets with human supervision and used it to collect the largest robot dataset ever collected via teleoperation.
                
            </div>
            <div class="excerpt-continue">
              <a class="button" href="/blog/roboturk/">
                Continue reading
              </a>
            </div>
          </div>
        </div>
      </div>
      
    
      
      <div class="post-teaser">
        
        <div class="post-img">

        
          <img src="https://ai.stanford.edu/blog/assets/img/posts/2019-09-05-acteach/alg-mini.png">
        

        </div>
        
        <div class="info">
          <header>
            <a class="post-link" href="/blog/acteach/">
              <h2> AC-Teach: A Bayesian Actor-Critic Method for Policy Learning with an Ensemble of Suboptimal Teachers </h2>
            </a>

             <p class="meta">
              <a href='https://www.andreykurenkov.com/'>Andrey Kurenkov</a> and <a href="http://web.stanford.edu/~amandlek/">Ajay Mandlekar</a>
              <!-- September 11, 2019 -->
            </p>
          </header>
          <div class="excerpt">
            <!--  -->
            <div class="excerpt-text">
                
                  Presenting AC-Teach, a unifying approach to leverage advice from an ensemble of sub-optimal teachers in order to accelerate the learning process of actor-critic reinforcement learning agents.
                
            </div>
            <div class="excerpt-continue">
              <a class="button" href="/blog/acteach/">
                Continue reading
              </a>
            </div>
          </div>
        </div>
      </div>
      
    
      
      <div class="post-teaser">
        
        <div class="post-img">

        
          <img src="https://ai.stanford.edu/blog/assets/img/posts/2019-08-28-minimax-optimal-pac/feature.png">
        

        </div>
        
        <div class="info">
          <header>
            <a class="post-link" href="/blog/minimax-optimal-pac/">
              <h2> Policy Certificates and Minimax-Optimal PAC Bounds for Episodic Reinforcement Learning </h2>
            </a>

             <p class="meta">
              <a href='https://cdann.net/'>Christoph Dann</a>
              <!-- August 28, 2019 -->
            </p>
          </header>
          <div class="excerpt">
            <!--  -->
            <div class="excerpt-text">
                
                  Introducing a new method that achieves minimax-optimal probably approximately correct (and regret) bounds which match the statistical worst-case lower bounds in the dominating terms for reinforcement learning.
                
            </div>
            <div class="excerpt-continue">
              <a class="button" href="/blog/minimax-optimal-pac/">
                Continue reading
              </a>
            </div>
          </div>
        </div>
      </div>
      
    
  </div>

  
  <div class="paginate pager">

      

      
          <a href="/blog/rl/page/2/index.html" class="button" >
          Next
          <i class="fa fa-chevron-right"></i>
          </a>
      

  </div>
  

</div>


    </div>
    <hr>
<footer class="site-footer">
    
   <div class="footer-icons">
        <ul>
        <!-- Social icons from Font Awesome, if enabled -->
        
<li>
	<a href="http://0.0.0.0:4000/blog/feed.xml" title="Follow RSS feed">
		<span class="fa-stack fa-lg">
            <i class="fa fa-circle fa-stack-2x"></i>
            <i class="fa fa-rss fa-stack-1x fa-inverse"></i>
        </span>
	</a>
</li>







































<li>
	<a href="https://twitter.com/StanfordAILab" title="Follow on Twitter" class="type">
        <span class="fa-stack fa-lg">
            <i class="fa fa-circle fa-stack-2x"></i>
            <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
        </span>
	</a>
</li>








        </ul>
    </div>
    <p class="text">&copy; 2019 Stanford AI Lab
</p>
 
</footer>



  </body>
</html>
