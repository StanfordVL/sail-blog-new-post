<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <!-- Main JS (navbar.js, katex_init.js and masonry_init.js)-->
    <script defer=true src="/blog/assets/js/main.min.js"></script>
    
    <!-- CSS -->
    <link rel="stylesheet" href="/blog/assets/css/main.css">

    <!--Favicon-->
    <link rel="icon" type="image/png" href="/blog/assets/img/favicon-32x32.png" sizes="32x32" />
    <link rel="icon" type="image/png" href="/blog/assets/img/favicon-16x16.png" sizes="16x16" />

    <!-- Canonical -->
    <link rel="canonical" href="http://0.0.0.0:4000/blog/bias-nlp/">

    <!-- RSS -->
    <link rel="alternate" type="application/atom+xml" title="The Stanford AI Lab Blog" href="http://0.0.0.0:4000/blog/feed.xml"/>

    <!-- Font Awesome -->
    <!-- <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet"> -->
    <link rel="stylesheet" type="text/css" href="/blog/assets/css/vendor/font-awesome.min.css">
    
    <!-- Bootstrap-3.3.7 isolation CSS -->
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" integrity="sha384-JcKb8q3iqJ61gNV9KGb8thSsNjpSL0n8PARn9HuZOnIxN0hoP+VmmDGMN5t9UJ0Z" crossorigin="anonymous">


    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js" integrity="sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha384-B4gt1jrGC7Jh4AgTPSdUtOBvfO8shuf57BaghqFfPlYxofvL8/KUEfYiJOMMV+rV" crossorigin="anonymous"></script>
    
    <!-- JQuery 2.2.4 -->
    <script src="https://code.jquery.com/jquery-2.2.4.min.js" integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin="anonymous"></script>
    
    <link rel="stylesheet" type="text/css" href="/blog/assets/css/vendor/bigfoot-number.css">
    <script type="text/javascript" src="/blog/assets/js/vendor/bigfoot.min.js"></script>
    <script type="text/javascript">
    $(document).ready(function() { 
      $.bigfoot();
      
    window.onload = function() {

        var videos = document.getElementsByTagName("video"),
            fraction = 0.8;

        function checkScroll() {

            for (var i = 0; i < videos.length; i++) {

                var video = videos[i];

                var x = video.offsetLeft,
                    y = video.offsetTop,
                    w = video.offsetWidth,
                    h = video.offsetHeight,
                    r = x + w, //right
                    b = y + h, //bottom
                    visibleX, visibleY, visible;

                visibleX = Math.max(0, Math.min(w, window.pageXOffset + window.innerWidth - x, r - window.pageXOffset));
                visibleY = Math.max(0, Math.min(h, window.pageYOffset + window.innerHeight - y, b - window.pageYOffset));

                visible = visibleX * visibleY / (w * h);

                if (visible > fraction) {
                    video.play();
                } else {
                    video.pause();
                }

            }

        }

        window.addEventListener('scroll', checkScroll, false);
        window.addEventListener('resize', checkScroll, false);
    }
    }); 
    </script>
    


    <!-- Google Fonts -->
    
    <link href="//fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,700italic,400italic" rel="stylesheet" type="text/css"> 
    

    <!-- KaTeX 0.8.3 -->
    
    <link rel="stylesheet" type="text/css" href="/blog/assets/css/vendor/katex.min.css">
    <script src="/blog/assets/js/vendor/katex.min.js">
    </script>
    
    
    
    <!-- Google Analytics -->
    
    <script>
        (function(i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r;
            i[r] = i[r] || function() {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date();
            a = s.createElement(o),
                m = s.getElementsByTagName(o)[0];
            a.async = 1;
            a.src = g;
            m.parentNode.insertBefore(a, m)
        })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'UA-129018108-1', 'auto');
        ga('send', 'pageview');

    </script>
    
    
    <!-- seo tags -->
    <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Measuring Bias in NLP (with Confidence!) | SAIL Blog</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Measuring Bias in NLP (with Confidence!)" />
<meta name="author" content="<a href='https://kawine.github.io/'>Kawin Ethayarajh</a>" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Countless studies have found that “bias” – typically with respect to race and gender – pervades the embeddings and predictions of the black-box models that dominate natural language processing (NLP). For example, the language model GPT-3, of OpenAI fame, can generate racist rants when given the right prompt. Attempts to detect hate speech can itself harm minority populations, whose dialect is more likely to be flagged as hateful." />
<meta property="og:description" content="Countless studies have found that “bias” – typically with respect to race and gender – pervades the embeddings and predictions of the black-box models that dominate natural language processing (NLP). For example, the language model GPT-3, of OpenAI fame, can generate racist rants when given the right prompt. Attempts to detect hate speech can itself harm minority populations, whose dialect is more likely to be flagged as hateful." />
<link rel="canonical" href="http://0.0.0.0:4000/blog/bias-nlp/" />
<meta property="og:url" content="http://0.0.0.0:4000/blog/bias-nlp/" />
<meta property="og:site_name" content="SAIL Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-11-11T00:00:00-08:00" />
<script type="application/ld+json">
{"description":"Countless studies have found that “bias” – typically with respect to race and gender – pervades the embeddings and predictions of the black-box models that dominate natural language processing (NLP). For example, the language model GPT-3, of OpenAI fame, can generate racist rants when given the right prompt. Attempts to detect hate speech can itself harm minority populations, whose dialect is more likely to be flagged as hateful.","author":{"@type":"Person","name":"<a href='https://kawine.github.io/'>Kawin Ethayarajh</a>"},"@type":"BlogPosting","url":"http://0.0.0.0:4000/blog/bias-nlp/","headline":"Measuring Bias in NLP (with Confidence!)","dateModified":"2020-11-11T00:00:00-08:00","datePublished":"2020-11-11T00:00:00-08:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://0.0.0.0:4000/blog/bias-nlp/"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    
    <!-- Manual seo tags -->
    <title>Measuring Bias in NLP (with Confidence!) | The Stanford AI Lab Blog</title>
    <meta name="description" content="Measuring bias under uncertainty using confidence intervals and why we need bigger datasets to measure bias in NLP.">
    
    <!-- Twitter Cards -->
    <meta name="twitter:title" content="Measuring Bias in NLP (with Confidence!)">
    
    <meta name="twitter:description" content="Measuring bias under uncertainty using confidence intervals and why we need bigger datasets to measure bias in NLP.">
    
    <meta name="twitter:creator" content="@StanfordAILab">
    
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:image" content="http://0.0.0.0:4000/blog/assets/img/posts/2020-11-11-bias-nlp/scales.png">
    <meta name="og:image" content="http://0.0.0.0:4000/blog/assets/img/posts/2020-11-11-bias-nlp/scales.png">
    
</head>

  <body class="post-body">
      <!-- Toggle menu -->
<header class="site-header">

<nav class="clear navbar navbar-expand-lg navbar-light bg-white flex-column flex-md-row bd-navbar fixed-top" id="main_nav">
  
  <div class="container">

    <a class="navbar-brand mr-0 mr-md-2 text-black d-flex align-items-center" href="/blog/" aria-label="Bootstrap">
    
	  <div class="branding">
	    <a href="http://ai.stanford.edu/">
	      <img class="avatar" src="/blog/assets/img/sail-logo.png" alt=""/>
		  </a>

      <a href="/blog/">
	      <h1 class="site-title">
			    The Stanford AI Lab Blog
		    </h1> 
		  </a>
	  </div>
    
    
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
    </button>
    
    <div class="collapse navbar-collapse" id="navbarResponsive">

      <ul class="navbar-nav ml-auto">
      
      
      <li class="nav-item">
      <a class="nav-link" href="/blog/about">About</a>
      </li>
      
      <li class="nav-item dropdown">
        <a class="nav-link dropdown-toggle" data-toggle="dropdown" href="#" role="button" aria-haspopup="true" aria-expanded="false">Posts</a>
        <div class="dropdown-menu">
        <a class="dropdown-item" href="/blog/">All</a>
        <a class="dropdown-item" href="/blog/conferences">Conferences</a>
        <a class="dropdown-item" href="/blog/vision">Computer Vision</a>
        <a class="dropdown-item" href="/blog/robotics">Robotics</a>
        <a class="dropdown-item" href="/blog/nlp">NLP</a>
        <a class="dropdown-item" href="/blog/ml">Machine Learning</a>
        <a class="dropdown-item" href="/blog/rl">Reinforcement Learning</a>

        </div>
      </li>
        
      
      <li class="nav-item">
      <a class="nav-link" href="/blog/subscribe">Subscribe</a>
      </li>
      
      
      <li class="nav-item">
      <a class="nav-link" href="http://ai.stanford.edu/">SAIL</a>
      </li>
      
      </ul> 

    </div>

  </div>
</nav>

</header>

  
    <div class="content">
      

<article>

  <header id="main">
    
    <h1 id="post_title">Measuring Bias in NLP (with Confidence!)</h1>
    <p class="meta">
    <a href='https://kawine.github.io/'>Kawin Ethayarajh</a>
    <div class="post-date">November 11, 2020</div>
    </p>
  <hr>
  </header>


  <section class="post-content">
  
    <p>Countless studies have found that “bias” – typically with respect to race and gender – pervades the <a href="https://arxiv.org/abs/1904.03310">embeddings</a> and <a href="https://arxiv.org/abs/1804.09301">predictions</a> of the black-box models that dominate natural language processing (NLP). For example, the language model <a href="https://en.wikipedia.org/wiki/GPT-3">GPT-3</a>, of OpenAI fame, can generate <a href="https://www.technologyreview.com/2020/10/23/1011116/chatbot-gpt3-openai-facebook-google-safety-fix-racist-sexist-language-ai/">racist rants</a> when given the right prompt. Attempts to detect hate speech can itself harm minority populations, <a href="https://www.aclweb.org/anthology/P19-1163.pdf">whose dialect is more likely to be flagged as hateful</a>.</p>

<p>This, in turn, has led to a wave of work on how to “<a href="http://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-d">debias</a>” models, only for others to find ways in which debiased models <a href="https://arxiv.org/abs/1903.03862">are still biased</a>, and so on.</p>

<p>But are these claims of NLP models being biased (or unbiased) being made with enough evidence?</p>

<p>Consider the sentence <em>“The doctor gave instructions to the nurse before she left.”</em> A <a href="https://en.wikipedia.org/wiki/Coreference#Coreference_resolution">co-reference resolution system</a>, tasked with finding which person the pronoun “she” is referring to<sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup>, may incorrectly predict that it’s the nurse. Does this incorrect prediction – which conforms to gender stereotypes that doctors are usually male – mean that the system is gender-biased? Possibly – but it may also make mistakes in the other direction with equal frequency (e.g., thinking “he” refers to a nurse when it doesn’t). What if the system makes gender-stereotypical mistakes on not one sentence, but 100, or 1000? Then we could be more confident in claiming that it’s biased.</p>

<p>In my ACL 2020 paper, “<a href="https://www.aclweb.org/anthology/2020.acl-main.262/">Measuring Fairness under Uncertainty with Bernstein Bounds</a>”, I go over how, in the haste to claim the presence or absence of bias, the inherent uncertainty in measuring bias is often overlooked in the literature:</p>

<ul>
  <li>
    <p><strong>Bias is not a single number</strong>. When we test how biased a model is, we are <em>estimating</em> its bias on a sample of the data; our estimate may suggest that the model is biased or unbiased, but the opposite could still be true.</p>
  </li>
  <li>
    <p><strong>This uncertainty can be captured using confidence intervals.</strong> Instead of reporting a single number for bias, practitioners should report an interval, based on factors such as the desired confidence and the proposed definition of “bias”.</p>
  </li>
  <li>
    <p><strong>Existing datasets are too small to conclusively identify bias.</strong> Existing datasets for measuring specific biases can only be used to make 95% confidence claims when the bias estimate is egregiously high; to catch more subtle bias, the NLP community needs bigger datasets.</p>
  </li>
</ul>

<p>Although this problem can exist with any kind of model, we focus on a remedy for classification models in particular.</p>

<h3 id="bernstein-bounded-unfairness">Bernstein-Bounded Unfairness</h3>

<p>A bias estimate, made using a small sample of data, likely differs from the true bias (i.e., at the population-level). How can we express our uncertainty about the estimate? We propose a method called Bernstein-bounded unfairness that translates this uncertainty into a confidence interval<sup id="fnref:2"><a href="#fn:2" class="footnote">2</a></sup>.</p>

<p>Let’s say we want to measure whether some <a href="https://en.wikipedia.org/wiki/Protected_group">protected group</a> <script type="math/tex">A</script> – that is legally protected due to an attribute such as race or gender – is being discriminated against by some classifier, relative to some unprotected group <script type="math/tex">B</script>. They occur in the population with frequency <script type="math/tex">\gamma_A, \gamma_B</script> respectively. We need</p>

<ul>
  <li>
    <p>An annotation function <script type="math/tex">f</script> that maps each example <script type="math/tex">x</script> to <script type="math/tex">A, B,</script> or neither. Note that the annotation function maps inputs to the protected/unprotected groups, not to the output space <script type="math/tex">Y</script>. For example, if we wanted to study how a sentiment classifier performed across different racial groups, then the inputs <script type="math/tex">x</script> would be sentences, labels <script type="math/tex">y</script> would be the sentiment, and the annotation function <script type="math/tex">f</script> might map <script type="math/tex">x</script> to {white, non-white} depending on the racial group of the sentence author.</p>
  </li>
  <li>
    <p>A cost function <script type="math/tex">c : (y, \hat{y}) \to [0,C]</script> that describes the cost of incorrectly predicting <script type="math/tex">\hat{y}</script> when the true label is <script type="math/tex">y</script>, where <script type="math/tex">C</script> is the maximum possible cost. Since a model making an incorrect prediction for <script type="math/tex">x</script> is an undesirable outcome for the group that <script type="math/tex">x</script> belongs to, we frame this as a cost that must be borne by the group.</p>
  </li>
</ul>

<p>We want to choose these functions such that our bias metric of choice – which we call the <em>groupwise disparity</em> <script type="math/tex">\delta(f,c)</script> – can be expressed as the difference in expected cost borne by the protected and unprotected groups. Given a model that makes predictions <script type="math/tex">\hat{y}_a</script> for protected <script type="math/tex">x_a \in A</script> and <script type="math/tex">\hat{y}_b</script> for unprotected <script type="math/tex">x_b \in B</script>, we want to express the bias as:</p>

<script type="math/tex; mode=display">\delta(f,c) = \mathbb{E}_a[c(y_a, \hat{y}_a)] - \mathbb{E}_b[c(y_b, \hat{y}_b)]</script>

<p>If the protected group is incurring higher costs in expectation, it is being biased against. For example, if we want to determine whether a classifier is more accurate on the unprotected group <script type="math/tex">B</script>, then we would set the cost function to be the 1-0 loss (1 for an incorrect prediction, 0 for a correct one). If <script type="math/tex">B</script> has a lower cost on average then <script type="math/tex">A</script>, then it would mean that the classifier is more accurate on <script type="math/tex">B</script>.</p>

<p>For a desired confidence level <script type="math/tex">\rho \in [0,1)</script>, a dataset of <script type="math/tex">n</script> examples, and the variance <script type="math/tex">\sigma^2</script> of the amortized groupwise disparity across examples, the confidence interval <script type="math/tex">t</script> would be</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
t &= \frac{B + \sqrt{B^2 - 8 n \sigma^2 \log \left[\frac{1}{2} (1 - \rho) \right]}}{2n} \\
\text{where } B &= -\frac{2 C}{3 \gamma} \log \left[ \frac{1}{2} (1 - \rho) \right],  \gamma = \min(\gamma_A, \gamma_B)
\end{aligned} %]]></script>

<p>If we set <script type="math/tex">\rho = 0.95</script>, we could claim with 95% confidence that the true bias experienced by the protected group lies in the interval <script type="math/tex">[ \hat{\delta} - t, \hat{\delta} + t]</script>, where <script type="math/tex">\hat{\delta}</script> is our bias estimate.</p>

<h3 id="why-we-need-bigger-datasets">Why We Need Bigger Datasets</h3>

<p>If we want to say with 95% confidence that a classifier is biased <em>to some extent</em> – but want to spend as little time annotating data as possible – we need to find the smallest <script type="math/tex">n</script> such that <script type="math/tex">0 \not\in [ \hat{\delta} - t, \hat{\delta} + t]</script>. We can do this by working backwards from the formula for <script type="math/tex">t</script> given above (see paper for details).</p>

<p>Let’s go back to our original example. Say we want to figure out whether a co-reference resolution system, tasked with matching pronouns to the nouns they refer to, is gender-biased or not. We have a dataset of 500 examples to test whether the model does better on gender-stereotypical examples (e.g., a female nurse) than non-gender-stereotypical examples (e.g., a male nurse). Since we are measuring the difference in accuracy, we set the cost function to be the 1-0 loss.</p>

<p>On this dataset, our bias estimate for a model we’re evaluating is <script type="math/tex">\bar{\delta} = 0.05</script>. Is this enough to claim with 95% confidence that the model is gender-biased?</p>

<p>In this scenario <script type="math/tex">C = 1, \bar{\delta} = 0.05, \rho = 0.95</script>. We assume that there are equally many stereotypical and non-stereotypical examples and that the variance is maximal, so <script type="math/tex">\gamma = 0.5, \sigma^2 = 4</script>.</p>

<p>With these settings, <script type="math/tex">n > 11903</script>; we would need a dataset of more than 11903 examples to claim with 95% confidence that the co-reference resolution system is gender-biased. This is roughly 3.8 times larger than <a href="https://arxiv.org/abs/1804.06876">WinoBias</a>, the largest dataset currently available for this purpose. We could only use WinoBias if <script type="math/tex">\bar{\delta} = 0.0975</script> – that is, if the sample bias were almost twice as high.</p>

<p align="center">
	<img src="/blog/assets/img/posts/2020-11-11-bias-nlp/bbu_3.png" style="width: 80%" />
    <figcaption>As seen above, the WinoBias dataset cannot be used to make claims of bias with 95% confidence unless the sample bias is egregiously high.</figcaption>
</p>

<h3 id="conclusion">Conclusion</h3>

<p>In the haste to claim the presence or absence of bias in models, the uncertainty in estimating bias is often overlooked in the literature. A model’s bias is often thought of as a single number, even though this number is ultimately an estimate and not the final word on whether the model is or is not biased.</p>

<p>We proposed a method called Bernstein-bounded unfairness for capturing this uncertainty using confidence intervals. To faithfully reflect the range of possible conclusions, we recommend that NLP practitioners measuring bias not only report their bias estimate but also this confidence interval.</p>

<p>What if we want to catch more subtle bias? Although it may be possible to derive tighter confidence intervals, what we really need are larger bias-specific datasets. The datasets we currently have are undoubtedly helpful, but they need to be much larger in order to diagnose biases with confidence.</p>

<h5 id="acknowledgements">Acknowledgements</h5>

<p class="small-text"> 
Many thanks to Krishnapriya Vishnubhotla, Michelle Lee, and Kaitlyn Zhou for their feedback on this blog post.
</p>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>The goal of coreference resolution more broadly is to find all expressions that refer to the same entity in a text. For example, in “I gave my mother Sally a gift for her birthday.”, the terms “my mother”, “Sally”, and “her” all refer to the same entity. <a href="#fnref:1" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>We use <a href="https://en.wikipedia.org/wiki/Bernstein_inequalities_(probability_theory)">Bernstein’s inequality</a> to derive the confidence intervals, hence the name Bernstein-bounded unfairness. This inequality tells us with what probability the average of <script type="math/tex">n</script> independent random variables will be within a constant $t$ of their true mean $\mu$. <a href="#fnref:2" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>

  
  </section>
  <hr>
  Keep on top of the latest SAIL Blog posts via <a class="social-icon" href="http://0.0.0.0:4000/blog/feed.xml">RSS <i class="fa fa-rss-square fa fa" title="Twitter"></i></a>, <a class="social-icon" href="https://twitter.com/StanfordAILab">Twitter <i class="fa fa-twitter-square fa fa" title="Twitter"></i></a>, or email:

<!-- Begin Mailchimp Signup Form -->
<link href="//cdn-images.mailchimp.com/embedcode/horizontal-slim-10_7.css" rel="stylesheet" type="text/css">
<style type="text/css">
	#mc_embed_signup{background:#fff; clear:left; font:14px Helvetica,Arial,sans-serif; width:100%;}
	/* Add your own Mailchimp form style overrides in your site stylesheet or in this style block.
	   We recommend moving this block and the preceding CSS link to the HEAD of your HTML file. */
</style>
<div id="mc_embed_signup">
<form action="https://stanford.us19.list-manage.com/subscribe/post?u=3a6484754abf2fc18724ec835&amp;id=028823e92b" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <div id="mc_embed_signup_scroll">
	
	<input type="email" value="" name="EMAIL" class="email" id="mce-EMAIL" placeholder="email address" required>
    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_3a6484754abf2fc18724ec835_028823e92b" tabindex="-1" value=""></div>
    <div class="clear"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button"></div>
    </div>
</form>
</div>

<!--End mc_embed_signup-->



  <!-- Social media shares -->
  <div class="share-buttons">
    <ul class="share-buttons">
        <div class="meta">Share</div>
        
        <li>
            <a href="https://www.facebook.com/sharer/sharer.php?u=http://0.0.0.0:4000/blog/bias-nlp/" target="_blank" title="Share on Facebook">
			<i class="fa fa-facebook-square fa-2x" aria-hidden="true"></i>
			<span class="sr-only">Share on Facebook</span>
		</a>
        </li>
         
        <li>
            <a href="https://twitter.com/intent/tweet?source=http://0.0.0.0:4000/blog/bias-nlp/&text=Measuring+Bias+in+NLP+%28with+Confidence%21%29%20%7C%20SAIL+Blog:%20http://0.0.0.0:4000/blog/bias-nlp/" target="_blank" title="Tweet">
			<i class="fa fa-twitter-square fa-2x" aria-hidden="true"></i>
			<span class="sr-only">Tweet</span>
		</a>
        </li>
            
        <li>
            <a href="https://getpocket.com/save?url=http://0.0.0.0:4000/blog/bias-nlp/&title=Measuring+Bias+in+NLP+%28with+Confidence%21%29%20%7C%20SAIL+Blog" target="_blank" title="Add to Pocket">
			<i class="fa fa fa-get-pocket fa-2x" aria-hidden="true"></i>
			<span class="sr-only">Add to Pocket</span>
		</a>
        </li>
         
        <li>
            <a href="http://www.reddit.com/submit?url=http://0.0.0.0:4000/blog/bias-nlp/&title=Measuring+Bias+in+NLP+%28with+Confidence%21%29%20%7C%20SAIL+Blog" target="_blank" title="Share on Reddit">
			<i class="fa fa-reddit-square fa-2x" aria-hidden="true"></i>
			<span class="sr-only">Share on Reddit</span>
		</a>
        </li>
           
        <li>
            <a href="mailto:?subject=Measuring+Bias+in+NLP+%28with+Confidence%21%29%20%7C%20SAIL+Blog&body=:%20http://0.0.0.0:4000/blog/bias-nlp/" target="_blank" title="Email">
			<i class="fa fa-envelope-square fa-2x" aria-hidden="true"></i>
			<span class="sr-only">Email</span>
		</a>
        </li>
        
    </ul>
</div>

  <!-- Tag list -->
  
  


<footer>
  <div class="tag-list">
    
      <div class="meta">Tags</div>
    

    
    <a class="button" href="/blog/tags#bias">
      <p><i class="fa fa-tag fa-fw"></i> bias</p>
    </a>
    
    <a class="button" href="/blog/tags#ethics">
      <p><i class="fa fa-tag fa-fw"></i> ethics</p>
    </a>
    
    <a class="button" href="/blog/tags#fairness">
      <p><i class="fa fa-tag fa-fw"></i> fairness</p>
    </a>
    
    <a class="button" href="/blog/tags#gpt-3">
      <p><i class="fa fa-tag fa-fw"></i> gpt-3</p>
    </a>
    
    <a class="button" href="/blog/tags#gpt3">
      <p><i class="fa fa-tag fa-fw"></i> gpt3</p>
    </a>
    
    <a class="button" href="/blog/tags#nlp">
      <p><i class="fa fa-tag fa-fw"></i> nlp</p>
    </a>
    
  </div>
</footer>

</article>

<!-- Disqus -->


<!-- Post navigation -->

  <div id="post-nav">
  
  <div id="previous-post" class="post-nav-post">
      <a href="/blog/DrRepair/">
      <p>Previous post</p>
        Learning to Fix Programs from Error Messages
      </a>
  </div>
  
  
  <div id="next-post" class="post-nav-post">
      <a href="/blog/bootleg/">
      <p>Next post</p>
        Bootleg: Chasing the Tail with Self-Supervised Named Entity Disambiguation
      </a>
  </div>
  
</div>



    </div>
    <hr>
<footer class="site-footer">
    
   <div class="footer-icons">
        <ul>
        <!-- Social icons from Font Awesome, if enabled -->
        
<li>
	<a href="http://0.0.0.0:4000/blog/feed.xml" title="Follow RSS feed">
		<span class="fa-stack fa-lg">
            <i class="fa fa-circle fa-stack-2x"></i>
            <i class="fa fa-rss fa-stack-1x fa-inverse"></i>
        </span>
	</a>
</li>







































<li>
	<a href="https://twitter.com/StanfordAILab" title="Follow on Twitter" class="type">
        <span class="fa-stack fa-lg">
            <i class="fa fa-circle fa-stack-2x"></i>
            <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
        </span>
	</a>
</li>








        </ul>
    </div>
    <p class="text">&copy; 2019 Stanford AI Lab
</p>
 
</footer>



  </body>
</html>
