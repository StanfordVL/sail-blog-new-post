<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <!-- Main JS (navbar.js, katex_init.js and masonry_init.js)-->
    <script defer=true src="/blog/assets/js/main.min.js"></script>
    
    <!-- CSS -->
    <link rel="stylesheet" href="/blog/assets/css/main.css">

    <!--Favicon-->
    <link rel="icon" type="image/png" href="/blog/assets/img/favicon-32x32.png" sizes="32x32" />
    <link rel="icon" type="image/png" href="/blog/assets/img/favicon-16x16.png" sizes="16x16" />

    <!-- Canonical -->
    <link rel="canonical" href="http://0.0.0.0:4000/blog/batch-active-preference-learning/">

    <!-- RSS -->
    <link rel="alternate" type="application/atom+xml" title="The Stanford AI Lab Blog" href="http://0.0.0.0:4000/blog/feed.xml"/>

    <!-- Font Awesome -->
    <!-- <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet"> -->
    <link rel="stylesheet" type="text/css" href="/blog/assets/css/vendor/font-awesome.min.css">
    
    <!-- Bootstrap-3.3.7 isolation CSS -->
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" integrity="sha384-JcKb8q3iqJ61gNV9KGb8thSsNjpSL0n8PARn9HuZOnIxN0hoP+VmmDGMN5t9UJ0Z" crossorigin="anonymous">


    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js" integrity="sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha384-B4gt1jrGC7Jh4AgTPSdUtOBvfO8shuf57BaghqFfPlYxofvL8/KUEfYiJOMMV+rV" crossorigin="anonymous"></script>
    
    <!-- JQuery 2.2.4 -->
    <script src="https://code.jquery.com/jquery-2.2.4.min.js" integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin="anonymous"></script>
    
    <link rel="stylesheet" type="text/css" href="/blog/assets/css/vendor/bigfoot-number.css">
    <script type="text/javascript" src="/blog/assets/js/vendor/bigfoot.min.js"></script>
    <script type="text/javascript">
    $(document).ready(function() { 
      $.bigfoot();
      
    window.onload = function() {

        var videos = document.getElementsByTagName("video"),
            fraction = 0.8;

        function checkScroll() {

            for (var i = 0; i < videos.length; i++) {

                var video = videos[i];

                var x = video.offsetLeft,
                    y = video.offsetTop,
                    w = video.offsetWidth,
                    h = video.offsetHeight,
                    r = x + w, //right
                    b = y + h, //bottom
                    visibleX, visibleY, visible;

                visibleX = Math.max(0, Math.min(w, window.pageXOffset + window.innerWidth - x, r - window.pageXOffset));
                visibleY = Math.max(0, Math.min(h, window.pageYOffset + window.innerHeight - y, b - window.pageYOffset));

                visible = visibleX * visibleY / (w * h);

                if (visible > fraction) {
                    video.play();
                } else {
                    video.pause();
                }

            }

        }

        window.addEventListener('scroll', checkScroll, false);
        window.addEventListener('resize', checkScroll, false);
    }
    }); 
    </script>
    


    <!-- Google Fonts -->
    
    <link href="//fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,700italic,400italic" rel="stylesheet" type="text/css"> 
    

    <!-- KaTeX 0.8.3 -->
    
    <link rel="stylesheet" type="text/css" href="/blog/assets/css/vendor/katex.min.css">
    <script src="/blog/assets/js/vendor/katex.min.js">
    </script>
    
    
    
    <!-- Google Analytics -->
    
    <script>
        (function(i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r;
            i[r] = i[r] || function() {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date();
            a = s.createElement(o),
                m = s.getElementsByTagName(o)[0];
            a.async = 1;
            a.src = g;
            m.parentNode.insertBefore(a, m)
        })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'UA-129018108-1', 'auto');
        ga('send', 'pageview');

    </script>
    
    
    <!-- seo tags -->
    <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Batch-Active Preference-Based Learning of Reward Functions | SAIL Blog</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Batch-Active Preference-Based Learning of Reward Functions" />
<meta name="author" content="<a href='http://stanford.edu/~ebiyik/'>Erdem Bıyık</a>" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Efficient reward learning is hard. With a focus on preference-based learning methods, we show how sample-efficiency can be achieved along with computational efficiency by using batch-active methods." />
<meta property="og:description" content="Efficient reward learning is hard. With a focus on preference-based learning methods, we show how sample-efficiency can be achieved along with computational efficiency by using batch-active methods." />
<link rel="canonical" href="http://0.0.0.0:4000/blog/batch-active-preference-learning/" />
<meta property="og:url" content="http://0.0.0.0:4000/blog/batch-active-preference-learning/" />
<meta property="og:site_name" content="SAIL Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-12-10T00:00:00-08:00" />
<script type="application/ld+json">
{"description":"Efficient reward learning is hard. With a focus on preference-based learning methods, we show how sample-efficiency can be achieved along with computational efficiency by using batch-active methods.","author":{"@type":"Person","name":"<a href='http://stanford.edu/~ebiyik/'>Erdem Bıyık</a>"},"@type":"BlogPosting","url":"http://0.0.0.0:4000/blog/batch-active-preference-learning/","headline":"Batch-Active Preference-Based Learning of Reward Functions","dateModified":"2018-12-10T00:00:00-08:00","datePublished":"2018-12-10T00:00:00-08:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://0.0.0.0:4000/blog/batch-active-preference-learning/"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    
    <!-- Manual seo tags -->
    <title>Batch-Active Preference-Based Learning of Reward Functions | The Stanford AI Lab Blog</title>
    <meta name="description" content="The Stanford AI Lab (SAIL) Blog is a place for SAIL students, faculty, and researchers to share our work with the general public.">
    
    <!-- Twitter Cards -->
    <meta name="twitter:title" content="Batch-Active Preference-Based Learning of Reward Functions">
    
    <meta name="twitter:description" content="The Stanford AI Lab (SAIL) Blog is a place for SAIL students, faculty, and researchers to share our work with the general public.">
    
    <meta name="twitter:creator" content="@StanfordAILab">
    
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:image" content="http://0.0.0.0:4000/blog/assets/img/posts/2018-12-10-batch-active-preference-learning/feature.png">
    <meta name="og:image" content="http://0.0.0.0:4000/blog/assets/img/posts/2018-12-10-batch-active-preference-learning/feature.png">
    
</head>

  <body class="post-body">
      <!-- Toggle menu -->
<header class="site-header">

<nav class="clear navbar navbar-expand-lg navbar-light bg-white flex-column flex-md-row bd-navbar fixed-top" id="main_nav">
  
  <div class="container">

    <a class="navbar-brand mr-0 mr-md-2 text-black d-flex align-items-center" href="/blog/" aria-label="Bootstrap">
    
	  <div class="branding">
	    <a href="http://ai.stanford.edu/">
	      <img class="avatar" src="/blog/assets/img/sail-logo.png" alt=""/>
		  </a>

      <a href="/blog/">
	      <h1 class="site-title">
			    The Stanford AI Lab Blog
		    </h1> 
		  </a>
	  </div>
    
    
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
    </button>
    
    <div class="collapse navbar-collapse" id="navbarResponsive">

      <ul class="navbar-nav ml-auto">
      
      
      <li class="nav-item">
      <a class="nav-link" href="/blog/about">About</a>
      </li>
      
      <li class="nav-item dropdown">
        <a class="nav-link dropdown-toggle" data-toggle="dropdown" href="#" role="button" aria-haspopup="true" aria-expanded="false">Posts</a>
        <div class="dropdown-menu">
        <a class="dropdown-item" href="/blog/">All</a>
        <a class="dropdown-item" href="/blog/conferences">Conferences</a>
        <a class="dropdown-item" href="/blog/vision">Computer Vision</a>
        <a class="dropdown-item" href="/blog/robotics">Robotics</a>
        <a class="dropdown-item" href="/blog/nlp">NLP</a>
        <a class="dropdown-item" href="/blog/ml">Machine Learning</a>
        <a class="dropdown-item" href="/blog/rl">Reinforcement Learning</a>

        </div>
      </li>
        
      
      <li class="nav-item">
      <a class="nav-link" href="/blog/subscribe">Subscribe</a>
      </li>
      
      
      <li class="nav-item">
      <a class="nav-link" href="http://ai.stanford.edu/">SAIL</a>
      </li>
      
      </ul> 

    </div>

  </div>
</nav>

</header>

  
    <div class="content">
      

<article>

  <header id="main">
    
    <h1 id="post_title">Batch-Active Preference-Based Learning of Reward Functions</h1>
    <p class="meta">
    <a href='http://stanford.edu/~ebiyik/'>Erdem Bıyık</a>
    <div class="post-date">December 10, 2018</div>
    </p>
  <hr>
  </header>


  <section class="post-content">
  
    <p>Machine learning has become quite successful in several domains in the
past decade. An important portion of this success can be associated to
the availability of large amounts of data. However, collecting and
labeling data can be costly and time-consuming in many fields such as
speech recognition, text classification, image recognition, as well as
in robotics. In addition to lack of labeled data, <em>robot learning</em> has a
few other challenges that makes it particularly difficult:</p>

<ul>
  <li>
    <p>Learning from demonstrations<sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup> has recently been popular for
many tasks. However, we cannot just rely on collecting
demonstrations from humans to learn the desired behavior of a
robot, since human experts usually provide suboptimal
demonstrations or have difficulty operating a robot with more than
a few degrees of freedom. Imagine you are to control a drone. Even
if you are an expert on it, how optimal can you be on completing a
task, e.g. following a specific trajectory as quickly as possible?</p>
  </li>
  <li>
    <p>We could just use reinforcement learning to have the robot optimize
for itself, but what will be the reward function? Beyond cases
where it is easy to automatically measure success or failure, it
is not just hard to come up with an analytical reward function,
but it is also hard to have humans assign reward values on what
robots do. Imagine you are watching a robot grasping an object.
Could you reliably answer if you were asked: “On a scale of 0 to
10, how good was that?”. Even if you can answer, how precise can
you be?</p>
  </li>
  <li>
    <p>Both demonstrations and human reward labeling have another shared
problem: scaling. Given that most supervised learning and
reinforcement learning techniques need tons of data, it would take
humans giving demonstrations or labeling rewards for years and
years just to train one agent. This is clearly not practical.</p>
  </li>
</ul>

<p>So what can we do? One alternative is <em>preference-based methods -</em>
instead of asking users to assign reward values, we will show them two
options and ask them which one they would prefer. Furthermore, we are
going to use active learning techniques to have the robot itself ask for
queries that would give the most information and thus not need the human
to toil the years just to teach the robot one skill. But, there’s an
issue: while the use of active learning helps overcoming scalability
issues regarding data size, it is computationally not practical as it
needs to solve an optimization for each query selection. Luckily, we
have a solution: combine preference learning and batch active learning
to generate several queries at once! This is the premise of our CoRL
paper “<a href="https://arxiv.org/abs/1810.04303"><strong>Batch Active Preference-Based Learning of Reward
Functions</strong></a>”, which we will
overview in this post.</p>

<h3 id="preference-based-learning">Preference-based Learning</h3>

<p>Let’s start with some background. Is Preference-based learning really a
reliable machine learning technique?</p>

<figure class="figure"><div class="figure__main">
<p><a href="/blog/assets/img/posts/2018-12-10-batch-active-preference-learning/image7.gif"><img class="postimage_unpadded" src="/blog/assets/img/posts/2018-12-10-batch-active-preference-learning/image7.gif" /></a></p>
</div><figcaption class="figure__caption" style="padding-top:0;"><p>Preference queries: Given the movements of the pink car, which trajectory of the orange car would you prefer following?</p>
</figcaption></figure>

<p>In fact, psychologists studied this subject decades ago and concluded
humans are pretty reliable on answering preference queries when the
number of options to compare is low enough<sup id="fnref:2"><a href="#fn:2" class="footnote">2</a></sup> . In this post, we will
focus on pairwise comparisons. Given we can now reliably collect data,
the next natural question would be: How are we going to use these
comparisons to learn the underlying reward function?</p>

<p>To develop our learning algorithm, we will first model the structure of
the reward function. We will assume that the reward value of a
trajectory is a linear function of some high-level features: <script type="math/tex">R(\xi) = \omega^{T}\phi(\xi)</script></p>

<p>For example, for an autonomous driving task, these features could be the
alignment of the car with the road and with the lane, the speed, the
distance to the closest car, etc<sup id="fnref:3"><a href="#fn:3" class="footnote">3</a></sup>. In this autonomous driving
context, <script type="math/tex">\xi</script> represents a trajectory, <script type="math/tex">\phi(\xi)</script> is the corresponding
feature-vector and <script type="math/tex">\omega</script> is a vector consisting of weights that define the
reward function.</p>

<p>We then model how humans make their choices, again based on some
psychology literature<sup id="fnref:4"><a href="#fn:4" class="footnote">4</a></sup> . Given two trajectories <script type="math/tex">\xi_{A}</script> and
<script type="math/tex">\xi_{B}</script>, the difference on the reward values is simply
<script type="math/tex">^{T}(\phi(\xi_{A})\  - \ \phi(\xi_{B}))\  =^{T}\psi</script>. Then, the
probability of user choosing <script type="math/tex">\xi_{A}</script> is:</p>

<script type="math/tex; mode=display">P(I_{A}|\ ) = \frac{1}{1 + exp( - I_{A}^{T}\psi)}</script>

<p>where <script type="math/tex">I_{A} = sign(^{T}\psi)</script>, and it being either 1 or -1 shows the
output of the query.</p>

<figure class="figure"><div class="figure__main">
<p><a href="/blog/assets/img/posts/2018-12-10-batch-active-preference-learning/image1.jpg"><img class="postimage_unpadded" src="/blog/assets/img/posts/2018-12-10-batch-active-preference-learning/image1.jpg" /></a></p>
</div><figcaption class="figure__caption" style="padding-top:0;"><p>The model of the overall decision process. The dynamical system denoted
by <script type="math/tex">D</script> produces the trajectories with respect to its initial state
<script type="math/tex">x^{0}</script> and control inputs of human and robot agents <script type="math/tex">u_{H}</script> and <script type="math/tex">u_{R}</script>.
The output of the query is then a linear combination of the difference
in the trajectory features.</p>
</figcaption></figure>

<p>The way we are going to learn the weights vector, and by extension the
reward function, is Bayesian:
<script type="math/tex">p(\omega |\ I_{i})\  \propto \ p(I_{i}\ | \omega)p(\omega)</script>, where <script type="math/tex">i</script> denotes the
query number, and after each query we update the distribution over <script type="math/tex">\omega</script>.</p>

<p>Up to this point, we have shown how preference-based learning can help
in robot learning. However, there remains an important problem: How many
such comparisons are needed to have the robot learn the reward function?
And is it always possible to learn the reward function in this way?</p>

<p><strong>Active Preference-based Learning</strong></p>

<p>To reduce the number of required comparisons, we want to actively
synthesize queries, i.e. we want to maximize the information received
from each query to learn the reward function as quickly as possible.
While optimal querying is NP-hard, Sadigh et al. showed that modeling
this problem as a <em>maximum volume removal</em> problem works well in
practice (<a href="https://dorsa.fyi/publications/sadigh2017active.pdf"><em>see this
paper</em></a>). In the
same work, they also showed the query selection problem can be
formulated as:</p>

<script type="math/tex; mode=display">\text{ma}x_{x^{0},\ u_{H_{A}},\ u_{H_{B}},\ u_{R}}min\{ E\lbrack 1 - p(I_{i}\ |\ )\rbrack,\ E\lbrack 1 - p( - I_{i}\ | \omega )\rbrack\}</script>

<p>where the robot actions and the initial state are assumed to be
identical among the two query trajectories. The optimization objective
can be approximated by sampling <script type="math/tex">\omega</script> and the optimization can be locally
solved. One can easily note that we want to generate the queries for
which we are very unsure about the outcome with the current knowledge of
<script type="math/tex">\omega</script>. Another interpretation is that we want to maximize the conditional
entropy of <script type="math/tex">I_{i}</script> given <script type="math/tex">\omega</script>. While the practicality of this method has
been analyzed in <a href="https://dorsa.fyi/publications/sadigh2017active.pdf"><em>this
paper</em></a>, query
generation times remained a huge limitation.</p>

<p><strong>Batch-Active Preference-based Learning</strong></p>

<p>To speed up the query synthesization process, we can generate a batch of
queries at once. This is again NP-hard to do optimally. Moreover, the
queries to be generated are not independent. One query might carry a
significant portion of information that another query already has. In
this case, while both queries are individually very informative, having
both of them in the batch is wasteful.</p>

<p>Then, we can describe the problem as follows. We have one feature
difference vector <script type="math/tex">\psi</script> for each query. And for each of them, we can
compute the optimization objective given above. While these values will
represent how much we desire that query to be in the batch, we also want
<script type="math/tex">\psi</script>-values to be as different as possible from each other.</p>

<p>The general approach to this problem is the following: Among <script type="math/tex">M</script>
queries, we first select <script type="math/tex">B</script> of them that individually maximize the
optimization objective. To further select <script type="math/tex">b</script> queries from this
<em>preselected set</em> to eliminate similarities between queries, we present
four different methods:</p>

<ol>
  <li>
    <ul>
      <li><strong>Greedy Selection:</strong> We simply select <script type="math/tex">b</script> individual maximizers.</li>
    </ul>
  </li>
</ol>

<figure class="figure"><div class="figure__main">
<p><a href="/blog/assets/img/posts/2018-12-10-batch-active-preference-learning/image6.gif"><img class="postimage_50" src="/blog/assets/img/posts/2018-12-10-batch-active-preference-learning/image6.gif" /></a></p>
</div></figure>

<p>2 -  <strong>Medoids Selection:</strong> We cluster <script type="math/tex">\psi</script>-vectors using
    <a href="https://en.wikipedia.org/wiki/K-medoids"><em>K-medoids</em></a> algorithm
    into <script type="math/tex">b</script> clusters and then we select the medoids.</p>

<figure class="figure"><div class="figure__main">
<p><a href="/blog/assets/img/posts/2018-12-10-batch-active-preference-learning/image2.gif"><img class="postimage_50" src="/blog/assets/img/posts/2018-12-10-batch-active-preference-learning/image2.gif" /></a></p>
</div></figure>

<p>3 -  <strong>Boundary Medoids Selection:</strong> Medoids selection algorithm can be
    improved by only choosing the queries that correspond to
    the boundary. For that, we first take the convex hull of the
    preselected set and eliminate the queries that are inside
    this volume. Then, we apply K-medoids algorithm on the remaining
    vectors to cluster them into <script type="math/tex">b</script> clusters, and then finally we
    select the medoids.</p>

<figure class="figure"><div class="figure__main">
<p><a href="/blog/assets/img/posts/2018-12-10-batch-active-preference-learning/image4.gif"><img class="postimage_50" src="/blog/assets/img/posts/2018-12-10-batch-active-preference-learning/image4.gif" /></a></p>
</div></figure>

<p>4 -  <strong>Successive Elimination:</strong> An important observation is that the
    problem we are trying to solve while selecting <script type="math/tex">b</script> of the queries
    out of the preselected set is actually similar to the <em>max-sum
    diversification problem</em>, where the aim is to select a fixed-size
    subset of points whose average distance to each other
    is maximized. Unfortunately, this problem is also known to
    be NP-hard. However, our problem is slightly different, because we
    have the optimization objective values that represent the value of
    each query to us. Hence, we propose the following algorithm: At
    every iteration of the algorithm, we select two closest points in
    the preselected set, and remove the one with lower information
    entropy (or optimization objective). And we repeat this until we
    end up with <script type="math/tex">b</script> queries.</p>

<figure class="figure"><div class="figure__main">
<p><a href="/blog/assets/img/posts/2018-12-10-batch-active-preference-learning/image5.gif"><img class="postimage_50" src="/blog/assets/img/posts/2018-12-10-batch-active-preference-learning/image5.gif" /></a></p>
</div></figure>

<p><strong>Theoretical Guarantees:</strong> In the paper, we have showed the convergence
is guaranteed under some additional assumptions with greedy selection
and successive elimination methods, as they will always keep the most
informative query in the batch.</p>

<p><strong>Experiments &amp; Results</strong></p>

<p>We did experiments with a simple linear dynamical system (LDS) and 5
different simulations from MuJoCo, OpenAI Gym, and a driving simulator
presented in <a href="https://dorsa.fyi/publications/sadigh2016planning.pdf"><em>another
work</em></a>. We
assumed a true reward function and attempted to estimate it using our
methods with <script type="math/tex">b = 10</script>.</p>

<figure class="figure"><div class="figure__main">
<p><a href="/blog/assets/img/posts/2018-12-10-batch-active-preference-learning/image3.png"><img class="postimage_unpadded" src="/blog/assets/img/posts/2018-12-10-batch-active-preference-learning/image3.png" /></a></p>
</div></figure>

<p>We evaluated each algorithm with a metric <script type="math/tex">m</script> that quantifies how close
the estimated reward function is to the true function after <script type="math/tex">N</script> queries.
So, how well did the various querying methods do?</p>

<figure class="figure"><div class="figure__main">
<p><a href="/blog/assets/img/posts/2018-12-10-batch-active-preference-learning/image12.png"><img class="postimage_unpadded" src="/blog/assets/img/posts/2018-12-10-batch-active-preference-learning/image12.png" /></a></p>
</div></figure>

<p>There we have it: the greedy algorithm is <strong>significantly</strong> outperformed
by the three other batch-active methods. The performances are ordered
from the worst to the best as greedy, medoids, boundary medoids, and
successive elimination. In fact successive elimination significantly
outperformed medoid selection, too.</p>

<p>Simulated environments also led to similar results and showed the
time-efficiency of batch-active methods. They also showed how local
optima can potentially impair the non-batch active method.</p>

<figure class="figure"><div class="figure__main">
<p><a href="/blog/assets/img/posts/2018-12-10-batch-active-preference-learning/image10.png"><img class="postimage_unpadded" src="/blog/assets/img/posts/2018-12-10-batch-active-preference-learning/image10.png" /></a></p>
</div></figure>

<p>In the table below, we show the average query time in seconds for each
method.</p>

<figure class="figure"><div class="figure__main">
<p><a href="/blog/assets/img/posts/2018-12-10-batch-active-preference-learning/table.png"><img class="postimage_unpadded" src="/blog/assets/img/posts/2018-12-10-batch-active-preference-learning/table.png" /></a></p>
</div></figure>

<p>Batch active results in a speed up of factor 15 to 50! As one might
infer there is a tradeoff between how fast we generate queries and how
fast we converge to the true reward function in terms of the number of
queries.</p>

<figure class="figure"><div class="figure__main">
<p><a href="/blog/assets/img/posts/2018-12-10-batch-active-preference-learning/image8.png"><img class="postimage_unpadded" src="/blog/assets/img/posts/2018-12-10-batch-active-preference-learning/image8.png" /></a></p>
</div></figure>

<p>Lastly, we perform usability studies by recruiting 10 human subjects to
respond the queries for Driver and Tosser tasks. We have seen that our
methods are able to efficiently learn different human preferences.</p>

<figure class="figure"><div class="figure__main">
<p><a href="/blog/assets/img/posts/2018-12-10-batch-active-preference-learning/image9.png"><img class="postimage_unpadded" src="/blog/assets/img/posts/2018-12-10-batch-active-preference-learning/image9.png" /></a></p>
</div></figure>

<figure class="figure"><div class="figure__main">
<p><a href="/blog/assets/img/posts/2018-12-10-batch-active-preference-learning/image11.png"><img class="postimage_unpadded" src="/blog/assets/img/posts/2018-12-10-batch-active-preference-learning/image11.png" /></a></p>
</div></figure>

<p>We also present demonstrative examples of the learning process for both
simulation environments.</p>

<p>Driver:</p>
<figure>
<iframe width="560" height="315" src="https://www.youtube.com/embed/MaswyWRep5g" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</figure>

<p>Tosser:</p>
<figure>
<iframe width="560" height="315" src="https://www.youtube.com/embed/cQ7vvUg9rU4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</figure>

<p><strong>What’s Next?</strong></p>

<p>So, as our results showed Batch Active learning not only improves
convergence but does it fast. We are very enthusiastic about the
continuation of this work. Some potential directions that we are
currently working on include, but are not limited to:</p>

<ul>
  <li>Varying batch-sizes could increase performance, i.e. it is
intuitively a good idea to start with small batches and then
increase batch size as we start with no information.</li>
  <li>To improve the usability in complex tasks, an end-to-end framework
that also learns the feature transformations would help a lot.</li>
  <li>While the current approach is useful with simulations, it is
important to incorporate safety constraints when working with
actual robots. In other words, we cannot simply generate
trajectories with any input when the system of interest
is safety-critical.</li>
</ul>

<p>Hopefully, an extension of this work will one day make machine learning
as successful in robotics as in the other domains where it already works
wonders.</p>

<p>This post is based on the following paper:</p>

<p><strong>Batch Active Preference-Based Learning of Reward Functions</strong>
(<a href="https://arxiv.org/abs/1810.04303"><em>arXiv</em></a>)</p>

<p>Erdem Bıyık, Dorsa Sadigh</p>

<p><em>Proceedings of the 2nd Conference on Robot Learning (CoRL), October
2018</em></p>

<p>It was also shared on the <a href="http://iliad.stanford.edu/blog/2018/10/06/batch-active-preference-based-learning-of-reward-functions/">iliad lab’s blog</a>.</p>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>Also known as ‘Programming by Demonstration’, this is just what it
sounds like: teaching a robot skills by <a href="https://youtu.be/br5PM9r91Fg"><em>demonstrating how to do
them</em></a>. <a href="#fnref:1" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>See, for example, <a href="http://www.psych.utoronto.ca/users/peterson/psy430s2001/Miller%20GA%20Magical%20Seven%20Psych%20Review%201955.pdf"><em>this famous
paper</em></a>. <a href="#fnref:2" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>This assumption is actually pretty mild, because those high-level
feature transformations of trajectories could be the neural network
embeddings. <a href="#fnref:3" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:4">
      <p>See <a href="https://en.wikipedia.org/wiki/Luce%27s_choice_axiom"><em>Luce’s choice
axiom</em></a> and
<a href="https://www.ri.cmu.edu/pub_files/2016/6/claus.pdf"><em>this work</em></a>. <a href="#fnref:4" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>

  
  </section>
  <hr>
  Keep on top of the latest SAIL Blog posts via <a class="social-icon" href="http://0.0.0.0:4000/blog/feed.xml">RSS <i class="fa fa-rss-square fa fa" title="Twitter"></i></a>, <a class="social-icon" href="https://twitter.com/StanfordAILab">Twitter <i class="fa fa-twitter-square fa fa" title="Twitter"></i></a>, or email:

<!-- Begin Mailchimp Signup Form -->
<link href="//cdn-images.mailchimp.com/embedcode/horizontal-slim-10_7.css" rel="stylesheet" type="text/css">
<style type="text/css">
	#mc_embed_signup{background:#fff; clear:left; font:14px Helvetica,Arial,sans-serif; width:100%;}
	/* Add your own Mailchimp form style overrides in your site stylesheet or in this style block.
	   We recommend moving this block and the preceding CSS link to the HEAD of your HTML file. */
</style>
<div id="mc_embed_signup">
<form action="https://stanford.us19.list-manage.com/subscribe/post?u=3a6484754abf2fc18724ec835&amp;id=028823e92b" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <div id="mc_embed_signup_scroll">
	
	<input type="email" value="" name="EMAIL" class="email" id="mce-EMAIL" placeholder="email address" required>
    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_3a6484754abf2fc18724ec835_028823e92b" tabindex="-1" value=""></div>
    <div class="clear"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button"></div>
    </div>
</form>
</div>

<!--End mc_embed_signup-->



  <!-- Social media shares -->
  <div class="share-buttons">
    <ul class="share-buttons">
        <div class="meta">Share</div>
        
        <li>
            <a href="https://www.facebook.com/sharer/sharer.php?u=http://0.0.0.0:4000/blog/batch-active-preference-learning/" target="_blank" title="Share on Facebook">
			<i class="fa fa-facebook-square fa-2x" aria-hidden="true"></i>
			<span class="sr-only">Share on Facebook</span>
		</a>
        </li>
         
        <li>
            <a href="https://twitter.com/intent/tweet?source=http://0.0.0.0:4000/blog/batch-active-preference-learning/&text=Batch-Active+Preference-Based+Learning+of+Reward+Functions%20%7C%20SAIL+Blog:%20http://0.0.0.0:4000/blog/batch-active-preference-learning/" target="_blank" title="Tweet">
			<i class="fa fa-twitter-square fa-2x" aria-hidden="true"></i>
			<span class="sr-only">Tweet</span>
		</a>
        </li>
            
        <li>
            <a href="https://getpocket.com/save?url=http://0.0.0.0:4000/blog/batch-active-preference-learning/&title=Batch-Active+Preference-Based+Learning+of+Reward+Functions%20%7C%20SAIL+Blog" target="_blank" title="Add to Pocket">
			<i class="fa fa fa-get-pocket fa-2x" aria-hidden="true"></i>
			<span class="sr-only">Add to Pocket</span>
		</a>
        </li>
         
        <li>
            <a href="http://www.reddit.com/submit?url=http://0.0.0.0:4000/blog/batch-active-preference-learning/&title=Batch-Active+Preference-Based+Learning+of+Reward+Functions%20%7C%20SAIL+Blog" target="_blank" title="Share on Reddit">
			<i class="fa fa-reddit-square fa-2x" aria-hidden="true"></i>
			<span class="sr-only">Share on Reddit</span>
		</a>
        </li>
           
        <li>
            <a href="mailto:?subject=Batch-Active+Preference-Based+Learning+of+Reward+Functions%20%7C%20SAIL+Blog&body=:%20http://0.0.0.0:4000/blog/batch-active-preference-learning/" target="_blank" title="Email">
			<i class="fa fa-envelope-square fa-2x" aria-hidden="true"></i>
			<span class="sr-only">Email</span>
		</a>
        </li>
        
    </ul>
</div>

  <!-- Tag list -->
  
  


<footer>
  <div class="tag-list">
    
      <div class="meta">Tags</div>
    

    
    <a class="button" href="/blog/tags#autonomy">
      <p><i class="fa fa-tag fa-fw"></i> autonomy</p>
    </a>
    
    <a class="button" href="/blog/tags#demonstration">
      <p><i class="fa fa-tag fa-fw"></i> demonstration</p>
    </a>
    
    <a class="button" href="/blog/tags#robotics">
      <p><i class="fa fa-tag fa-fw"></i> robotics</p>
    </a>
    
  </div>
</footer>

</article>

<!-- Disqus -->


<!-- Post navigation -->

  <div id="post-nav">
  
  <div id="previous-post" class="post-nav-post">
      <a href="/blog/deep-learning-structure-and-innate-priors/">
      <p>Previous post</p>
        Deep Learning, Structure and Innate Priors
      </a>
  </div>
  
  
  <div id="next-post" class="post-nav-post">
      <a href="/blog/ai-and-the-future-of-work/">
      <p>Next post</p>
        AI and the Future of Work
      </a>
  </div>
  
</div>



    </div>
    <hr>
<footer class="site-footer">
    
   <div class="footer-icons">
        <ul>
        <!-- Social icons from Font Awesome, if enabled -->
        
<li>
	<a href="http://0.0.0.0:4000/blog/feed.xml" title="Follow RSS feed">
		<span class="fa-stack fa-lg">
            <i class="fa fa-circle fa-stack-2x"></i>
            <i class="fa fa-rss fa-stack-1x fa-inverse"></i>
        </span>
	</a>
</li>







































<li>
	<a href="https://twitter.com/StanfordAILab" title="Follow on Twitter" class="type">
        <span class="fa-stack fa-lg">
            <i class="fa fa-circle fa-stack-2x"></i>
            <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
        </span>
	</a>
</li>








        </ul>
    </div>
    <p class="text">&copy; 2019 Stanford AI Lab
</p>
 
</footer>



  </body>
</html>
